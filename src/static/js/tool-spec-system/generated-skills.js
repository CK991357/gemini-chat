/**
 * @file è‡ªåŠ¨ç”Ÿæˆçš„æŠ€èƒ½æ³¨å†Œè¡¨ - ç”± build-skills.js è„šæœ¬ç”Ÿæˆ
 * !!! è¯·å‹¿ç›´æ¥ç¼–è¾‘æ­¤æ–‡ä»¶ !!!
 * ç”Ÿæˆæ—¶é—´: 2025-11-16T07:45:22.502Z
 * æŠ€èƒ½æ•°é‡: 6
 */

export const SKILLS_DATA = {
  "crawl4ai": {
    "metadata": {
      "name": "crawl4ai",
      "description": "åŠŸèƒ½å¼ºå¤§çš„ç½‘é¡µæŠ“å–å·¥å…·ã€‚\n\n**ã€æ“ä½œåè®®ã€‘ä½¿ç”¨å¤æ‚æ¨¡å¼æ—¶ï¼Œå¿…é¡»éµå¾ªä»¥ä¸‹ä¸¤æ­¥æµç¨‹ï¼š**\n\n**ç¬¬ä¸€æ­¥ï¼šè·å–å‚æ•°æ ¼å¼ (Get Help)**\n- **è§„åˆ™:** åœ¨è°ƒç”¨ä»»ä½•æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯ `extract` æ¨¡å¼ä¹‹å‰ï¼Œ**å¿…é¡»**å…ˆè°ƒç”¨ `tavily_search` å·¥å…·è·å–è¯¥æ¨¡å¼ç²¾ç¡®çš„å‚æ•°ç»“æ„å’Œç¤ºä¾‹ã€‚\n- **æŸ¥è¯¢æ ¼å¼:** `help(crawl4ai:<æ¨¡å¼åç§°>)`\n- **ç¤ºä¾‹:** `help(crawl4ai:extract)`\n\n**ç¬¬äºŒæ­¥ï¼šåŸºäºæ ¼å¼è°ƒç”¨å·¥å…· (Execute Tool)**\n- æ ¹æ®è·å–åˆ°çš„æŒ‡å—å’Œç¤ºä¾‹ï¼Œæ„å»ºå¹¶è°ƒç”¨å·¥å…·ã€‚\n\n**æ¨¡å¼ç›®å½•:** `scrape`, `deep_crawl`, `batch_crawl`, `extract`, `pdf_export`, `screenshot`\n",
      "tool_name": "crawl4ai",
      "category": "web-crawling",
      "priority": 9,
      "tags": [
        "web-scraping",
        "screenshot",
        "pdf-export",
        "data-extraction",
        "crawling",
        "automation",
        "content-extraction",
        "crawl4ai"
      ],
      "version": 1.1
    },
    "content": "# Crawl4AI ç½‘é¡µæŠ“å–å·¥å…·æŒ‡å—\r\n\r\nCrawl4AI æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„å¼€æºç½‘é¡µæŠ“å–å’Œæ•°æ®å¤„ç†å·¥å…·ï¼Œæ”¯æŒ 7 ç§ä¸åŒçš„å·¥ä½œæ¨¡å¼ã€‚æ‰€æœ‰äºŒè¿›åˆ¶è¾“å‡ºï¼ˆæˆªå›¾ã€PDFï¼‰éƒ½ä»¥ base64 ç¼–ç è¿”å›ï¼Œä¾¿äºæ¨¡å‹å¤„ç†ã€‚\r\n\r\n## ğŸ¯ ã€è‡³å…³é‡è¦ã€‘é€šç”¨è°ƒç”¨ç»“æ„\r\n\r\n**æ‰€æœ‰å¯¹ `crawl4ai` çš„è°ƒç”¨éƒ½å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹åµŒå¥—ç»“æ„ï¼** è¿™æ˜¯ä¸€ä¸ªé€šç”¨è§„åˆ™ï¼Œé€‚ç”¨äºæ‰€æœ‰æ¨¡å¼ã€‚\r\n\r\n```json\r\n{\r\n  \"mode\": \"<æ¨¡å¼åç§°>\",\r\n  \"parameters\": {\r\n    \"param1\": \"value1\",\r\n    \"param2\": \"value2\"\r\n    // ...å…·ä½“æ¨¡å¼çš„æ‰€æœ‰å‚æ•°éƒ½æ”¾åœ¨è¿™é‡Œ\r\n  }\r\n}\r\n```\r\n\r\n### âŒ å¸¸è§è‡´å‘½é”™è¯¯ï¼šæœªåµŒå¥—å‚æ•°\r\n\r\n```json\r\n// è¿™æ˜¯ä¸€ä¸ªç»å¯¹ä¼šå¯¼è‡´å¤±è´¥çš„é”™è¯¯è°ƒç”¨ï¼\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"url\": \"https://example.com\" // é”™è¯¯ï¼'url' å¿…é¡»åœ¨ 'parameters' å†…éƒ¨\r\n}\r\n```\r\n\r\n### âœ… æ­£ç¡®çš„åŸºç¡€è°ƒç”¨æ¨¡å¼\r\n\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\"\r\n  }\r\n}\r\n```\r\n\r\n## ğŸ“‹ å¯ç”¨æ¨¡å¼å¿«é€Ÿé€‰æ‹©æŒ‡å—\r\n\r\n| æ¨¡å¼ | åŠŸèƒ½æè¿° | ä¸»è¦ç”¨é€” | å¤æ‚åº¦ | æ¨èåœºæ™¯ |\r\n|------|----------|----------|---------|----------|\r\n| `scrape` | æŠ“å–å•ä¸ªç½‘é¡µ | è·å–é¡µé¢å†…å®¹ã€æˆªå›¾ã€PDF | â­â­ | å•é¡µé¢å†…å®¹è·å– |\r\n| `deep_crawl` | æ·±åº¦æ™ºèƒ½çˆ¬å– | ä½¿ç”¨ç­–ç•¥æ·±åº¦çˆ¬å–ç½‘ç«™ | â­â­â­â­ | ç½‘ç«™å†…å®¹æ¢ç´¢ |\r\n| `batch_crawl` | æ‰¹é‡ URL å¤„ç† | åŒæ—¶å¤„ç†å¤šä¸ª URL | â­â­ | æ‰¹é‡æ•°æ®æ”¶é›† |\r\n| `extract` | ç»“æ„åŒ–æ•°æ®æå– | åŸºäº CSS æˆ– LLM æå–æ•°æ® | â­â­â­ | ç‰¹å®šæ•°æ®æå– |\r\n| `pdf_export` | PDF å¯¼å‡º | å°†ç½‘é¡µå¯¼å‡ºä¸º PDF | â­ | æ–‡æ¡£ä¿å­˜ |\r\n| `screenshot` | æˆªå›¾æ•è· | æ•è·ç½‘é¡µæˆªå›¾ | â­ | è§†è§‰è¯æ®ä¿å­˜ |\r\n\r\n## ğŸ¯ ä½¿ç”¨åœºæ™¯å¿«é€ŸæŒ‡å—\r\n\r\n### åœºæ™¯1ï¼šå¿«é€Ÿè·å–é¡µé¢å†…å®¹\r\n```json\r\n{\r\n  \"mode\": \"scrape\", \r\n  \"parameters\": {\r\n    \"url\": \"https://example.com/article\",\r\n    \"format\": \"markdown\",\r\n    \"word_count_threshold\": 10,\r\n    \"include_links\": true,\r\n    \"include_images\": true\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯2ï¼šæ‰¹é‡æ”¶é›†äº§å“ä¿¡æ¯\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\r\n      \"https://example.com/product1\",\r\n      \"https://example.com/product2\", \r\n      \"https://example.com/product3\"\r\n    ],\r\n    \"concurrent_limit\": 3\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯3ï¼šæ·±åº¦ç ”ç©¶æŸä¸ªç½‘ç«™\r\n```json\r\n{\r\n  \"mode\": \"deep_crawl\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com/docs\",\r\n    \"max_depth\": 3,\r\n    \"keywords\": [\"æ•™ç¨‹\", \"æŒ‡å—\", \"API\"],\r\n    \"strategy\": \"best_first\"\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯4ï¼šæå–ç»“æ„åŒ–æ•°æ®\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"name\": \"Article\",\r\n      \"baseSelector\": \".article-content\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"title\",\r\n          \"selector\": \"h1\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"author\",\r\n          \"selector\": \".author\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"content\",\r\n          \"selector\": \".content\",\r\n          \"type\": \"text\"\r\n        }\r\n      ]\r\n    },\r\n    \"extraction_type\": \"css\"\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯5ï¼šä¿å­˜ç½‘é¡µè¯æ®\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"return_screenshot\": true,\r\n    \"return_pdf\": true,\r\n    \"screenshot_quality\": 90,\r\n    \"screenshot_max_width\": 1200\r\n  }\r\n}\r\n```\r\n\r\n## ğŸš€ è¯¦ç»†æ¨¡å¼è¯´æ˜\r\n\r\n### 1. æŠ“å–å•ä¸ªç½‘é¡µ (`scrape`)\r\n\r\næŠ“å–å•ä¸ªç½‘é¡µå†…å®¹ï¼Œæ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼å’Œåª’ä½“æ•è·ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"format\": \"markdown\",\r\n    \"css_selector\": \".article-content\",\r\n    \"include_links\": true,\r\n    \"include_images\": true,\r\n    \"return_screenshot\": true,\r\n    \"return_pdf\": false,\r\n    \"screenshot_quality\": 80,\r\n    \"screenshot_max_width\": 1200,\r\n    \"word_count_threshold\": 10,\r\n    \"exclude_external_links\": true\r\n  }\r\n}\r\n```\r\n\r\n**âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆå‚æ•°æœªåµŒå¥—ï¼‰:**\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"url\": \"https://example.com\", // é”™è¯¯ï¼ç¼ºå°‘parametersåŒ…è£…\r\n  \"format\": \"markdown\"\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦æŠ“å–çš„ç½‘é¡µ URLï¼Œå¿…é¡»ä»¥ `http://` æˆ– `https://` å¼€å¤´\r\n- `format`: è¾“å‡ºæ ¼å¼ï¼Œ`markdown`(é»˜è®¤)/`html`/`text`\r\n- `css_selector`: æå–ç‰¹å®šå†…å®¹çš„ CSS é€‰æ‹©å™¨\r\n- `include_links`: æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«é“¾æ¥ï¼Œé»˜è®¤ true\r\n- `include_images`: æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«å›¾ç‰‡ï¼Œé»˜è®¤ true\r\n- `return_screenshot`: æ˜¯å¦è¿”å›æˆªå›¾(base64)ï¼Œé»˜è®¤ false\r\n- `return_pdf`: æ˜¯å¦è¿”å› PDF(base64)ï¼Œé»˜è®¤ false\r\n- `screenshot_quality`: æˆªå›¾è´¨é‡(10-100)ï¼Œé»˜è®¤ 70\r\n- `screenshot_max_width`: æˆªå›¾æœ€å¤§å®½åº¦ï¼Œé»˜è®¤ 1920\r\n- `word_count_threshold`: å†…å®¹å—æœ€å°å•è¯æ•°ï¼Œé»˜è®¤ 10\r\n- `exclude_external_links`: æ˜¯å¦æ’é™¤å¤–éƒ¨é“¾æ¥ï¼Œé»˜è®¤ true\r\n\r\n### 2. æ·±åº¦ç½‘ç«™çˆ¬å– (`deep_crawl`)\r\n\r\nä½¿ç”¨æ™ºèƒ½ç­–ç•¥æ·±åº¦çˆ¬å–æ•´ä¸ªç½‘ç«™ï¼Œæ”¯æŒå…³é”®è¯è¯„åˆ†å’Œ URL è¿‡æ»¤ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"deep_crawl\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"max_depth\": 3,\r\n    \"max_pages\": 50,\r\n    \"strategy\": \"best_first\",\r\n    \"include_external\": false,\r\n    \"keywords\": [\"äº§å“\", \"ä»·æ ¼\", \"è§„æ ¼\"],\r\n    \"url_patterns\": [\"/products/\", \"/docs/\"],\r\n    \"stream\": false\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): èµ·å§‹ URL\r\n- `max_depth`: æœ€å¤§çˆ¬å–æ·±åº¦ï¼Œé»˜è®¤ 2\r\n- `max_pages`: æœ€å¤§é¡µé¢æ•°ï¼Œé»˜è®¤ 50\r\n- `strategy`: çˆ¬å–ç­–ç•¥ï¼Œ`bfs`(é»˜è®¤)/`dfs`/`best_first`\r\n- `include_external`: æ˜¯å¦è·Ÿè¸ªå¤–éƒ¨é“¾æ¥ï¼Œé»˜è®¤ false\r\n- `keywords`: ç”¨äºç›¸å…³æ€§è¯„åˆ†çš„å…³é”®è¯åˆ—è¡¨\r\n- `url_patterns`: URL æ¨¡å¼è¿‡æ»¤åˆ—è¡¨\r\n- `stream`: æ˜¯å¦æµå¼è¿”å›ç»“æœï¼Œé»˜è®¤ false\r\n\r\n### 3. æ‰¹é‡ URL å¤„ç† (`batch_crawl`)\r\n\r\nåŒæ—¶å¤„ç†å¤šä¸ª URLï¼Œé€‚ç”¨äºæ‰¹é‡æ•°æ®æ”¶é›†ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\r\n      \"https://example.com/page1\",\r\n      \"https://example.com/page2\",\r\n      \"https://example.com/page3\"\r\n    ],\r\n    \"stream\": false,\r\n    \"concurrent_limit\": 3\r\n  }\r\n}\r\n```\r\n\r\n**âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆurlsä¸æ˜¯æ•°ç»„ï¼‰:**\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": \"https://example.com/page1\" // é”™è¯¯ï¼urlså¿…é¡»æ˜¯æ•°ç»„\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `urls` (å¿…éœ€): URL åˆ—è¡¨ï¼Œå¿…é¡»æ˜¯æ•°ç»„æ ¼å¼\r\n- `stream`: æ˜¯å¦æµå¼è¿”å›ï¼Œé»˜è®¤ false\r\n- `concurrent_limit`: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ 3\r\n\r\n### 4. ç»“æ„åŒ–æ•°æ®æå– (`extract`)\r\n\r\nä»ç½‘é¡µä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼Œæ”¯æŒ CSS é€‰æ‹©å™¨å’Œ LLM æ™ºèƒ½æå–ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹ (CSS æå–):**\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"name\": \"Article\",\r\n      \"baseSelector\": \".article-content\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"title\",\r\n          \"selector\": \"h1\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"author\",\r\n          \"selector\": \".author\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"publish_date\",\r\n          \"selector\": \".date\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"content\",\r\n          \"selector\": \".content\",\r\n          \"type\": \"text\"\r\n        }\r\n      ]\r\n    },\r\n    \"css_selector\": \".article-content\",\r\n    \"extraction_type\": \"css\"\r\n  }\r\n}\r\n```\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹ (LLM æå–):**\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"type\": \"object\",\r\n      \"properties\": {\r\n        \"title\": {\"type\": \"string\"},\r\n        \"author\": {\"type\": \"string\"},\r\n        \"summary\": {\"type\": \"string\"},\r\n        \"key_points\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\r\n      }\r\n    },\r\n    \"extraction_type\": \"llm\",\r\n    \"prompt\": \"ä»æ–‡ç« ä¸­æå–æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦å’Œå…³é”®è¦ç‚¹\"\r\n  }\r\n}\r\n```\r\n\r\n**ğŸ›¡ï¸ è‡ªåŠ¨ä¿®å¤æœºåˆ¶ï¼š**\r\næˆ‘ä»¬çš„å·¥å…·ä¼šè‡ªåŠ¨ç¡®ä¿ `schema_definition` åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼š\r\n- å¦‚æœç¼ºå°‘ `baseSelector`ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º `css_selector` æˆ– `'body'`\r\n- å¦‚æœç¼ºå°‘ `fields`ï¼Œè‡ªåŠ¨åˆ›å»ºé»˜è®¤å­—æ®µé…ç½®  \r\n- å¦‚æœç¼ºå°‘ `name`ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º `\"ExtractedData\"`\r\n\r\n**ğŸ’¡ æœ€ä½³å®è·µï¼š** è™½ç„¶å·¥å…·ä¼šè‡ªåŠ¨ä¿®å¤ï¼Œä½†æä¾›å®Œæ•´çš„ schema å¯ä»¥è·å¾—æ›´ç²¾ç¡®çš„æå–ç»“æœã€‚\r\n\r\n**âš ï¸ é‡è¦æç¤º:**\r\n- **å‚æ•°åç§°**: ç”¨äºå®šä¹‰æå–ç»“æ„çš„å‚æ•°å¿…é¡»å‘½åä¸º `schema_definition`\r\n- **å¸¸è§é”™è¯¯**: è¯·å‹¿ä½¿ç”¨ `schema` ä½œä¸ºå‚æ•°åï¼Œè¿™ä¼šå¯¼è‡´è°ƒç”¨å¤±è´¥\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦æå–çš„ç½‘é¡µ URL\r\n- `schema_definition` (å¿…éœ€): å®šä¹‰è¾“å‡ºç»“æ„çš„ JSON schema\r\n- `css_selector`: åŸºç¡€ CSS é€‰æ‹©å™¨ï¼ˆCSS æå–æ—¶ä½¿ç”¨ï¼‰\r\n- `extraction_type`: æå–ç±»å‹ï¼Œ`css`(é»˜è®¤)/`llm`\r\n- `prompt`: LLM æå–çš„æç¤ºè¯­\r\n\r\n## ğŸ“‹ Schema Definition ç»“æ„è¯´æ˜\r\n\r\n### CSS æå–æ¨¡å¼å¿…éœ€çš„ schema ç»“æ„ï¼š\r\n```json\r\n{\r\n  \"name\": \"YourSchemaName\",           // å¿…éœ€ï¼šschema åç§°\r\n  \"baseSelector\": \"css-selector\",     // å¿…éœ€ï¼šåŸºç¡€ CSS é€‰æ‹©å™¨\r\n  \"fields\": [                         // å¿…éœ€ï¼šå­—æ®µå®šä¹‰æ•°ç»„\r\n    {\r\n      \"name\": \"field_name\",           // å¿…éœ€ï¼šå­—æ®µåç§°\r\n      \"selector\": \"css-selector\",     // å¿…éœ€ï¼šå­—æ®µé€‰æ‹©å™¨\r\n      \"type\": \"text\",                 // å¿…éœ€ï¼šå­—æ®µç±»å‹\r\n      \"multiple\": true                // å¯é€‰ï¼šæ˜¯å¦å…è®¸å¤šä¸ªå€¼\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### LLM æå–æ¨¡å¼ schema ç»“æ„ï¼š\r\n```json\r\n{\r\n  \"type\": \"object\",\r\n  \"properties\": {\r\n    \"field1\": {\"type\": \"string\"},\r\n    \"field2\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\r\n  }\r\n}\r\n```\r\n\r\n### 5. PDF å¯¼å‡º (`pdf_export`)\r\n\r\nå°†ç½‘é¡µå¯¼å‡ºä¸º PDF æ ¼å¼ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"pdf_export\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com/document\",\r\n    \"return_as_base64\": true\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦å¯¼å‡ºä¸º PDF çš„ç½‘é¡µ URL\r\n- `return_as_base64`: æ˜¯å¦è¿”å› base64 ç¼–ç ï¼Œé»˜è®¤ true\r\n\r\n### 6. æˆªå›¾æ•è· (`screenshot`)\r\n\r\næ•è·ç½‘é¡µæˆªå›¾ï¼Œæ”¯æŒè´¨é‡å‹ç¼©å’Œå°ºå¯¸è°ƒæ•´ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"screenshot\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"full_page\": true,\r\n    \"return_as_base64\": true,\r\n    \"quality\": 80,\r\n    \"max_width\": 1200,\r\n    \"max_height\": 3000\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦æˆªå›¾çš„ç½‘é¡µ URL\r\n- `full_page`: æ˜¯å¦æˆªå–æ•´ä¸ªé¡µé¢ï¼Œé»˜è®¤ true\r\n- `return_as_base64`: æ˜¯å¦è¿”å› base64 ç¼–ç ï¼Œé»˜è®¤ true\r\n- `quality`: æˆªå›¾è´¨é‡(10-100)ï¼Œé»˜è®¤ 70\r\n- `max_width`: æœ€å¤§å®½åº¦ï¼Œé»˜è®¤ 1920\r\n- `max_height`: æœ€å¤§é«˜åº¦ï¼Œé»˜è®¤ 5000\r\n\r\n## ğŸ”„ å¸¸è§å·¥ä½œæµ\r\n\r\n### æ–°é—»æ–‡ç« é‡‡é›†å·¥ä½œæµ\r\n**ç›®æ ‡**: è‡ªåŠ¨æ”¶é›†å’Œåˆ†ææ–°é—»å†…å®¹\r\n1. **å‘ç°é˜¶æ®µ**: ä½¿ç”¨ `deep_crawl` å‘ç°ç›¸å…³æ–‡ç« é“¾æ¥\r\n2. **é‡‡é›†é˜¶æ®µ**: ä½¿ç”¨ `batch_crawl` æ‰¹é‡è·å–å†…å®¹  \r\n3. **æå–é˜¶æ®µ**: ä½¿ç”¨ `extract` ç»“æ„åŒ–æå–å…³é”®ä¿¡æ¯\r\n\r\n### ç«å“åˆ†æå·¥ä½œæµ\r\n**ç›®æ ‡**: ç³»ç»ŸåŒ–åˆ†æç«äº‰å¯¹æ‰‹ç½‘ç«™\r\n1. **è¯æ®æ”¶é›†**: ä½¿ç”¨ `screenshot` æ•è·ç«å“é¡µé¢\r\n2. **å†…å®¹åˆ†æ**: ä½¿ç”¨ `scrape` è·å–è¯¦ç»†å†…å®¹\r\n3. **æ–‡æ¡£ä¿å­˜**: ä½¿ç”¨ `pdf_export` ä¿å­˜è¯æ®\r\n\r\n### äº§å“ç›®å½•çˆ¬å–å·¥ä½œæµ  \r\n**ç›®æ ‡**: å»ºç«‹å®Œæ•´çš„äº§å“æ•°æ®åº“\r\n1. **ç›®å½•æ¢ç´¢**: ä½¿ç”¨ `deep_crawl` å‘ç°æ‰€æœ‰äº§å“é¡µé¢\r\n2. **æ•°æ®æå–**: ä½¿ç”¨ `extract` æå–äº§å“ä¿¡æ¯\r\n\r\n## ğŸ› ï¸ æ•…éšœæ’é™¤\r\n\r\n### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ\r\n\r\n#### æ€§èƒ½é—®é¢˜\r\n- **è¶…æ—¶é—®é¢˜**: å‡å°‘ `max_pages` æˆ– `max_depth`ï¼Œé™ä½ `concurrent_limit`\r\n- **å†…å­˜é—®é¢˜**: å¯ç”¨ `stream: true`ï¼Œå‡å°‘æ‰¹é‡å¤„ç†çš„ URL æ•°é‡\r\n\r\n#### å†…å®¹è´¨é‡é—®é¢˜  \r\n- **å†…å®¹ç¼ºå¤±**: è°ƒæ•´ `word_count_threshold`ï¼Œæ£€æŸ¥ `css_selector`\r\n- **æˆªå›¾ä¸å®Œæ•´**: å¢åŠ  `max_height` å€¼ï¼Œç¡®ä¿ `full_page: true`\r\n\r\n#### ç½‘ç»œé—®é¢˜\r\n- **è¿æ¥å¤±è´¥**: æ£€æŸ¥ URL æ ¼å¼ï¼ŒéªŒè¯ç½‘ç»œè¿æ¥\r\n- **è¢«ç½‘ç«™å±è”½**: é™ä½çˆ¬å–é€Ÿåº¦ï¼Œå¢åŠ è¯·æ±‚é—´éš”\r\n\r\n#### Extract æ¨¡å¼ç‰¹å®šé—®é¢˜\r\n- **ç©ºç»“æœ**: æ£€æŸ¥ `fields` æ•°ç»„ä¸­çš„ `selector` æ˜¯å¦å‡†ç¡®åŒ¹é…é¡µé¢å…ƒç´ \r\n- **å­—æ®µç¼ºå¤±**: ç¡®ä¿ `schema_definition` åŒ…å«å®Œæ•´çš„ `name`ã€`baseSelector`ã€`fields` ç»“æ„\r\n- **è‡ªåŠ¨ä¿®å¤**: å·¥å…·ä¼šè‡ªåŠ¨è¡¥å…¨ç¼ºå¤±å­—æ®µï¼Œä½†æ‰‹åŠ¨æä¾›å®Œæ•´ schema æ•ˆæœæ›´å¥½\r\n\r\n### è°ƒè¯•æŠ€å·§\r\n\r\n1. **ä»ç®€å•å¼€å§‹**: å…ˆç”¨ `scrape` æ¨¡å¼æµ‹è¯•å•ä¸ªé¡µé¢\r\n2. **é€æ­¥å¢åŠ å¤æ‚åº¦**: ç¡®è®¤åŸºç¡€åŠŸèƒ½æ­£å¸¸åå†ä½¿ç”¨é«˜çº§æ¨¡å¼  \r\n3. **æ£€æŸ¥å‚æ•°**: ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ­£ç¡®åµŒå¥—åœ¨ `parameters` å¯¹è±¡å†…\r\n4. **éªŒè¯è¾“å‡º**: å…ˆæµ‹è¯•å°è§„æ¨¡æ•°æ®ï¼Œç¡®è®¤è¾“å‡ºæ ¼å¼ç¬¦åˆé¢„æœŸ\r\n\r\n## âš ï¸ é‡è¦æç¤º\r\n\r\n### âœ… æ­£ç¡®åšæ³•\r\n- **å‚æ•°åµŒå¥—**: æ‰€æœ‰å‚æ•°å¿…é¡»æ”¾åœ¨ `parameters` å¯¹è±¡å†…\r\n- **URL æ ¼å¼**: å¿…é¡»ä»¥ `http://` æˆ– `https://` å¼€å¤´  \r\n- **æ¨¡å¼é€‰æ‹©**: æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å¼\r\n- **å†…å­˜ç®¡ç†**: å¤§é‡æ•°æ®æ—¶ä½¿ç”¨æµå¼å¤„ç† (`stream: true`)\r\n- **Schema å®Œæ•´æ€§**: ä¸º CSS æå–æä¾›å®Œæ•´çš„ `name`ã€`baseSelector`ã€`fields` ç»“æ„\r\n\r\n### âŒ å¸¸è§é”™è¯¯\r\n\r\n**é”™è¯¯ 1: ç¼ºå°‘åµŒå¥—å‚æ•°**\r\n```json\r\n// âŒ é”™è¯¯\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"url\": \"https://example.com\"\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\"\r\n  }\r\n}\r\n```\r\n\r\n**é”™è¯¯ 2: URL ç¼ºå°‘åè®®**\r\n```json\r\n// âŒ é”™è¯¯\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"example.com\"\r\n  }\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\"\r\n  }\r\n}\r\n```\r\n\r\n**é”™è¯¯ 3: é”™è¯¯çš„å‚æ•°ç±»å‹**\r\n```json\r\n// âŒ é”™è¯¯ - urls åº”è¯¥æ˜¯æ•°ç»„\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": \"https://example.com\"\r\n  }\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\"https://example.com\"]\r\n  }\r\n}\r\n```\r\n\r\n**é”™è¯¯ 4: extractæ¨¡å¼ä½¿ç”¨é”™è¯¯çš„å‚æ•°å**\r\n```json\r\n// âŒ é”™è¯¯ - åº”è¯¥ä½¿ç”¨ schema_definition\r\n{\r\n  \"mode\": \"extract\", \r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"schema\": { // é”™è¯¯ï¼åº”è¯¥æ˜¯ schema_definition\r\n      \"title\": \"string\"\r\n    }\r\n  }\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\", \r\n    \"schema_definition\": {\r\n      \"name\": \"Article\",\r\n      \"baseSelector\": \".content\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"title\",\r\n          \"selector\": \"h1\",\r\n          \"type\": \"text\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## ğŸª é«˜çº§ä½¿ç”¨æŠ€å·§\r\n\r\n### 1. ç»„åˆä½¿ç”¨åª’ä½“æ•è·\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"include_links\": true,\r\n    \"include_images\": true,\r\n    \"return_screenshot\": true,\r\n    \"return_pdf\": true,\r\n    \"screenshot_quality\": 90,\r\n    \"screenshot_max_width\": 1200,\r\n    \"word_count_threshold\": 15\r\n  }\r\n}\r\n```\r\n\r\n### 2. æ™ºèƒ½æ·±åº¦çˆ¬å–\r\n```json\r\n{\r\n  \"mode\": \"deep_crawl\",\r\n  \"parameters\": {\r\n    \"url\": \"https://docs.example.com\",\r\n    \"strategy\": \"best_first\",\r\n    \"keywords\": [\"API\", \"æ•™ç¨‹\", \"ç¤ºä¾‹\"],\r\n    \"max_depth\": 3,\r\n    \"max_pages\": 30\r\n  }\r\n}\r\n```\r\n\r\n### 3. æ‰¹é‡å¤„ç†é‡è¦é¡µé¢\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\r\n      \"https://example.com/home\",\r\n      \"https://example.com/about\", \r\n      \"https://example.com/contact\",\r\n      \"https://example.com/products\"\r\n    ],\r\n    \"concurrent_limit\": 2\r\n  }\r\n}\r\n```\r\n\r\n### 4. æ™ºèƒ½å†…å®¹æå–\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"name\": \"NewsArticle\",\r\n      \"baseSelector\": \".article-container\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"headline\",\r\n          \"selector\": \"h1.news-title\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"author\",\r\n          \"selector\": \".author-name\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"publish_date\",\r\n          \"selector\": \".publish-date\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"main_content\",\r\n          \"selector\": \".article-body\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"tags\",\r\n          \"selector\": \".tag\",\r\n          \"type\": \"text\",\r\n          \"multiple\": true\r\n        }\r\n      ]\r\n    },\r\n    \"extraction_type\": \"css\"\r\n  }\r\n}\r\n```\r\n\r\n## ğŸ“ æœ€ä½³å®è·µæ€»ç»“\r\n\r\n1. **é€‰æ‹©åˆé€‚çš„æ¨¡å¼**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æœ€ç®€å•æœ‰æ•ˆçš„æ¨¡å¼\r\n2. **æ¸è¿›å¼æµ‹è¯•**: ä»å°è§„æ¨¡å¼€å§‹æµ‹è¯•ï¼Œé€æ­¥æ‰©å¤§èŒƒå›´  \r\n3. **èµ„æºç®¡ç†**: æ³¨æ„å¹¶å‘æ•°å’Œå†…å­˜ä½¿ç”¨ï¼Œé¿å…è¿‡åº¦è¯·æ±‚\r\n4. **é”™è¯¯å¤„ç†**: å‡†å¤‡å¥½å¤„ç†ç½‘ç»œé”™è¯¯å’Œå†…å®¹è§£æå¤±è´¥çš„æƒ…å†µ\r\n5. **åˆæ³•ä½¿ç”¨**: éµå®ˆç½‘ç«™çš„ robots.txt å’ŒæœåŠ¡æ¡æ¬¾\r\n6. **æ ¼å¼æ£€æŸ¥**: æ¯æ¬¡è°ƒç”¨å‰ç¡®è®¤å‚æ•°æ­£ç¡®åµŒå¥—åœ¨ `parameters` å¯¹è±¡å†…\r\n7. **å‚æ•°éªŒè¯**: ç¡®ä¿ URL åŒ…å«åè®®ï¼Œæ•°ç»„å‚æ•°æ­£ç¡®æ ¼å¼\r\n8. **å‘½åè§„èŒƒ**: extractæ¨¡å¼å¿…é¡»ä½¿ç”¨ `schema_definition` å‚æ•°å\r\n9. **å†…å®¹æ§åˆ¶**: ä½¿ç”¨ `include_links` å’Œ `include_images` æ§åˆ¶è¾“å‡ºå†…å®¹\r\n10. **è´¨é‡ä¼˜åŒ–**: ä½¿ç”¨ `word_count_threshold` è¿‡æ»¤ä½è´¨é‡å†…å®¹å—\r\n11. **Schema å®Œæ•´æ€§**: ä¸º CSS æå–æä¾›å®Œæ•´çš„ schema ç»“æ„ä»¥è·å¾—æœ€ä½³ç»“æœ\r\n```",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\crawl4ai",
    "lastUpdated": "2025-11-16T07:45:22.488Z"
  },
  "firecrawl": {
    "metadata": {
      "name": "firecrawl",
      "description": "å¤šåŠŸèƒ½ç½‘é¡µæŠ“å–å’Œæ•°æ®æå–å·¥å…·ï¼Œæ”¯æŒåŒæ­¥æŠ“å–ã€æœç´¢ã€ç½‘ç«™åœ°å›¾è·å–å’Œå¼‚æ­¥çˆ¬å–",
      "tool_name": "firecrawl",
      "category": "web-crawling",
      "priority": 7,
      "tags": [
        "web-scraping",
        "data-extraction",
        "crawling",
        "automation",
        "firecrawl"
      ],
      "version": 1
    },
    "content": "# å·¥å…·è°ƒç”¨ç¤ºä¾‹ï¼ˆFirecrawlï¼‰\n\n`firecrawl` æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½ç½‘é¡µæŠ“å–å’Œæ•°æ®æå–å·¥å…·ï¼Œé€šè¿‡ `mode` å‚æ•°è°ƒç”¨ä¸åŒåŠŸèƒ½ã€‚å…¶ `parameters` ç»“æ„æ˜¯åµŒå¥—çš„ã€‚\n\n**âœ… æ­£ç¡®çš„è°ƒç”¨ç»“æ„:**\n```json\n{\"mode\": \"<åŠŸèƒ½æ¨¡å¼>\", \"parameters\": {\"<å‚æ•°å>\": \"<å‚æ•°å€¼>\"}}\n```\n\n**ğŸ’¡ é‡è¦æç¤º:**\n- `scrape`ã€`search`ã€`map` æ˜¯åŒæ­¥æ“ä½œï¼Œç«‹å³è¿”å›ç»“æœ\n- `crawl`ã€`extract` æ˜¯å¼‚æ­¥æ“ä½œï¼Œè¿”å› `job_id` ç”¨äºåç»­çŠ¶æ€æ£€æŸ¥\n- æ‰€æœ‰å‚æ•°éƒ½å¿…é¡»åœ¨ `parameters` å¯¹è±¡å†…ï¼Œä¸è¦æ”¾åœ¨é¡¶å±‚\n- URL å¿…é¡»ä»¥ `http://` æˆ– `https://` å¼€å¤´\n\n## åŠŸèƒ½æ¨¡å¼è¯¦è§£\n\n### â¡ï¸ ç¤ºä¾‹ 1: æŠ“å–å•ä¸ªç½‘é¡µ (`scrape`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"scrape\", \n  \"parameters\": {\n    \"url\": \"https://docs.firecrawl.dev/\",\n    \"formats\": [\"markdown\"]  // å¯é€‰ï¼š[\"markdown\", \"html\"]ï¼Œé»˜è®¤ markdown\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 2: ç½‘é¡µæœç´¢ (`search`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"search\", \n  \"parameters\": {\n    \"query\": \"äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•\",\n    \"limit\": 5\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 3: è·å–ç½‘ç«™åœ°å›¾ (`map`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"map\", \n  \"parameters\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 4: å¼‚æ­¥çˆ¬å–ç½‘ç«™ (`crawl`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"crawl\", \n  \"parameters\": {\n    \"url\": \"https://firecrawl.dev\", \n    \"limit\": 5\n  }\n}\n```\n*æ­¤è°ƒç”¨ä¼šè¿”å›ä¸€ä¸ª `job_id`ï¼Œç”¨äºåç»­æŸ¥è¯¢ã€‚*\n\n### â¡ï¸ ç¤ºä¾‹ 5: ç»“æ„åŒ–æ•°æ®æå– (`extract`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"extract\", \n  \"parameters\": {\n    \"urls\": [\"https://news.example.com/article\"],\n    \"prompt\": \"æå–æ–‡ç« æ ‡é¢˜ã€ä½œè€…å’Œå‘å¸ƒæ—¶é—´\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"title\": {\"type\": \"string\"},\n        \"author\": {\"type\": \"string\"}, \n        \"publish_time\": {\"type\": \"string\"}\n      }\n    }\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 6: æ£€æŸ¥å¼‚æ­¥ä»»åŠ¡çŠ¶æ€ (`check_status`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"check_status\", \n  \"parameters\": {\n    \"job_id\": \"some-unique-job-identifier\"\n  }\n}\n```\n\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\n\n- **ç¼ºå°‘ `mode` å‚æ•°:** `{\"parameters\": {\"url\": \"...\"}}`\n- **ç¼ºå°‘åµŒå¥—çš„ `parameters` å¯¹è±¡:** `{\"mode\": \"scrape\", \"url\": \"...\"}`\n- **å°†å‚æ•°æ”¾åœ¨é¡¶å±‚:** `{\"url\": \"...\"}` \n- **ä½¿ç”¨æ— æ•ˆçš„ URL æ ¼å¼:** `{\"mode\": \"scrape\", \"parameters\": {\"url\": \"example.com\"}}` (ç¼ºå°‘åè®®)\n- **é”™è¯¯çš„å‚æ•°ç±»å‹:** `{\"mode\": \"extract\", \"parameters\": {\"urls\": \"https://example.com\"}}` (urls åº”è¯¥æ˜¯æ•°ç»„)",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\firecrawl",
    "lastUpdated": "2025-11-16T07:45:22.492Z"
  },
  "glm4v_analyze_image": {
    "metadata": {
      "name": "glm4v_analyze_image",
      "description": "æ™ºè°±AIçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå›¾åƒåˆ†æã€å†…å®¹è¯†åˆ«å’Œè§†è§‰é—®ç­”",
      "tool_name": "glm4v_analyze_image",
      "category": "vision",
      "priority": 7,
      "tags": [
        "image-analysis",
        "vision",
        "recognition",
        "visual-qa",
        "multimodal"
      ],
      "version": 1
    },
    "content": "# GLM-4Vå›¾åƒåˆ†æå·¥å…·æŒ‡å—\r\n\r\n## æ ¸å¿ƒèƒ½åŠ›\r\n- å›¾åƒå†…å®¹è¯†åˆ«å’Œæè¿°\r\n- è§†è§‰é—®ç­”å’Œæ¨ç†\r\n- å›¾åƒç»†èŠ‚åˆ†æ\r\n- å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆ\r\n\r\n## è°ƒç”¨è§„èŒƒ\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\",\r\n    \"image_url\": \"å›¾ç‰‡URL\",\r\n    \"prompt\": \"åˆ†ææç¤ºè¯­\"\r\n  }\r\n}\r\n```\r\n\r\nä»¥ä¸‹æ˜¯è°ƒç”¨ `glm4v_analyze_image` å·¥å…·çš„**æ­£ç¡®**å’Œ**é”™è¯¯**ç¤ºä¾‹ã€‚è¯·åŠ¡å¿…éµå¾ªæ­£ç¡®æ ¼å¼ã€‚\r\n\r\n## âœ… æ­£ç¡®ç¤ºä¾‹\r\n```json\r\n{\"model\": \"glm-4v-flash\", \"image_url\": \"https://path/to/image.jpg\", \"prompt\": \"Describe this image.\"}\r\n```\r\n\r\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\r\n\r\n- **ç¼ºå°‘å¼•å·æˆ–é€—å·:** \r\n  ```json\r\n  {\"model\": \"glm-4v-flash\", \"image_url\": \"https://path/to/image.jpg\", \"prompt\": \"Describe this image.\"}\r\n  ```\r\n  (ç¼ºå°‘ `}`)\r\n\r\n- **å‚æ•°åé”™è¯¯:** \r\n  ```json\r\n  {\"img_url\": \"https://path/to/image.jpg\"}\r\n  ```\r\n  (åº”ä¸º \"image_url\" è€Œé \"img_url\")\r\n\r\n- **æ¨¡å‹åç§°é”™è¯¯:** \r\n  ```json\r\n  {\"model\": \"glm4v-flash\", \"image_url\": \"https://path/to/image.jpg\", \"prompt\": \"Describe this image.\"}\r\n  ```\r\n  (åº”ä¸º \"glm-4v-flash\")\r\n  \r\n## å…³é”®æŒ‡ä»¤\r\n1. **æ¨¡å‹é€‰æ‹©**: ä½¿ç”¨ `glm-4v-flash` æ¨¡å‹\r\n2. **å›¾ç‰‡æ ¼å¼**: æ”¯æŒå¸¸è§å›¾ç‰‡æ ¼å¼ï¼ˆJPEG, PNG, WebPç­‰ï¼‰\r\n3. **æç¤ºè¯­è®¾è®¡**: æ¸…æ™°å…·ä½“çš„åˆ†ææŒ‡ä»¤\r\n4. **URLæœ‰æ•ˆæ€§**: ç¡®ä¿å›¾ç‰‡URLå¯å…¬å¼€è®¿é—®\r\n\r\n## ä½¿ç”¨åœºæ™¯\r\n\r\n### å›¾åƒæè¿°\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\", \r\n    \"image_url\": \"https://example.com/image.jpg\",\r\n    \"prompt\": \"è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹\"\r\n  }\r\n}\r\n```\r\n\r\n### è§†è§‰é—®ç­”\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\",\r\n    \"image_url\": \"https://example.com/image.jpg\", \r\n    \"prompt\": \"å›¾ç‰‡ä¸­æœ‰å¤šå°‘äººï¼Ÿä»–ä»¬åœ¨åšä»€ä¹ˆï¼Ÿ\"\r\n  }\r\n}\r\n```\r\n\r\n### ç»†èŠ‚åˆ†æ\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\",\r\n    \"image_url\": \"https://example.com/image.jpg\",\r\n    \"prompt\": \"åˆ†æå›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹å’ŒæŠ€æœ¯ç»†èŠ‚\"\r\n  }\r\n}\r\n```\r\n\r\n## æœ€ä½³å®è·µ\r\n\r\n### æç¤ºè¯­è®¾è®¡\r\n- **å…·ä½“æ˜ç¡®**: \"æè¿°å›¾ç‰‡ä¸­äººç‰©çš„åŠ¨ä½œå’Œè¡¨æƒ…\"\r\n- **ä»»åŠ¡å¯¼å‘**: \"è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰ç‰©ä½“å¹¶åˆ†ç±»\"\r\n- **ç»†èŠ‚è¦æ±‚**: \"æ³¨æ„é¢œè‰²ã€å½¢çŠ¶ã€ç©ºé—´å…³ç³»ç­‰ç»†èŠ‚\"\r\n\r\n### é”™è¯¯å¤„ç†\r\n- æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ\r\n- ç¡®è®¤å›¾ç‰‡æ ¼å¼æ”¯æŒ\r\n- å¤„ç†ç½‘ç»œè¶…æ—¶æƒ…å†µ\r\n\r\n## èƒ½åŠ›èŒƒå›´\r\n- âœ… ç‰©ä½“è¯†åˆ«å’Œåˆ†ç±»\r\n- âœ… åœºæ™¯ç†è§£å’Œæè¿°  \r\n- âœ… æ–‡å­—è¯†åˆ«ï¼ˆOCRï¼‰\r\n- âœ… æƒ…æ„Ÿå’Œæ°›å›´åˆ†æ\r\n- âœ… æŠ€æœ¯ç»†èŠ‚æå–\r\n\r\n## é™åˆ¶è¯´æ˜\r\n- âŒ ä¸èƒ½å¤„ç†æ•æ„Ÿæˆ–ä¸å½“å†…å®¹\r\n- âŒ å›¾ç‰‡å¤§å°å’Œåˆ†è¾¨ç‡æœ‰é™åˆ¶\r\n- âŒ å®æ—¶è§†é¢‘æµä¸æ”¯æŒ\r\n- âŒ 3Dæ¨¡å‹åˆ†æä¸æ”¯æŒ\r\n\r\n## æ€§èƒ½ä¼˜åŒ–\r\n- ä½¿ç”¨åˆé€‚çš„å›¾ç‰‡å°ºå¯¸\r\n- æä¾›å…·ä½“çš„åˆ†æéœ€æ±‚\r\n- åˆ†æ­¥éª¤è¿›è¡Œå¤æ‚åˆ†æ\r\n- ç»“åˆå…¶ä»–å·¥å…·è¿›è¡ŒéªŒè¯",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\glm4v_analyze_image",
    "lastUpdated": "2025-11-16T07:45:22.493Z"
  },
  "python_sandbox": {
    "metadata": {
      "name": "python_sandbox",
      "description": "åœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡ŒPythonä»£ç ã€‚\n\n**ã€æ“ä½œåè®®ã€‘å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œå¿…é¡»éµå¾ªä»¥ä¸‹ä¸¤æ­¥æµç¨‹ï¼š**\n\n**ç¬¬ä¸€æ­¥ï¼šè·å–ä»£ç æ¨¡æ¿ (Get Help)**\n- **è§„åˆ™:** åœ¨ç¼–å†™ä»»ä½•æ¶‰åŠæ•°æ®åˆ†æã€å¯è§†åŒ–ã€æ–‡æ¡£ç”Ÿæˆçš„ä»£ç ä¹‹å‰ï¼Œ**å¿…é¡»**å…ˆè°ƒç”¨ `tavily_search` å·¥å…·è·å–å®˜æ–¹ä»£ç æ¨¡æ¿ã€‚\n- **æŸ¥è¯¢æ ¼å¼:** `help(python_sandbox:<ä»»åŠ¡å…³é”®è¯>)`\n- **ç¤ºä¾‹:**\n  - `help(python_sandbox:visualize)` -> è·å–å¯è§†åŒ–å›¾è¡¨æŒ‡å—\n  - `help(python_sandbox:document_automation)` -> è·å–æ–‡æ¡£ç”ŸæˆæŒ‡å—\n\n**ç¬¬äºŒæ­¥ï¼šåŸºäºæ¨¡æ¿ç¼–å†™ä»£ç  (Execute Code)**\n- æ ¹æ®è·å–åˆ°çš„æŒ‡å—å’Œç¤ºä¾‹ï¼Œç¼–å†™å¹¶æ‰§è¡Œä»£ç ã€‚\n \n**ä»»åŠ¡å…³é”®è¯ç›®å½•:** `visualize`, `data-analysis`, `document_automation`, `ml`, `math`,- `scientific_computing`\n",
      "tool_name": "python_sandbox",
      "category": "code",
      "priority": 10,
      "tags": [
        "python",
        "code",
        "visualization",
        "data-analysis",
        "chart",
        "document",
        "automation",
        "machine-learning",
        "reporting",
        "excel",
        "word",
        "pdf",
        "ppt"
      ],
      "version": 2,
      "references": [
        "matplotlib_cookbook.md",
        "pandas_cheatsheet.md",
        "report_generator_workflow.md",
        "ml_workflow.md",
        "sympy_cookbook.md",
        "scipy_cookbook.md"
      ]
    },
    "content": "# Pythonæ²™ç›’å·¥å…·ä½¿ç”¨æŒ‡å—\n\n## ğŸ¯ æ ¸å¿ƒèƒ½åŠ›æ¦‚è§ˆ\n\nPythonæ²™ç›’æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„ä»£ç æ‰§è¡Œç¯å¢ƒï¼Œæ”¯æŒï¼š\n- **æ•°æ®åˆ†æä¸å¤„ç†**: ä½¿ç”¨Pandasè¿›è¡Œæ•°æ®æ¸…æ´—ã€è½¬æ¢ã€èšåˆ\n- **å¯è§†åŒ–å›¾è¡¨**: ä½¿ç”¨Matplotlib, Seaborn, Plotlyç”Ÿæˆå„ç§å›¾è¡¨\n- **æ–‡æ¡£è‡ªåŠ¨åŒ–**: åˆ›å»ºå’Œç¼–è¾‘Excel, Word, PDF, PPTæ–‡ä»¶\n- **æœºå™¨å­¦ä¹ **: ä½¿ç”¨scikit-learnè¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°\n- **ç§‘å­¦ä¸æ•°å­¦è®¡ç®—**: ä½¿ç”¨Sympyè¿›è¡Œç¬¦å·è®¡ç®—å’Œå…¬å¼è¯æ˜\n- **å·¥ä½œæµç¼–æ’**: å¤æ‚ä»»åŠ¡çš„è‡ªåŠ¨åŒ–æ‰§è¡Œç®¡é“\n\n## ğŸš€ åŸºç¡€è°ƒç”¨è§„èŒƒ\n\n### ç®€å•ä»£ç æ‰§è¡Œ\nå¯¹äºç®€å•çš„ã€ä¸€æ¬¡æ€§çš„ä»£ç æ‰§è¡Œï¼Œè¯·éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š\n\n**è°ƒç”¨æ ¼å¼:**\n```json\n{\"code\": \"print('Hello, world!')\"}\n```\n\n**è¾“å‡ºè§„èŒƒ:**\n- å›¾ç‰‡ï¼šå¿…é¡»ä»¥åŒ…å« `type: \"image\"` å’Œ `image_base64` çš„JSONå¯¹è±¡å½¢å¼è¾“å‡º\n- æ–‡ä»¶ï¼šå¿…é¡»ä»¥åŒ…å« `type: \"word|excel|...\"` å’Œ `data_base64` çš„JSONå¯¹è±¡å½¢å¼è¾“å‡º\n- è¯¦ç»†è§„èŒƒè¯·å‚è€ƒç›¸å…³ references/ æ–‡ä»¶\n\n---\n\n## ğŸ“š å·¥ä½œæµä¸å‚è€ƒæŒ‡å—\n\nå½“ä½ éœ€è¦æ‰§è¡Œä¸€é¡¹å…·ä½“çš„ã€å¤æ‚çš„ä»»åŠ¡æ—¶ï¼Œ**è¯·é¦–å…ˆæŸ¥é˜…ç›¸å…³çš„å‚è€ƒæ–‡ä»¶**ä»¥è·å–æœ€ä½³å®è·µå’Œä»£ç æ¨¡æ¿ã€‚\n\n### **1. æ•°æ®å¯è§†åŒ–**\n- **ä»»åŠ¡**: åˆ›å»ºå›¾è¡¨ï¼Œå¦‚æ¡å½¢å›¾ã€æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ã€çƒ­åŠ›å›¾ç­‰\n- **æŒ‡ä»¤**: **å¿…é¡»æŸ¥é˜… `references/matplotlib_cookbook.md`**ã€‚è¯¥æ–‡ä»¶åŒ…å«äº†æ ‡å‡†çš„å›¾è¡¨ç”Ÿæˆæ¨¡æ¿ï¼Œç¡®ä¿äº†é«˜è´¨é‡çš„ã€ç»Ÿä¸€é£æ ¼çš„è¾“å‡º\n\n### **2. æ•°æ®æ¸…æ´—ä¸åˆ†æ**\n- **ä»»åŠ¡**: å¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ï¼Œè¿›è¡Œæè¿°æ€§ç»Ÿè®¡æˆ–ç›¸å…³æ€§åˆ†æ\n- **æŒ‡ä»¤**: **è¯·å‚è€ƒ `references/pandas_cheatsheet.md`** ä¸­çš„æ•°æ®æ¸…æ´—æµæ°´çº¿ç¤ºä¾‹\n\n### **3. è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ**\n- **ä»»åŠ¡**: ç”ŸæˆåŒ…å«å›¾è¡¨å’Œæ•°æ®çš„Wordã€Excelæˆ–PDFæŠ¥å‘Š\n- **æŒ‡ä»¤**: **éµå¾ª `references/report_generator_workflow.md`** ä¸­çš„å‘¨æŠ¥ç”Ÿæˆå™¨å·¥ä½œæµã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•ç»„åˆæ•°æ®ã€å›¾è¡¨å’Œæ–‡æ¡£åº“æ¥åˆ›å»ºå¤æ‚çš„æŠ¥å‘Š\n\n### **4. æœºå™¨å­¦ä¹ **\n- **ä»»åŠ¡**: è®­ç»ƒåˆ†ç±»æˆ–å›å½’æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½\n- **æŒ‡ä»¤**: **å­¦ä¹ å¹¶ä½¿ç”¨ `references/ml_workflow.md`** ä¸­çš„ä»£ç ç»“æ„æ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹\n\n### **5. ç¬¦å·æ•°å­¦ä¸å…¬å¼è¯æ˜**\n- **ä»»åŠ¡**: è§£ä»£æ•°æ–¹ç¨‹ã€è¿›è¡Œå¾®ç§¯åˆ†è®¡ç®—ã€ç®€åŒ–æ•°å­¦è¡¨è¾¾å¼ã€è¯æ˜æ•°å­¦å…¬å¼\n- **æŒ‡ä»¤**: **å¿…é¡»ä½¿ç”¨ `sympy` åº“ï¼Œå¹¶ä¸¥æ ¼å‚è€ƒ `references/sympy_cookbook.md`** ä¸­çš„å‡½æ•°å’Œç¤ºä¾‹æ¥æ„å»ºä½ çš„è§£å†³æ–¹æ¡ˆ\n\n---\n\n## ğŸ’¡ æ ¸å¿ƒå·¥ä½œæµæ¨¡å¼\n\n### å…¬å¼è¯æ˜å·¥ä½œæµ\n1. **å®šä¹‰ç¬¦å·**: ä½¿ç”¨ `sympy.symbols()` å®šä¹‰æ‰€æœ‰å˜é‡\n2. **æ„å»ºè¡¨è¾¾å¼**: å°†å…¬å¼çš„å·¦è¾¹å’Œå³è¾¹æ„å»ºä¸ºä¸¤ä¸ªç‹¬ç«‹çš„`sympy`è¡¨è¾¾å¼\n3. **å°è¯•ç›´æ¥ç®€åŒ–**: ä½¿ç”¨ `sympy.simplify(LHS - RHS)`ï¼Œå¦‚æœç»“æœä¸º0ï¼Œåˆ™è¯æ˜æˆç«‹\n4. **è‹¥ä¸ä¸º0ï¼Œå°è¯•å˜æ¢**: ä½¿ç”¨ `expand()`, `factor()`, `trigsimp()` ç­‰å‡½æ•°å¯¹è¡¨è¾¾å¼è¿›è¡Œå˜æ¢ï¼Œå†æ¬¡å°è¯•æ­¥éª¤3\n5. **è¾“å‡ºæ­¥éª¤**: å°†ä½ çš„æ¯ä¸€æ­¥æ¨ç†å’Œä½¿ç”¨çš„`sympy`ä»£ç æ¸…æ™°åœ°å‘ˆç°å‡ºæ¥\n\n### ETLç®¡é“æ¨¡å¼ (Extract-Transform-Load)\n1. **Extract**: ä»æ•°æ®æºæå–åŸå§‹æ•°æ®\n2. **Transform**: æ¸…æ´—ã€è½¬æ¢ã€å¤„ç†æ•°æ®\n3. **Load**: ç”Ÿæˆè¾“å‡ºç»“æœï¼ˆå›¾è¡¨ã€æ–‡æ¡£ã€åˆ†ææŠ¥å‘Šï¼‰\n\n### åˆ†ææŠ¥å‘Šå·¥ä½œæµ\n1. **æ•°æ®æ”¶é›†**: è·å–æˆ–ç”Ÿæˆæ‰€éœ€æ•°æ®\n2. **æ•°æ®å¤„ç†**: æ¸…æ´—ã€è½¬æ¢ã€åˆ†ææ•°æ®\n3. **å¯è§†åŒ–**: åˆ›å»ºç›¸å…³å›¾è¡¨å’Œå¯è§†åŒ–\n4. **æŠ¥å‘Šç”Ÿæˆ**: æ•´åˆæ•°æ®å’Œå›¾è¡¨åˆ°æœ€ç»ˆæ–‡æ¡£\n\n---\n\n## ğŸ“‹ å¯ç”¨åº“å¿«é€Ÿå‚è€ƒ\n\n### æ•°æ®å¤„ç†\n- `pandas==2.2.2` - æ•°æ®åˆ†ææ ¸å¿ƒåº“\n- `numpy==1.26.4` - æ•°å€¼è®¡ç®—\n- `scipy==1.14.1` - ç§‘å­¦è®¡ç®—\n\n### å¯è§†åŒ–\n- `matplotlib==3.8.4` - åŸºç¡€ç»˜å›¾åº“\n- `seaborn==0.13.2` - ç»Ÿè®¡å¯è§†åŒ–\n- `plotly==5.18.0` - äº¤äº’å¼å›¾è¡¨\n\n### æ–‡æ¡£ç”Ÿæˆ\n- `python-docx==1.1.2` - Wordæ–‡æ¡£\n- `reportlab==4.0.7` - PDFç”Ÿæˆ\n- `python-pptx==0.6.23` - PPTæ¼”ç¤ºæ–‡ç¨¿\n- `openpyxl==3.1.2` - Excelæ–‡ä»¶æ“ä½œ\n\n### æœºå™¨å­¦ä¹ ä¸æ•°å­¦\n- `scikit-learn==1.4.2` - æœºå™¨å­¦ä¹ \n- `sympy==1.12` - ç¬¦å·æ•°å­¦\n- `statsmodels==0.14.1` - ç»Ÿè®¡æ¨¡å‹\n\n---\n\n## ğŸš¨ é‡è¦æé†’\n\n1. **å†…å­˜ç®¡ç†**: åŠæ—¶å…³é—­å›¾è¡¨å’Œæ–‡ä»¶æµï¼Œä½¿ç”¨ `plt.close('all')`\n2. **æ€§èƒ½ä¼˜åŒ–**: é¿å…åœ¨å¾ªç¯ä¸­åˆ›å»ºå¤§å‹å¯¹è±¡\n3. **è¾“å‡ºçº¯å‡€**: ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®ï¼Œé¿å…é¢å¤–æ–‡æœ¬\n4. **æŒ‰éœ€åŠ è½½**: å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œä¼˜å…ˆå‚è€ƒå¯¹åº”çš„referencesæ–‡ä»¶\n5. **é”™è¯¯å¤„ç†**: åœ¨å…³é”®æ“ä½œä¸­æ·»åŠ try-catchå—\n\né€šè¿‡è¿™ä¸ªç»“æ„åŒ–çš„æŒ‡å—å’Œä¸°å¯Œçš„å‚è€ƒæ–‡ä»¶ï¼Œæ‚¨å¯ä»¥é«˜æ•ˆåœ°å®Œæˆå„ç§å¤æ‚çš„Pythonç¼–ç¨‹ä»»åŠ¡ã€‚",
    "resources": {
      "references": {
        "matplotlib_cookbook.md": "# Matplotlib å›¾è¡¨èœè°±\n\n## ğŸ¨ å›¾è¡¨ç±»å‹æŒ‡å—\n\n### ä½•æ—¶ä½¿ç”¨ä½•ç§å›¾è¡¨\n\n- **æ¡å½¢å›¾**: ç”¨äºæ¯”è¾ƒä¸åŒç±»åˆ«çš„æ•°æ®\n- **æŠ˜çº¿å›¾**: ç”¨äºæ˜¾ç¤ºæ•°æ®éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿\n- **æ•£ç‚¹å›¾**: ç”¨äºè§‚å¯Ÿä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³ç³»\n- **ç®±çº¿å›¾**: ç”¨äºå±•ç¤ºæ•°æ®åˆ†å¸ƒå’Œè¯†åˆ«å¼‚å¸¸å€¼\n- **çƒ­åŠ›å›¾**: ç”¨äºæ˜¾ç¤ºçŸ©é˜µæ•°æ®çš„é¢œè‰²ç¼–ç \n- **é¥¼å›¾**: ç”¨äºæ˜¾ç¤ºå„éƒ¨åˆ†å æ€»ä½“çš„æ¯”ä¾‹\n\n## ğŸ“Š åŸºç¡€å›¾è¡¨æ¨¡æ¿\n\n### æ ‡å‡†æ¡å½¢å›¾\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport io\nimport base64\nimport json\n\n# æ•°æ®å‡†å¤‡\ndata = {'Category': ['A', 'B', 'C', 'D'], 'Values': [23, 45, 56, 78]}\ndf = pd.DataFrame(data)\n\n# åˆ›å»ºå›¾è¡¨\nplt.figure(figsize=(10, 6))\nplt.bar(df['Category'], df['Values'], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\nplt.title('äº§å“é”€å”®é¢å¯¹æ¯”', fontsize=14, fontweight='bold')\nplt.xlabel('äº§å“ç±»åˆ«')\nplt.ylabel('é”€å”®é¢ (ä¸‡å…ƒ)')\nplt.grid(True, alpha=0.3)\n\n# è¾“å‡ºå¤„ç†\nbuf = io.BytesIO()\nplt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\nbuf.seek(0)\nimage_base64 = base64.b64encode(buf.read()).decode('utf-8')\nbuf.close()\nplt.close('all')\n\nresult = {\n    \"type\": \"image\",\n    \"title\": \"äº§å“é”€å”®é¢å¯¹æ¯”å›¾\",\n    \"image_base64\": image_base64\n}\nprint(json.dumps(result))\n```\n\n### å¤šå­å›¾å¸ƒå±€\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport io\nimport base64\nimport json\n\n# åˆ›å»ºå¤šå­å›¾\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# å­å›¾1: æŠ˜çº¿å›¾\nx = np.linspace(0, 10, 100)\naxes[0,0].plot(x, np.sin(x), label='sin(x)')\naxes[0,0].plot(x, np.cos(x), label='cos(x)')\naxes[0,0].set_title('ä¸‰è§’å‡½æ•°')\naxes[0,0].legend()\naxes[0,0].grid(True, alpha=0.3)\n\n# å­å›¾2: æ•£ç‚¹å›¾\nx_scatter = np.random.normal(0, 1, 100)\ny_scatter = np.random.normal(0, 1, 100)\naxes[0,1].scatter(x_scatter, y_scatter, alpha=0.6)\naxes[0,1].set_title('éšæœºæ•£ç‚¹å›¾')\n\n# å­å›¾3: ç›´æ–¹å›¾\ndata_hist = np.random.normal(0, 1, 1000)\naxes[1,0].hist(data_hist, bins=30, alpha=0.7, edgecolor='black')\naxes[1,0].set_title('æ•°æ®åˆ†å¸ƒç›´æ–¹å›¾')\n\n# å­å›¾4: ç®±çº¿å›¾\ndata_box = [np.random.normal(0, 1, 100) for _ in range(4)]\naxes[1,1].boxplot(data_box, labels=['A', 'B', 'C', 'D'])\naxes[1,1].set_title('å¤šç»„æ•°æ®ç®±çº¿å›¾')\n\nplt.tight_layout()\n\n# è¾“å‡ºå¤„ç†\nbuf = io.BytesIO()\nplt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\nbuf.seek(0)\nimage_base64 = base64.b64encode(buf.read()).decode('utf-8')\nbuf.close()\nplt.close('all')\n\nresult = {\n    \"type\": \"image\", \n    \"title\": \"å¤šå›¾è¡¨åˆ†æé¢æ¿\",\n    \"image_base64\": image_base64\n}\nprint(json.dumps(result))\n```\n\n## ğŸ¯ é«˜çº§å¯è§†åŒ–æŠ€å·§\n\n### å•†åŠ¡é£æ ¼å›¾è¡¨\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# è®¾ç½®å•†åŠ¡é£æ ¼\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\n\n# åˆ›å»ºå•†åŠ¡å›¾è¡¨\ndef create_business_chart(data, chart_type='bar'):\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    if chart_type == 'bar':\n        bars = ax.bar(data.index, data.values, color='#2E86AB', alpha=0.8)\n        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾\n        for bar in bars:\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                   f'{height:.1f}', ha='center', va='bottom')\n    \n    elif chart_type == 'line':\n        ax.plot(data.index, data.values, marker='o', linewidth=2, markersize=6)\n    \n    ax.set_title('å•†åŠ¡å›¾è¡¨', fontsize=16, fontweight='bold', pad=20)\n    ax.grid(True, alpha=0.3)\n    return fig\n```\n\n## ğŸš€ Plotly é«˜çº§äº¤äº’å¼å¯è§†åŒ–\n\n### å¤æ‚äº¤äº’å¼å›¾è¡¨\n```python\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\nimport json\n\ndef create_advanced_plotly_dashboard():\n    \"\"\"åˆ›å»ºé«˜çº§ Plotly äº¤äº’å¼ä»ªè¡¨æ¿\"\"\"\n    \n    # ç”Ÿæˆç¤ºä¾‹æ•°æ®\n    np.random.seed(42)\n    n_points = 100\n    \n    # ä¸»æ•°æ®\n    main_data = pd.DataFrame({\n        'x': np.random.randn(n_points),\n        'y': np.random.randn(n_points),\n        'z': np.random.randn(n_points),\n        'category': np.random.choice(['A', 'B', 'C'], n_points),\n        'size': np.random.uniform(10, 100, n_points),\n        'value': np.random.uniform(0, 1, n_points)\n    })\n    \n    # æ—¶é—´åºåˆ—æ•°æ®\n    dates = pd.date_range('2024-01-01', periods=50, freq='D')\n    time_series_data = pd.DataFrame({\n        'date': dates,\n        'series1': np.random.randn(50).cumsum() + 100,\n        'series2': np.random.randn(50).cumsum() + 95,\n        'series3': np.random.randn(50).cumsum() + 105\n    })\n    \n    # åˆ›å»ºå­å›¾ä»ªè¡¨æ¿\n    fig = make_subplots(\n        rows=3, cols=2,\n        subplot_titles=('3Dæ•£ç‚¹å›¾', 'æ—¶é—´åºåˆ—è¶‹åŠ¿', 'å¹³è¡Œåæ ‡å›¾', 'çƒ­åŠ›å›¾', 'æ—­æ—¥å›¾', 'é›·è¾¾å›¾'),\n        specs=[\n            [{\"type\": \"scatter3d\"}, {\"type\": \"scatter\"}],\n            [{\"type\": \"parcoords\"}, {\"type\": \"heatmap\"}],\n            [{\"type\": \"sunburst\"}, {\"type\": \"scatterpolar\"}]\n        ],\n        vertical_spacing=0.08,\n        horizontal_spacing=0.08\n    )\n    \n    # 1. 3Dæ•£ç‚¹å›¾\n    for category in main_data['category'].unique():\n        category_data = main_data[main_data['category'] == category]\n        fig.add_trace(\n            go.Scatter3d(\n                x=category_data['x'],\n                y=category_data['y'],\n                z=category_data['z'],\n                mode='markers',\n                marker=dict(\n                    size=category_data['size']/20,\n                    color=category_data['value'],\n                    colorscale='Viridis',\n                    opacity=0.7,\n                    colorbar=dict(title=\"Value\")\n                ),\n                name=f'Category {category}',\n                text=[f'Value: {v:.2f}' for v in category_data['value']],\n                hoverinfo='text'\n            ),\n            row=1, col=1\n        )\n    \n    # 2. æ—¶é—´åºåˆ—å›¾\n    for i, col in enumerate(['series1', 'series2', 'series3']):\n        fig.add_trace(\n            go.Scatter(\n                x=time_series_data['date'],\n                y=time_series_data[col],\n                mode='lines+markers',\n                name=col,\n                line=dict(width=2),\n                marker=dict(size=4)\n            ),\n            row=1, col=2\n        )\n    \n    # 3. å¹³è¡Œåæ ‡å›¾\n    fig.add_trace(\n        go.Parcoords(\n            line=dict(\n                color=main_data['value'],\n                colorscale='Electric',\n                showscale=True,\n                colorbar=dict(title=\"Value\")\n            ),\n            dimensions=[\n                dict(label='X', values=main_data['x']),\n                dict(label='Y', values=main_data['y']),\n                dict(label='Z', values=main_data['z']),\n                dict(label='Size', values=main_data['size']),\n                dict(label='Value', values=main_data['value'])\n            ]\n        ),\n        row=2, col=1\n    )\n    \n    # 4. çƒ­åŠ›å›¾\n    correlation_matrix = main_data[['x', 'y', 'z', 'size', 'value']].corr()\n    fig.add_trace(\n        go.Heatmap(\n            z=correlation_matrix.values,\n            x=correlation_matrix.columns,\n            y=correlation_matrix.columns,\n            colorscale='RdBu',\n            zmid=0,\n            colorbar=dict(title=\"Correlation\")\n        ),\n        row=2, col=2\n    )\n    \n    # 5. æ—­æ—¥å›¾\n    sunburst_data = pd.DataFrame({\n        'ids': ['Total', 'Total-A', 'Total-B', 'Total-C', 'A-1', 'A-2', 'B-1', 'B-2', 'C-1', 'C-2'],\n        'labels': ['Total', 'A', 'B', 'C', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2'],\n        'parents': ['', 'Total', 'Total', 'Total', 'Total-A', 'Total-A', 'Total-B', 'Total-B', 'Total-C', 'Total-C'],\n        'values': [100, 35, 30, 35, 20, 15, 18, 12, 25, 10]\n    })\n    \n    fig.add_trace(\n        go.Sunburst(\n            ids=sunburst_data['ids'],\n            labels=sunburst_data['labels'],\n            parents=sunburst_data['parents'],\n            values=sunburst_data['values'],\n            branchvalues=\"total\",\n            marker=dict(colors=px.colors.qualitative.Pastel)\n        ),\n        row=3, col=1\n    )\n    \n    # 6. é›·è¾¾å›¾\n    categories = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\n    \n    for category in main_data['category'].unique():\n        category_data = main_data[main_data['category'] == category]\n        radar_values = [\n            category_data['x'].mean(),\n            category_data['y'].mean(),\n            category_data['z'].mean(),\n            category_data['size'].mean(),\n            category_data['value'].mean()\n        ]\n        \n        fig.add_trace(\n            go.Scatterpolar(\n                r=radar_values + [radar_values[0]],  # é—­åˆé›·è¾¾å›¾\n                theta=categories + [categories[0]],\n                fill='toself',\n                name=f'Category {category}'\n            ),\n            row=3, col=2\n        )\n    \n    # æ›´æ–°å¸ƒå±€\n    fig.update_layout(\n        title_text=\"é«˜çº§äº¤äº’å¼å¯è§†åŒ–ä»ªè¡¨æ¿\",\n        height=1200,\n        showlegend=True,\n        template=\"plotly_white\"\n    )\n    \n    # è½¬æ¢ä¸ºJSONè¾“å‡º\n    result = {\n        \"type\": \"plotly_advanced_dashboard\",\n        \"title\": \"é«˜çº§äº¤äº’å¼å¯è§†åŒ–ä»ªè¡¨æ¿\",\n        \"description\": \"åŒ…å«3Dæ•£ç‚¹å›¾ã€æ—¶é—´åºåˆ—ã€å¹³è¡Œåæ ‡ã€çƒ­åŠ›å›¾ã€æ—­æ—¥å›¾å’Œé›·è¾¾å›¾çš„å¤æ‚ä»ªè¡¨æ¿\",\n        \"note\": \"éœ€è¦åœ¨å‰ç«¯ä½¿ç”¨Plotly.jsè¿›è¡Œæ¸²æŸ“ï¼Œæ­¤JSONåŒ…å«å®Œæ•´çš„å›¾è¡¨é…ç½®\",\n        \"chart_config\": fig.to_json()\n    }\n    print(json.dumps(result))\n\n# create_advanced_plotly_dashboard()\n```\n\n### åŠ¨æ€äº¤äº’å¼å›¾è¡¨\n```python\ndef create_dynamic_interactive_charts():\n    \"\"\"åˆ›å»ºåŠ¨æ€äº¤äº’å¼å›¾è¡¨\"\"\"\n    \n    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\n    \n    # åˆ›å»ºåŠ¨æ€æ•°æ®\n    np.random.seed(42)\n    time_points = 100\n    time_index = pd.date_range('2024-01-01', periods=time_points, freq='H')\n    \n    dynamic_data = pd.DataFrame({\n        'timestamp': time_index,\n        'temperature': 20 + 10 * np.sin(2 * np.pi * np.arange(time_points) / 24) + np.random.normal(0, 1, time_points),\n        'humidity': 50 + 20 * np.cos(2 * np.pi * np.arange(time_points) / 12) + np.random.normal(0, 3, time_points),\n        'pressure': 1013 + 5 * np.sin(2 * np.pi * np.arange(time_points) / 6) + np.random.normal(0, 0.5, time_points)\n    })\n    \n    # åˆ›å»ºåŠ¨æ€ä»ªè¡¨æ¿\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=('æ¸©åº¦è¶‹åŠ¿', 'æ¹¿åº¦åˆ†å¸ƒ', 'å‹åŠ›å˜åŒ–', 'å¤šå˜é‡å…³ç³»'),\n        specs=[\n            [{\"secondary_y\": False}, {\"type\": \"histogram\"}],\n            [{\"secondary_y\": True}, {\"type\": \"scatter\"}]\n        ]\n    )\n    \n    # 1. æ¸©åº¦è¶‹åŠ¿ï¼ˆå¸¦æ»šåŠ¨å¹³å‡ï¼‰\n    fig.add_trace(\n        go.Scatter(\n            x=dynamic_data['timestamp'],\n            y=dynamic_data['temperature'],\n            mode='lines',\n            name='å®é™…æ¸©åº¦',\n            line=dict(color='red', width=1)\n        ),\n        row=1, col=1\n    )\n    \n    # æ·»åŠ æ»šåŠ¨å¹³å‡\n    rolling_avg = dynamic_data['temperature'].rolling(window=6).mean()\n    fig.add_trace(\n        go.Scatter(\n            x=dynamic_data['timestamp'],\n            y=rolling_avg,\n            mode='lines',\n            name='6å°æ—¶å¹³å‡',\n            line=dict(color='darkred', width=3)\n        ),\n        row=1, col=1\n    )\n    \n    # 2. æ¹¿åº¦åˆ†å¸ƒç›´æ–¹å›¾\n    fig.add_trace(\n        go.Histogram(\n            x=dynamic_data['humidity'],\n            nbinsx=20,\n            name='æ¹¿åº¦åˆ†å¸ƒ',\n            marker_color='lightblue',\n            opacity=0.7\n        ),\n        row=1, col=2\n    )\n    \n    # 3. å‹åŠ›å˜åŒ–ï¼ˆåŒYè½´ï¼‰\n    fig.add_trace(\n        go.Scatter(\n            x=dynamic_data['timestamp'],\n            y=dynamic_data['pressure'],\n            mode='lines',\n            name='æ°”å‹',\n            line=dict(color='green', width=2)\n        ),\n        row=2, col=1\n    )\n    \n    # æ·»åŠ å‹åŠ›å˜åŒ–ç‡ï¼ˆæ¬¡Yè½´ï¼‰\n    pressure_change = dynamic_data['pressure'].diff().fillna(0)\n    fig.add_trace(\n        go.Scatter(\n            x=dynamic_data['timestamp'],\n            y=pressure_change,\n            mode='lines',\n            name='å‹åŠ›å˜åŒ–ç‡',\n            line=dict(color='orange', width=1, dash='dot')\n        ),\n        row=2, col=1,\n        secondary_y=True\n    )\n    \n    # 4. å¤šå˜é‡æ•£ç‚¹å›¾\n    fig.add_trace(\n        go.Scatter(\n            x=dynamic_data['temperature'],\n            y=dynamic_data['humidity'],\n            mode='markers',\n            marker=dict(\n                size=8,\n                color=dynamic_data['pressure'],\n                colorscale='Viridis',\n                showscale=True,\n                colorbar=dict(title=\"Pressure\")\n            ),\n            text=[f\"Time: {ts}\" for ts in dynamic_data['timestamp']],\n            name='æ¸©æ¹¿åº¦å…³ç³»'\n        ),\n        row=2, col=2\n    )\n    \n    # æ›´æ–°å¸ƒå±€\n    fig.update_layout(\n        title_text=\"åŠ¨æ€ç¯å¢ƒæ•°æ®ç›‘æ§ä»ªè¡¨æ¿\",\n        height=800,\n        showlegend=True,\n        template=\"plotly_dark\"\n    )\n    \n    # æ›´æ–°åæ ‡è½´æ ‡ç­¾\n    fig.update_xaxes(title_text=\"æ—¶é—´\", row=1, col=1)\n    fig.update_yaxes(title_text=\"æ¸©åº¦ (Â°C)\", row=1, col=1)\n    fig.update_xaxes(title_text=\"æ¹¿åº¦ (%)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"é¢‘æ¬¡\", row=1, col=2)\n    fig.update_xaxes(title_text=\"æ—¶é—´\", row=2, col=1)\n    fig.update_yaxes(title_text=\"æ°”å‹ (hPa)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"å˜åŒ–ç‡ (hPa/h)\", secondary_y=True, row=2, col=1)\n    fig.update_xaxes(title_text=\"æ¸©åº¦ (Â°C)\", row=2, col=2)\n    fig.update_yaxes(title_text=\"æ¹¿åº¦ (%)\", row=2, col=2)\n    \n    result = {\n        \"type\": \"plotly_dynamic_dashboard\",\n        \"title\": \"åŠ¨æ€ç¯å¢ƒæ•°æ®ç›‘æ§ä»ªè¡¨æ¿\",\n        \"description\": \"å®æ—¶æ•°æ®ç›‘æ§ä»ªè¡¨æ¿ï¼ŒåŒ…å«è¶‹åŠ¿åˆ†æã€åˆ†å¸ƒç»Ÿè®¡å’Œå¤šå˜é‡å…³ç³»\",\n        \"chart_config\": fig.to_json()\n    }\n    print(json.dumps(result))\n\n# create_dynamic_interactive_charts()\n```\n\n### åœ°ç†ç©ºé—´å¯è§†åŒ–\n```python\ndef create_geospatial_visualizations():\n    \"\"\"åˆ›å»ºåœ°ç†ç©ºé—´å¯è§†åŒ–\"\"\"\n    \n    import plotly.express as px\n    \n    # åˆ›å»ºåœ°ç†æ•°æ®\n    cities_data = pd.DataFrame({\n        'city': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'æˆéƒ½', 'æ­¦æ±‰', 'è¥¿å®‰'],\n        'lat': [39.9042, 31.2304, 23.1291, 22.5431, 30.2741, 30.5728, 30.5928, 34.3416],\n        'lon': [116.4074, 121.4737, 113.2644, 114.0579, 120.1551, 104.0668, 114.3055, 108.9398],\n        'population': [2171, 2428, 1530, 1303, 1036, 1658, 1121, 1299],  # ä¸‡äºº\n        'gdp': [3610, 3870, 2510, 2767, 1560, 1770, 1560, 1000]  # åäº¿å…ƒ\n    })\n    \n    # 1. æ•£ç‚¹åœ°å›¾\n    scatter_map = px.scatter_mapbox(\n        cities_data,\n        lat=\"lat\",\n        lon=\"lon\",\n        hover_name=\"city\",\n        hover_data={\"population\": True, \"gdp\": True},\n        size=\"population\",\n        color=\"gdp\",\n        color_continuous_scale=px.colors.sequential.Viridis,\n        size_max=30,\n        zoom=3,\n        height=600,\n        title=\"ä¸­å›½ä¸»è¦åŸå¸‚äººå£ä¸GDPåˆ†å¸ƒ\"\n    )\n    \n    scatter_map.update_layout(mapbox_style=\"open-street-map\")\n    \n    # 2. å¯†åº¦åœ°å›¾\n    # ç”Ÿæˆæ¨¡æ‹Ÿå¯†åº¦æ•°æ®\n    np.random.seed(42)\n    n_points = 1000\n    density_data = pd.DataFrame({\n        'lat': np.random.uniform(20, 45, n_points),\n        'lon': np.random.uniform(100, 125, n_points),\n        'value': np.random.exponential(2, n_points)\n    })\n    \n    density_map = px.density_mapbox(\n        density_data,\n        lat='lat',\n        lon='lon',\n        z='value',\n        radius=10,\n        center=dict(lat=32, lon=110),\n        zoom=3,\n        mapbox_style=\"stamen-terrain\",\n        title=\"æ¨¡æ‹Ÿæ•°æ®å¯†åº¦åˆ†å¸ƒå›¾\",\n        height=600\n    )\n    \n    result = {\n        \"type\": \"plotly_geospatial\",\n        \"title\": \"åœ°ç†ç©ºé—´å¯è§†åŒ–\",\n        \"description\": \"åŒ…å«æ•£ç‚¹åœ°å›¾å’Œå¯†åº¦åœ°å›¾çš„åœ°ç†ç©ºé—´åˆ†æ\",\n        \"scatter_map_config\": scatter_map.to_json(),\n        \"density_map_config\": density_map.to_json(),\n        \"note\": \"éœ€è¦åœ°å›¾æœåŠ¡æ”¯æŒï¼Œå»ºè®®ä½¿ç”¨OpenStreetMapæˆ–Mapbox\"\n    }\n    print(json.dumps(result))\n\n# create_geospatial_visualizations()\n```\n\n## ğŸ¨ é¢œè‰²å’Œæ ·å¼æŒ‡å—\n\n### é¢œè‰²æ–¹æ¡ˆ\n```python\n# å•†åŠ¡é¢œè‰²æ–¹æ¡ˆ\nbusiness_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#3B1F2B']\n\n# æ¸å˜è‰²æ–¹æ¡ˆ\ngradient_colors = ['#FF6B6B', '#FFE66D', '#4ECDC4', '#45B7D1']\n\n# åˆ†ç±»é¢œè‰²\ncategorical_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n```\n\n### å­—ä½“å’Œå¸ƒå±€ä¼˜åŒ–\n```python\ndef optimize_chart_layout(fig):\n    \"\"\"ä¼˜åŒ–å›¾è¡¨å¸ƒå±€\"\"\"\n    fig.tight_layout()\n    plt.subplots_adjust(top=0.9)  # ä¸ºæ ‡é¢˜ç•™å‡ºç©ºé—´\n    return fig\n```\n\nè¿™ä¸ªmatplotlib_cookbookæ–‡ä»¶ç°åœ¨æä¾›äº†ä»åŸºç¡€Matplotlibå›¾è¡¨åˆ°é«˜çº§Plotlyäº¤äº’å¼å¯è§†åŒ–çš„å®Œæ•´æŒ‡å—ï¼Œç¡®ä¿èƒ½å¤Ÿåˆ›å»ºä¸“ä¸šã€ç¾è§‚ä¸”äº¤äº’æ€§å¼ºçš„æ•°æ®å¯è§†åŒ–ã€‚\n",
        "ml_workflow.md": "# æœºå™¨å­¦ä¹ å·¥ä½œæµæŒ‡å—\n\n## ğŸ¯ æ ‡å‡†æœºå™¨å­¦ä¹ æµç¨‹\n\n### å®Œæ•´æ¨¡å‹è®­ç»ƒæ¨¡æ¿\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport io\nimport base64\nimport json\n\ndef standard_ml_workflow(X, y, problem_type='regression'):\n    \"\"\"æ ‡å‡†æœºå™¨å­¦ä¹ å·¥ä½œæµ\"\"\"\n    \n    # æ•°æ®é¢„å¤„ç†\n    if problem_type == 'classification':\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n    \n    # åˆ†å‰²æ•°æ®\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y if problem_type == 'classification' else None\n    )\n    \n    # ç‰¹å¾æ ‡å‡†åŒ–\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # é€‰æ‹©æ¨¡å‹\n    if problem_type == 'regression':\n        model = RandomForestRegressor(n_estimators=100, random_state=42)\n    else:\n        model = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    # è®­ç»ƒæ¨¡å‹\n    model.fit(X_train_scaled, y_train)\n    \n    # é¢„æµ‹\n    y_pred = model.predict(X_test_scaled)\n    \n    # è¯„ä¼°æŒ‡æ ‡\n    if problem_type == 'regression':\n        mse = mean_squared_error(y_test, y_pred)\n        rmse = np.sqrt(mse)\n        r2 = r2_score(y_test, y_pred)\n        metrics = {'mse': mse, 'rmse': rmse, 'r2_score': r2}\n    else:\n        accuracy = accuracy_score(y_test, y_pred)\n        report = classification_report(y_test, y_pred, output_dict=True)\n        metrics = {'accuracy': accuracy, 'classification_report': report}\n    \n    # ç‰¹å¾é‡è¦æ€§\n    feature_importance = pd.DataFrame({\n        'feature': X.columns,\n        'importance': model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    return {\n        'model': model,\n        'metrics': metrics,\n        'feature_importance': feature_importance,\n        'X_test': X_test,\n        'y_test': y_test,\n        'y_pred': y_pred\n    }\n```\n\n## ğŸ“Š å›å½’åˆ†æå·¥ä½œæµ\n\n```python\ndef regression_analysis_workflow():\n    \"\"\"å®Œæ•´çš„å›å½’åˆ†æå·¥ä½œæµ\"\"\"\n    \n    # ç”Ÿæˆç¤ºä¾‹æ•°æ®\n    np.random.seed(42)\n    n_samples = 1000\n    \n    # åˆ›å»ºç‰¹å¾\n    feature1 = np.random.normal(50, 15, n_samples)\n    feature2 = np.random.normal(100, 25, n_samples)\n    feature3 = np.random.normal(10, 3, n_samples)\n    feature4 = np.random.normal(0, 1, n_samples)  # å™ªå£°ç‰¹å¾\n    \n    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆä¸ç‰¹å¾æœ‰å¤æ‚å…³ç³»ï¼‰\n    target = (2.5 * feature1 + 1.8 * feature2 - 3.2 * feature3 + \n              0.5 * feature1 * feature3 + np.random.normal(0, 20, n_samples))\n    \n    df = pd.DataFrame({\n        'feature1': feature1,\n        'feature2': feature2,\n        'feature3': feature3,\n        'feature4': feature4,\n        'target': target\n    })\n    \n    # å‡†å¤‡æ•°æ®\n    X = df[['feature1', 'feature2', 'feature3', 'feature4']]\n    y = df['target']\n    \n    # æ‰§è¡Œæ ‡å‡†å·¥ä½œæµ\n    results = standard_ml_workflow(X, y, 'regression')\n    \n    # å¯è§†åŒ–ç»“æœ\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # 1. å®é™…å€¼ vs é¢„æµ‹å€¼\n    axes[0,0].scatter(results['y_test'], results['y_pred'], alpha=0.6)\n    axes[0,0].plot([results['y_test'].min(), results['y_test'].max()], \n                  [results['y_test'].min(), results['y_test'].max()], 'r--', lw=2)\n    axes[0,0].set_xlabel('å®é™…å€¼')\n    axes[0,0].set_ylabel('é¢„æµ‹å€¼')\n    axes[0,0].set_title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {results[\"metrics\"][\"r2_score\"]:.3f})')\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # 2. æ®‹å·®åˆ†æ\n    residuals = results['y_test'] - results['y_pred']\n    axes[0,1].scatter(results['y_pred'], residuals, alpha=0.6)\n    axes[0,1].axhline(y=0, color='r', linestyle='--')\n    axes[0,1].set_xlabel('é¢„æµ‹å€¼')\n    axes[0,1].set_ylabel('æ®‹å·®')\n    axes[0,1].set_title('æ®‹å·®åˆ†æ')\n    axes[0,1].grid(True, alpha=0.3)\n    \n    # 3. ç‰¹å¾é‡è¦æ€§\n    top_features = results['feature_importance'].head(10)\n    sns.barplot(data=top_features, x='importance', y='feature', ax=axes[0,2])\n    axes[0,2].set_title('ç‰¹å¾é‡è¦æ€§æ’å')\n    \n    # 4. è¯¯å·®åˆ†å¸ƒ\n    axes[1,0].hist(residuals, bins=30, alpha=0.7, edgecolor='black', density=True)\n    axes[1,0].set_xlabel('æ®‹å·®')\n    axes[1,0].set_ylabel('å¯†åº¦')\n    axes[1,0].set_title('è¯¯å·®åˆ†å¸ƒ')\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # 5. é¢„æµ‹è¯¯å·®ç®±çº¿å›¾\n    error_percentage = np.abs(residuals / results['y_test']) * 100\n    axes[1,1].boxplot(error_percentage)\n    axes[1,1].set_ylabel('ç›¸å¯¹è¯¯å·® (%)')\n    axes[1,1].set_title('é¢„æµ‹ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ')\n    axes[1,1].grid(True, alpha=0.3)\n    \n    # 6. å­¦ä¹ æ›²çº¿ï¼ˆç®€åŒ–ç‰ˆï¼‰\n    train_sizes = np.linspace(0.1, 1.0, 10)\n    train_scores = []\n    test_scores = []\n    \n    for size in train_sizes:\n        n_train = int(size * len(X))\n        X_train_sub = X.iloc[:n_train]\n        y_train_sub = y.iloc[:n_train]\n        \n        model = RandomForestRegressor(n_estimators=50, random_state=42)\n        model.fit(X_train_sub, y_train_sub)\n        \n        train_score = model.score(X_train_sub, y_train_sub)\n        test_score = model.score(results['X_test'], results['y_test'])\n        \n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    \n    axes[1,2].plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒå¾—åˆ†')\n    axes[1,2].plot(train_sizes, test_scores, 'o-', label='æµ‹è¯•å¾—åˆ†')\n    axes[1,2].set_xlabel('è®­ç»ƒæ ·æœ¬æ¯”ä¾‹')\n    axes[1,2].set_ylabel('RÂ²å¾—åˆ†')\n    axes[1,2].set_title('å­¦ä¹ æ›²çº¿')\n    axes[1,2].legend()\n    axes[1,2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # è¾“å‡ºå›¾è¡¨\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n    buf.seek(0)\n    chart_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    buf.close()\n    plt.close('all')\n    \n    # ç”Ÿæˆæ¨¡å‹æŠ¥å‘Š\n    result = {\n        \"type\": \"ml_report\",\n        \"title\": \"å›å½’åˆ†ææ¨¡å‹æŠ¥å‘Š\",\n        \"problem_type\": \"regression\",\n        \"model_performance\": results[\"metrics\"],\n        \"feature_importance\": results[\"feature_importance\"].to_dict('records'),\n        \"training_details\": {\n            \"training_samples\": len(X) - len(results['X_test']),\n            \"test_samples\": len(results['X_test']),\n            \"features_used\": list(X.columns),\n            \"model_type\": \"RandomForestRegressor\"\n        },\n        \"chart_preview\": chart_base64,\n        \"interpretation\": {\n            \"r2_interpretation\": \"RÂ²å€¼è¡¨ç¤ºæ¨¡å‹è§£é‡Šçš„ç›®æ ‡å˜é‡æ–¹å·®æ¯”ä¾‹\",\n            \"best_features\": top_features['feature'].head(3).tolist(),\n            \"recommendations\": \"å»ºè®®å…³æ³¨é‡è¦æ€§æœ€é«˜çš„ç‰¹å¾è¿›è¡Œè¿›ä¸€æ­¥åˆ†æ\"\n        }\n    }\n    print(json.dumps(result))\n\n# regression_analysis_workflow()\n```\n\n## ğŸ” åˆ†ç±»åˆ†æå·¥ä½œæµ\n\n```python\ndef classification_analysis_workflow():\n    \"\"\"å®Œæ•´çš„åˆ†ç±»åˆ†æå·¥ä½œæµ\"\"\"\n    \n    from sklearn.datasets import make_classification\n    \n    # ç”Ÿæˆåˆ†ç±»æ•°æ®\n    X, y = make_classification(\n        n_samples=1000, \n        n_features=10,\n        n_informative=6,\n        n_redundant=2,\n        n_classes=3,\n        random_state=42\n    )\n    \n    feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n    X_df = pd.DataFrame(X, columns=feature_names)\n    \n    # æ‰§è¡Œæ ‡å‡†å·¥ä½œæµ\n    results = standard_ml_workflow(X_df, y, 'classification')\n    \n    # å¯è§†åŒ–ç»“æœ\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # 1. æ··æ·†çŸ©é˜µ\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(results['y_test'], results['y_pred'])\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n    axes[0,0].set_xlabel('é¢„æµ‹æ ‡ç­¾')\n    axes[0,0].set_ylabel('çœŸå®æ ‡ç­¾')\n    axes[0,0].set_title('æ··æ·†çŸ©é˜µ')\n    \n    # 2. ç‰¹å¾é‡è¦æ€§\n    top_features = results['feature_importance'].head(10)\n    sns.barplot(data=top_features, x='importance', y='feature', ax=axes[0,1])\n    axes[0,1].set_title('ç‰¹å¾é‡è¦æ€§æ’å')\n    \n    # 3. ç±»åˆ«åˆ†å¸ƒ\n    unique, counts = np.unique(y, return_counts=True)\n    axes[0,2].pie(counts, labels=[f'Class {cls}' for cls in unique], autopct='%1.1f%%')\n    axes[0,2].set_title('ç±»åˆ«åˆ†å¸ƒ')\n    \n    # 4. ROCæ›²çº¿ï¼ˆå¤šåˆ†ç±»ç®€åŒ–ï¼‰\n    from sklearn.metrics import roc_curve, auc\n    from sklearn.preprocessing import label_binarize\n    \n    y_test_bin = label_binarize(results['y_test'], classes=[0, 1, 2])\n    y_pred_bin = label_binarize(results['y_pred'], classes=[0, 1, 2])\n    \n    for i in range(3):\n        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n        roc_auc = auc(fpr, tpr)\n        axes[1,0].plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n    \n    axes[1,0].plot([0, 1], [0, 1], 'k--')\n    axes[1,0].set_xlabel('å‡æ­£ç‡')\n    axes[1,0].set_ylabel('çœŸæ­£ç‡')\n    axes[1,0].set_title('ROCæ›²çº¿')\n    axes[1,0].legend()\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # 5. ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿\n    from sklearn.metrics import precision_recall_curve\n    \n    for i in range(3):\n        precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_pred_bin[:, i])\n        axes[1,1].plot(recall, precision, label=f'Class {i}')\n    \n    axes[1,1].set_xlabel('å¬å›ç‡')\n    axes[1,1].set_ylabel('ç²¾ç¡®ç‡')\n    axes[1,1].set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')\n    axes[1,1].legend()\n    axes[1,1].grid(True, alpha=0.3)\n    \n    # 6. åˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾\n    report_df = pd.DataFrame(results['metrics']['classification_report']).transpose()\n    sns.heatmap(report_df.iloc[:-3, :-1], annot=True, cmap='YlOrRd', ax=axes[1,2])\n    axes[1,2].set_title('åˆ†ç±»æŒ‡æ ‡çƒ­åŠ›å›¾')\n    \n    plt.tight_layout()\n    \n    # è¾“å‡ºå›¾è¡¨\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n    buf.seek(0)\n    chart_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    buf.close()\n    plt.close('all')\n    \n    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š\n    result = {\n        \"type\": \"ml_report\", \n        \"title\": \"åˆ†ç±»åˆ†ææ¨¡å‹æŠ¥å‘Š\",\n        \"problem_type\": \"classification\",\n        \"model_performance\": results[\"metrics\"],\n        \"feature_importance\": results[\"feature_importance\"].to_dict('records'),\n        \"training_details\": {\n            \"training_samples\": len(X) - len(results['X_test']),\n            \"test_samples\": len(results['X_test']),\n            \"n_classes\": len(np.unique(y)),\n            \"model_type\": \"RandomForestClassifier\"\n        },\n        \"chart_preview\": chart_base64\n    }\n    print(json.dumps(result))\n\n# classification_analysis_workflow()\n```\n\n## ğŸ“ˆ StatsModels ç»Ÿè®¡å»ºæ¨¡\n\n```python\ndef statistical_analysis_with_statsmodels():\n    \"\"\"ä½¿ç”¨ statsmodels è¿›è¡Œç»Ÿè®¡å»ºæ¨¡\"\"\"\n    import statsmodels.api as sm\n    import statsmodels.formula.api as smf\n    from statsmodels.tsa.seasonal import seasonal_decompose\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    \n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\n    np.random.seed(42)\n    n_samples = 200\n    \n    data = pd.DataFrame({\n        'x1': np.random.normal(0, 1, n_samples),\n        'x2': np.random.normal(0, 1, n_samples),\n        'x3': np.random.normal(0, 1, n_samples),\n        'group': np.random.choice(['A', 'B', 'C'], n_samples)\n    })\n    \n    # ç”Ÿæˆç›®æ ‡å˜é‡\n    data['y'] = 2 + 1.5 * data['x1'] + 0.8 * data['x2'] + np.random.normal(0, 0.5, n_samples)\n    \n    # 1. OLS å›å½’åˆ†æ\n    model = smf.ols('y ~ x1 + x2 + x3', data=data).fit()\n    \n    # å›å½’ç»“æœæ±‡æ€»\n    regression_summary = {\n        'rsquared': model.rsquared,\n        'rsquared_adj': model.rsquared_adj,\n        'f_statistic': model.fvalue,\n        'f_pvalue': model.f_pvalue,\n        'coefficients': model.params.to_dict(),\n        'pvalues': model.pvalues.to_dict(),\n        'confidence_intervals': model.conf_int().to_dict()\n    }\n    \n    # 2. æ®‹å·®åˆ†æ\n    residuals = model.resid\n    jarque_bera = sm.stats.jarque_bera(residuals)\n    durbin_watson = sm.stats.durbin_watson(residuals)\n    \n    diagnostic_tests = {\n        'jarque_bera_statistic': jarque_bera[0],\n        'jarque_bera_pvalue': jarque_bera[1],\n        'durbin_watson': durbin_watson\n    }\n    \n    # 3. å¤šé‡å…±çº¿æ€§æ£€æŸ¥\n    X_with_const = sm.add_constant(data[['x1', 'x2', 'x3']])\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = X_with_const.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n    \n    # 4. ANOVA æ–¹å·®åˆ†æ\n    anova_model = smf.ols('y ~ group', data=data).fit()\n    anova_table = sm.stats.anova_lm(anova_model, typ=2)\n    \n    # 5. æ—¶é—´åºåˆ—åˆ†æï¼ˆç¤ºä¾‹ï¼‰\n    dates = pd.date_range('2024-01-01', periods=100, freq='D')\n    ts_data = pd.DataFrame({\n        'date': dates,\n        'value': np.random.randn(100).cumsum() + 100\n    })\n    ts_data.set_index('date', inplace=True)\n    \n    # å­£èŠ‚æ€§åˆ†è§£\n    try:\n        decomposition = seasonal_decompose(ts_data['value'], model='additive', period=7)\n        decomposition_success = True\n    except:\n        decomposition_success = False\n    \n    result = {\n        \"type\": \"statistical_analysis\",\n        \"title\": \"StatsModels ç»Ÿè®¡å»ºæ¨¡åˆ†æ\",\n        \"regression_summary\": regression_summary,\n        \"diagnostic_tests\": diagnostic_tests,\n        \"multicollinearity_check\": vif_data.to_dict('records'),\n        \"anova_analysis\": {\n            \"f_statistic\": anova_table['F']['group'],\n            \"p_value\": anova_table['PR(>F)']['group']\n        },\n        \"time_series_analysis\": {\n            \"decomposition_performed\": decomposition_success\n        },\n        \"model_interpretation\": {\n            \"significant_features\": [var for var, pval in model.pvalues.items() if pval < 0.05 and var != 'Intercept'],\n            \"model_strength\": \"å¼ºæ¨¡å‹\" if model.rsquared > 0.7 else \"ä¸­ç­‰æ¨¡å‹\" if model.rsquared > 0.5 else \"å¼±æ¨¡å‹\"\n        }\n    }\n    print(json.dumps(result))\n\n# statistical_analysis_with_statsmodels()\n```\n\n## ğŸ”§ ç§‘å­¦è®¡ç®—ä¸ä¼˜åŒ–ï¼ˆSciPyï¼‰\n\n```python\ndef scipy_scientific_computing():\n    \"\"\"ä½¿ç”¨ SciPy è¿›è¡Œç§‘å­¦è®¡ç®—\"\"\"\n    from scipy import optimize, integrate, interpolate, stats\n    from scipy.fft import fft, fftfreq\n    import matplotlib.pyplot as plt\n    import io\n    import base64\n    import json\n    \n    results = {}\n    \n    # 1. ä¼˜åŒ–é—®é¢˜ - å‡½æ•°æœ€å°åŒ–\n    def objective_function(x):\n        return (x[0] - 2)**2 + (x[1] - 3)**2 + (x[0] * x[1] - 1)**2\n    \n    initial_guess = [0, 0]\n    optimization_result = optimize.minimize(objective_function, initial_guess, method='BFGS')\n    results['optimization'] = {\n        'optimal_point': optimization_result.x.tolist(),\n        'optimal_value': float(optimization_result.fun),\n        'success': bool(optimization_result.success)\n    }\n    \n    # 2. æ•°å€¼ç§¯åˆ†\n    def integrand(x):\n        return np.exp(-x**2) * np.sin(x)\n    \n    integral_result, integral_error = integrate.quad(integrand, 0, np.inf)\n    results['integration'] = {\n        'integral_value': integral_result,\n        'estimated_error': integral_error\n    }\n    \n    # 3. æ’å€¼\n    x_known = np.linspace(0, 10, 10)\n    y_known = np.sin(x_known)\n    interpolation_function = interpolate.interp1d(x_known, y_known, kind='cubic')\n    x_new = 5.5\n    y_interpolated = interpolation_function(x_new)\n    results['interpolation'] = {\n        'known_points': len(x_known),\n        'interpolated_value_at_5.5': float(y_interpolated),\n        'actual_sin_5.5': float(np.sin(5.5))\n    }\n    \n    # 4. ç»Ÿè®¡æ£€éªŒ\n    sample1 = np.random.normal(0, 1, 100)\n    sample2 = np.random.normal(0.5, 1, 100)\n    \n    # tæ£€éªŒ\n    t_stat, t_pvalue = stats.ttest_ind(sample1, sample2)\n    \n    # æ­£æ€æ€§æ£€éªŒ\n    normality_stat, normality_pvalue = stats.normaltest(sample1)\n    \n    results['statistical_tests'] = {\n        't_test': {\n            't_statistic': t_stat,\n            'p_value': t_pvalue,\n            'significant_difference': t_pvalue < 0.05\n        },\n        'normality_test': {\n            'statistic': normality_stat,\n            'p_value': normality_pvalue,\n            'is_normal': normality_pvalue > 0.05\n        }\n    }\n    \n    # 5. ä¿¡å·å¤„ç† - å‚…é‡Œå¶å˜æ¢\n    t = np.linspace(0, 1, 1000)\n    signal = np.sin(2 * np.pi * 5 * t) + 0.5 * np.sin(2 * np.pi * 20 * t)\n    fft_result = fft(signal)\n    freqs = fftfreq(len(t), t[1] - t[0])\n    \n    # æ‰¾åˆ°ä¸»è¦é¢‘ç‡\n    positive_freq_idx = np.where(freqs > 0)\n    dominant_freq_idx = np.argmax(np.abs(fft_result[positive_freq_idx]))\n    dominant_freq = freqs[positive_freq_idx][dominant_freq_idx]\n    \n    results['signal_processing'] = {\n        'dominant_frequency': dominant_freq,\n        'expected_frequencies': [5, 20]\n    }\n    \n    # å¯è§†åŒ–éƒ¨åˆ†ç»“æœ\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # ä¼˜åŒ–å‡½æ•°å¯è§†åŒ–\n    x1 = np.linspace(-1, 5, 100)\n    x2 = np.linspace(-1, 5, 100)\n    X1, X2 = np.meshgrid(x1, x2)\n    Z = objective_function([X1, X2])\n    \n    contour = axes[0,0].contour(X1, X2, Z, levels=20)\n    axes[0,0].clabel(contour, inline=True, fontsize=8)\n    axes[0,0].plot(optimization_result.x[0], optimization_result.x[1], 'ro', markersize=10)\n    axes[0,0].set_title('å‡½æ•°ä¼˜åŒ–')\n    axes[0,0].set_xlabel('x1')\n    axes[0,0].set_ylabel('x2')\n    \n    # ç§¯åˆ†å‡½æ•°å¯è§†åŒ–\n    x_int = np.linspace(0, 3, 100)\n    y_int = integrand(x_int)\n    axes[0,1].plot(x_int, y_int)\n    axes[0,1].fill_between(x_int, y_int, alpha=0.3)\n    axes[0,1].set_title('æ•°å€¼ç§¯åˆ†')\n    axes[0,1].set_xlabel('x')\n    axes[0,1].set_ylabel('f(x)')\n    \n    # æ’å€¼å¯è§†åŒ–\n    x_fine = np.linspace(0, 10, 100)\n    y_fine = interpolation_function(x_fine)\n    axes[1,0].plot(x_known, y_known, 'o', label='å·²çŸ¥ç‚¹')\n    axes[1,0].plot(x_fine, y_fine, '-', label='æ’å€¼æ›²çº¿')\n    axes[1,0].set_title('æ’å€¼åˆ†æ')\n    axes[1,0].legend()\n    \n    # ä¿¡å·å¤„ç†å¯è§†åŒ–\n    axes[1,1].plot(t, signal)\n    axes[1,1].set_title('ä¿¡å·åˆ†æ')\n    axes[1,1].set_xlabel('æ—¶é—´')\n    axes[1,1].set_ylabel('æŒ¯å¹…')\n    \n    plt.tight_layout()\n    \n    # è¾“å‡ºå›¾è¡¨\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n    buf.seek(0)\n    chart_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    buf.close()\n    plt.close('all')\n    \n    final_result = {\n        \"type\": \"scientific_computing\",\n        \"title\": \"SciPy ç§‘å­¦è®¡ç®—åˆ†æ\",\n        \"results\": results,\n        \"chart_preview\": chart_base64\n    }\n    print(json.dumps(final_result))\n\n# scipy_scientific_computing()\n```\n\n## ğŸ§ª æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–\n\n### äº¤å‰éªŒè¯ä¸è¶…å‚æ•°è°ƒä¼˜\n```python\nfrom sklearn.model_selection import GridSearchCV\n\ndef optimize_model(X, y, problem_type='regression'):\n    \"\"\"æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–\"\"\"\n    \n    if problem_type == 'regression':\n        model = RandomForestRegressor(random_state=42)\n        param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 5, 10]\n        }\n        scoring = 'r2'\n    else:\n        model = RandomForestClassifier(random_state=42)\n        param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 5, 10]\n        }\n        scoring = 'accuracy'\n    \n    grid_search = GridSearchCV(model, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n    grid_search.fit(X, y)\n    \n    return {\n        'best_params': grid_search.best_params_,\n        'best_score': grid_search.best_score_,\n        'best_estimator': grid_search.best_estimator_\n    }\n```\n\nè¿™ä¸ªæœºå™¨å­¦ä¹ å·¥ä½œæµæŒ‡å—ç°åœ¨åŒ…å«äº†ç»Ÿè®¡å»ºæ¨¡å’Œç§‘å­¦è®¡ç®—çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒä»åŸºç¡€æœºå™¨å­¦ä¹ åˆ°é«˜çº§ç»Ÿè®¡åˆ†æçš„å…¨æµç¨‹å¤„ç†ã€‚",
        "pandas_cheatsheet.md": "# Pandas æ•°æ®å¤„ç†é€ŸæŸ¥è¡¨\n\n## ğŸ”§ å¸¸ç”¨æ“ä½œé€ŸæŸ¥\n\n### æ•°æ®è¯»å–ä¸æŸ¥çœ‹\n```python\nimport pandas as pd\nimport numpy as np\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, 35, 28],\n    'Salary': [50000, 60000, 70000, 55000],\n    'Department': ['IT', 'HR', 'IT', 'Finance']\n})\n\n# åŸºæœ¬æŸ¥çœ‹\ndf.head()        # å‰5è¡Œ\ndf.info()        # æ•°æ®ä¿¡æ¯\ndf.describe()    # æ•°å€¼åˆ—ç»Ÿè®¡\ndf.shape         # æ•°æ®ç»´åº¦\n```\n\n### æ•°æ®æ¸…æ´—æ¨¡æ¿\n```python\ndef data_cleaning_pipeline(df):\n    \"\"\"å®Œæ•´çš„æ•°æ®æ¸…æ´—æµæ°´çº¿\"\"\"\n    \n    # 1. å¤„ç†ç¼ºå¤±å€¼\n    print(\"å¤„ç†å‰ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n    print(df.isnull().sum())\n    \n    # æ•°å€¼åˆ—ç”¨ä¸­ä½æ•°å¡«å……\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n    \n    # åˆ†ç±»åˆ—ç”¨ä¼—æ•°å¡«å……\n    categorical_cols = df.select_dtypes(include=['object']).columns\n    for col in categorical_cols:\n        df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')\n    \n    # 2. å¤„ç†å¼‚å¸¸å€¼ï¼ˆIQRæ–¹æ³•ï¼‰\n    def remove_outliers_iqr(df, column):\n        Q1 = df[column].quantile(0.25)\n        Q3 = df[column].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n    \n    for col in numeric_cols:\n        df = remove_outliers_iqr(df, col)\n    \n    # 3. æ•°æ®ç±»å‹è½¬æ¢\n    df['Age'] = df['Age'].astype(int)\n    \n    return df\n```\n\n### æ•°æ®è½¬æ¢æ“ä½œ\n```python\n# æ•°æ®ç­›é€‰\ndf_filtered = df[df['Age'] > 25]\ndf_department = df[df['Department'].isin(['IT', 'Finance'])]\n\n# æ•°æ®æ’åº\ndf_sorted = df.sort_values(['Department', 'Salary'], ascending=[True, False])\n\n# åˆ†ç»„èšåˆ\ndepartment_stats = df.groupby('Department').agg({\n    'Age': ['mean', 'min', 'max'],\n    'Salary': ['mean', 'sum', 'count']\n}).round(2)\n\n# æ•°æ®é€è§†è¡¨\npivot_table = pd.pivot_table(df, \n                           values='Salary', \n                           index='Department', \n                           columns=None, \n                           aggfunc=['mean', 'sum'])\n```\n\n## ğŸ“Š å®Œæ•´æ•°æ®æ¸…æ´—æµæ°´çº¿\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport io\nimport base64\nimport json\n\ndef comprehensive_data_analysis(df):\n    \"\"\"å®Œæ•´çš„æ•°æ®åˆ†æä¸æ¸…æ´—æµæ°´çº¿\"\"\"\n    \n    # æ•°æ®è´¨é‡æŠ¥å‘Š\n    quality_report = {\n        'original_rows': len(df),\n        'original_columns': len(df.columns),\n        'missing_values': df.isnull().sum().to_dict(),\n        'data_types': df.dtypes.astype(str).to_dict(),\n        'duplicate_rows': df.duplicated().sum()\n    }\n    \n    # æ•°æ®æ¸…æ´—\n    df_clean = df.copy()\n    \n    # 1. å¤„ç†ç¼ºå¤±å€¼\n    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n    \n    for col in numeric_cols:\n        df_clean[col].fillna(df_clean[col].median(), inplace=True)\n    \n    for col in categorical_cols:\n        df_clean[col].fillna(df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown', inplace=True)\n    \n    # 2. å¤„ç†å¼‚å¸¸å€¼\n    outliers_removed = 0\n    for col in numeric_cols:\n        Q1 = df_clean[col].quantile(0.25)\n        Q3 = df_clean[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        before = len(df_clean)\n        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n        outliers_removed += (before - len(df_clean))\n    \n    quality_report['cleaned_rows'] = len(df_clean)\n    quality_report['outliers_removed'] = outliers_removed\n    \n    # 3. ç»Ÿè®¡åˆ†æ\n    analysis_results = {\n        'descriptive_stats': df_clean.describe().to_dict(),\n        'correlation_matrix': df_clean.select_dtypes(include=[np.number]).corr().to_dict()\n    }\n    \n    # 4. ç”Ÿæˆå¯è§†åŒ–\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # æ•°å€¼åˆ—åˆ†å¸ƒ\n    if len(numeric_cols) > 0:\n        df_clean[numeric_cols[0]].hist(ax=axes[0,0], bins=15, alpha=0.7, edgecolor='black')\n        axes[0,0].set_title(f'{numeric_cols[0]}åˆ†å¸ƒ')\n    \n    # ç®±çº¿å›¾\n    if len(numeric_cols) > 0:\n        df_clean[numeric_cols].boxplot(ax=axes[0,1])\n        axes[0,1].set_title('æ•°å€¼åˆ—ç®±çº¿å›¾')\n    \n    # ç›¸å…³æ€§çƒ­åŠ›å›¾\n    if len(numeric_cols) > 1:\n        sns.heatmap(df_clean[numeric_cols].corr(), annot=True, cmap='coolwarm', ax=axes[1,0])\n        axes[1,0].set_title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')\n    \n    # åˆ†ç±»æ•°æ®ç»Ÿè®¡\n    if len(categorical_cols) > 0:\n        df_clean[categorical_cols[0]].value_counts().plot(kind='bar', ax=axes[1,1])\n        axes[1,1].set_title(f'{categorical_cols[0]}åˆ†å¸ƒ')\n        axes[1,1].tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    \n    # è¾“å‡ºå›¾è¡¨\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n    buf.seek(0)\n    chart_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    buf.close()\n    plt.close('all')\n    \n    # ç”Ÿæˆåˆ†ææŠ¥å‘Š\n    result = {\n        \"type\": \"analysis_report\",\n        \"title\": \"æ•°æ®æ¸…æ´—ä¸åˆ†ææŠ¥å‘Š\",\n        \"data_quality\": quality_report,\n        \"statistical_analysis\": analysis_results,\n        \"chart_preview\": chart_base64,\n        \"cleaned_data_sample\": df_clean.head().to_dict('records')\n    }\n    print(json.dumps(result))\n\n# ä½¿ç”¨ç¤ºä¾‹\n# data = {'Age': [25, 30, 35, 28, np.nan], 'Salary': [50000, 60000, 70000, 55000, 1000000]}\n# df = pd.DataFrame(data)\n# comprehensive_data_analysis(df)\n```\n\n## ğŸš€ é«˜çº§æ•°æ®å¤„ç†æŠ€å·§\n\n### æ•°æ®åˆå¹¶ä¸è¿æ¥\n```python\n# åˆå¹¶å¤šä¸ªDataFrame\ndf1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']})\ndf2 = pd.DataFrame({'A': ['A0', 'A2'], 'C': ['C0', 'C2']})\n\n# å†…è¿æ¥\nresult_inner = pd.merge(df1, df2, on='A', how='inner')\n\n# å·¦è¿æ¥\nresult_left = pd.merge(df1, df2, on='A', how='left')\n\n# å¤–è¿æ¥\nresult_outer = pd.merge(df1, df2, on='A', how='outer')\n```\n\n### æ—¶é—´åºåˆ—å¤„ç†\n```python\n# åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®\ndates = pd.date_range('2024-01-01', periods=100, freq='D')\nts_data = pd.DataFrame({\n    'date': dates,\n    'value': np.random.randn(100).cumsum()\n})\n\n# è®¾ç½®æ—¶é—´ç´¢å¼•\nts_data.set_index('date', inplace=True)\n\n# é‡é‡‡æ ·ï¼ˆæ—¥æ•°æ®è½¬ä¸ºå‘¨æ•°æ®ï¼‰\nweekly_data = ts_data.resample('W').mean()\n\n# ç§»åŠ¨å¹³å‡\nts_data['moving_avg'] = ts_data['value'].rolling(window=7).mean()\n```\n\nè¿™ä¸ªé€ŸæŸ¥è¡¨æä¾›äº†ä»åŸºç¡€åˆ°é«˜çº§çš„Pandasæ“ä½œæŒ‡å—ï¼Œæ˜¯æ•°æ®å¤„ç†ä»»åŠ¡çš„å¿…å¤‡å‚è€ƒã€‚\n",
        "report_generator_workflow.md": "# è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆå·¥ä½œæµ\n\n## ğŸ“‹ æŠ¥å‘Šç”Ÿæˆæ ‡å‡†æ¨¡æ¿\n\n### é€šç”¨æ–‡ä»¶è¾“å‡ºå‡½æ•°\n```python\nimport io\nimport base64\nimport json\n\ndef create_file_output(file_data, file_type, title, mime_type=None):\n    \"\"\"é€šç”¨æ–‡ä»¶è¾“å‡ºå‡½æ•°\"\"\"\n    result = {\n        \"type\": file_type,\n        \"title\": title,\n        \"data_base64\": base64.b64encode(file_data).decode('utf-8')\n    }\n    if mime_type:\n        result[\"mime_type\"] = mime_type\n    print(json.dumps(result))\n```\n\n## ğŸ“Š å‘¨æŠ¥è‡ªåŠ¨ç”Ÿæˆå™¨\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom docx import Document\nfrom docx.shared import Inches\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\nimport io\nimport base64\nimport json\nfrom datetime import datetime, timedelta\nimport numpy as np\n\ndef generate_weekly_report():\n    \"\"\"å®Œæ•´çš„å‘¨æŠ¥è‡ªåŠ¨ç”Ÿæˆå·¥ä½œæµ\"\"\"\n    \n    # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¦‚æœéœ€è¦ï¼‰\n    plt.rcParams['font.sans-serif'] = ['SimHei']\n    matplotlib.use('Agg')\n    \n    # 1. æ¨¡æ‹Ÿå‘¨æ•°æ®\n    dates = pd.date_range(start='2024-01-01', periods=7, freq='D')\n    sales_data = {\n        'date': dates,\n        'revenue': np.random.normal(10000, 2000, 7),\n        'orders': np.random.randint(50, 200, 7),\n        'customers': np.random.randint(30, 150, 7),\n        'cost': np.random.normal(4000, 800, 7)\n    }\n    df = pd.DataFrame(sales_data)\n    df['profit'] = df['revenue'] - df['cost']\n    df['profit_margin'] = (df['profit'] / df['revenue'] * 100).round(2)\n    \n    # 2. ç”Ÿæˆé”€å”®è¶‹åŠ¿å›¾\n    plt.figure(figsize=(15, 10))\n    \n    # å­å›¾1: æ”¶å…¥è¶‹åŠ¿\n    plt.subplot(2, 3, 1)\n    plt.plot(df['date'], df['revenue'], marker='o', linewidth=2, color='#2E86AB', label='æ”¶å…¥')\n    plt.plot(df['date'], df['cost'], marker='s', linewidth=2, color='#A23B72', label='æˆæœ¬')\n    plt.fill_between(df['date'], df['revenue'], df['cost'], alpha=0.3, color='#4ECDC4')\n    plt.title('æ”¶å…¥ä¸æˆæœ¬è¶‹åŠ¿', fontweight='bold')\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # å­å›¾2: è®¢å•é‡\n    plt.subplot(2, 3, 2)\n    plt.bar(df['date'], df['orders'], alpha=0.7, color='#F18F01')\n    plt.title('æ¯æ—¥è®¢å•é‡', fontweight='bold')\n    plt.xticks(rotation=45)\n    plt.grid(True, alpha=0.3)\n    \n    # å­å›¾3: åˆ©æ¶¦ç‡\n    plt.subplot(2, 3, 3)\n    plt.bar(df['date'], df['profit_margin'], alpha=0.7, color='#C73E1D')\n    plt.title('æ¯æ—¥åˆ©æ¶¦ç‡ (%)', fontweight='bold')\n    plt.xticks(rotation=45)\n    plt.grid(True, alpha=0.3)\n    \n    # å­å›¾4: æ”¶å…¥åˆ†å¸ƒ\n    plt.subplot(2, 3, 4)\n    plt.pie(df['revenue'], labels=df['date'].dt.strftime('%m-%d'), autopct='%1.1f%%')\n    plt.title('æ”¶å…¥åˆ†å¸ƒ', fontweight='bold')\n    \n    # å­å›¾5: å®¢æˆ·ä¸è®¢å•å…³ç³»\n    plt.subplot(2, 3, 5)\n    plt.scatter(df['customers'], df['orders'], s=df['revenue']/100, alpha=0.6)\n    plt.xlabel('å®¢æˆ·æ•°')\n    plt.ylabel('è®¢å•æ•°')\n    plt.title('å®¢æˆ·-è®¢å•-æ”¶å…¥å…³ç³»', fontweight='bold')\n    plt.grid(True, alpha=0.3)\n    \n    # å­å›¾6: ç´¯è®¡æŒ‡æ ‡\n    plt.subplot(2, 3, 6)\n    cumulative_revenue = df['revenue'].cumsum()\n    cumulative_orders = df['orders'].cumsum()\n    plt.plot(df['date'], cumulative_revenue, label='ç´¯è®¡æ”¶å…¥', linewidth=2)\n    plt.plot(df['date'], cumulative_orders * 50, label='ç´¯è®¡è®¢å•(ç¼©æ”¾)', linewidth=2)\n    plt.title('ç´¯è®¡æŒ‡æ ‡è¶‹åŠ¿', fontweight='bold')\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # 3. å°†å›¾è¡¨è½¬ä¸ºBase64\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n    buf.seek(0)\n    chart_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    buf.close()\n    plt.close('all')\n    \n    # 4. ç”ŸæˆWordæŠ¥å‘Š\n    doc = Document()\n    \n    # æ ‡é¢˜é¡µ\n    title = doc.add_heading('é”€å”®å‘¨æŠ¥', 0)\n    title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n    \n    # æŠ¥å‘Šä¿¡æ¯\n    doc.add_paragraph(f'æŠ¥å‘Šå‘¨æœŸ: {df[\"date\"].min().strftime(\"%Yå¹´%mæœˆ%dæ—¥\")} - {df[\"date\"].max().strftime(\"%Yå¹´%mæœˆ%dæ—¥\")}')\n    doc.add_paragraph(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime(\"%Yå¹´%mæœˆ%dæ—¥ %H:%M\")}')\n    doc.add_paragraph()\n    \n    # æ‰§è¡Œæ‘˜è¦\n    doc.add_heading('æ‰§è¡Œæ‘˜è¦', level=1)\n    total_revenue = df['revenue'].sum()\n    total_orders = df['orders'].sum()\n    avg_profit_margin = df['profit_margin'].mean()\n    best_day = df.loc[df['revenue'].idxmax()]\n    \n    summary_para = doc.add_paragraph()\n    summary_para.add_run('æœ¬å‘¨ä¸šç»©äº®ç‚¹:\\n').bold = True\n    summary_para.add_run(f'â€¢ æ€»è¥æ”¶: Â¥{total_revenue:,.2f}\\n')\n    summary_para.add_run(f'â€¢ æ€»è®¢å•: {total_orders:,} å•\\n')\n    summary_para.add_run(f'â€¢ å¹³å‡åˆ©æ¶¦ç‡: {avg_profit_margin:.1f}%\\n')\n    summary_para.add_run(f'â€¢ æœ€ä½³é”€å”®æ—¥: {best_day[\"date\"].strftime(\"%mæœˆ%dæ—¥\")} (Â¥{best_day[\"revenue\"]:,.2f})')\n    \n    # å…³é”®æŒ‡æ ‡è¡¨æ ¼\n    doc.add_heading('å…³é”®æŒ‡æ ‡', level=1)\n    metrics_data = [\n        ['æ€»è¥æ”¶', f'Â¥{total_revenue:,.2f}'],\n        ['æ€»è®¢å•é‡', f'{total_orders:,}'],\n        ['æ€»å®¢æˆ·æ•°', f'{df[\"customers\"].sum():,}'],\n        ['å¹³å‡å®¢å•ä»·', f'Â¥{(df[\"revenue\"].sum()/df[\"orders\"].sum()):.2f}'],\n        ['å¹³å‡åˆ©æ¶¦ç‡', f'{avg_profit_margin:.1f}%'],\n        ['å³°å€¼æ”¶å…¥æ—¥', best_day['date'].strftime('%mæœˆ%dæ—¥')]\n    ]\n    \n    table = doc.add_table(rows=1, cols=2)\n    table.style = 'Light Grid Accent 1'\n    hdr_cells = table.rows[0].cells\n    hdr_cells[0].text = 'æŒ‡æ ‡'\n    hdr_cells[1].text = 'æ•°å€¼'\n    \n    for metric, value in metrics_data:\n        row_cells = table.add_row().cells\n        row_cells[0].text = metric\n        row_cells[1].text = value\n    \n    # è¯¦ç»†æ•°æ®åˆ†æ\n    doc.add_heading('è¯¦ç»†åˆ†æ', level=1)\n    doc.add_paragraph('æœ¬å‘¨é”€å”®è¡¨ç°æ€»ä½“ç¨³å®šï¼Œå…·ä½“åˆ†æå¦‚ä¸‹:')\n    \n    # æ·»åŠ å›¾è¡¨å¼•ç”¨\n    doc.add_paragraph('è¯¦ç»†è¶‹åŠ¿åˆ†æè¯·å‚è€ƒé™„å›¾:')\n    \n    # ä¿å­˜Wordæ–‡æ¡£\n    doc_output = io.BytesIO()\n    doc.save(doc_output)\n    doc_output.seek(0)\n    \n    # 5. è¾“å‡ºç»“æœ\n    result = {\n        \"type\": \"word\",\n        \"title\": f\"é”€å”®å‘¨æŠ¥_{datetime.now().strftime('%Y%m%d')}\",\n        \"data_base64\": base64.b64encode(doc_output.read()).decode('utf-8'),\n        \"chart_preview\": chart_base64,\n        \"summary\": {\n            \"total_revenue\": total_revenue,\n            \"total_orders\": total_orders,\n            \"avg_profit_margin\": avg_profit_margin\n        }\n    }\n    print(json.dumps(result))\n\n# ä½¿ç”¨ç¤ºä¾‹\n# generate_weekly_report()\n```\n\n## ğŸ“ˆ ExcelæŠ¥å‘Šç”Ÿæˆ\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\nfrom openpyxl.chart import BarChart, Reference\nimport io\nimport base64\nimport json\n\ndef generate_excel_report():\n    \"\"\"ç”Ÿæˆæ ¼å¼åŒ–çš„ExcelæŠ¥å‘Š\"\"\"\n    \n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\n    np.random.seed(42)\n    departments = ['é”€å”®éƒ¨', 'å¸‚åœºéƒ¨', 'æŠ€æœ¯éƒ¨', 'äººäº‹éƒ¨', 'è´¢åŠ¡éƒ¨']\n    months = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ']\n    \n    data = []\n    for dept in departments:\n        for month in months:\n            data.append({\n                'éƒ¨é—¨': dept,\n                'æœˆä»½': month,\n                'é¢„ç®—': np.random.randint(100000, 500000),\n                'å®é™…æ”¯å‡º': np.random.randint(80000, 450000),\n                'å‘˜å·¥æ•°': np.random.randint(10, 50)\n            })\n    \n    df = pd.DataFrame(data)\n    df['æ”¯å‡ºå·®å¼‚'] = df['å®é™…æ”¯å‡º'] - df['é¢„ç®—']\n    df['å·®å¼‚ç‡'] = (df['æ”¯å‡ºå·®å¼‚'] / df['é¢„ç®—'] * 100).round(2)\n    \n    # åˆ›å»ºExcelå·¥ä½œç°¿\n    wb = Workbook()\n    ws = wb.active\n    ws.title = \"éƒ¨é—¨é¢„ç®—æŠ¥å‘Š\"\n    \n    # è®¾ç½®æ ‡é¢˜\n    ws['A1'] = 'éƒ¨é—¨é¢„ç®—æ‰§è¡ŒæŠ¥å‘Š'\n    ws['A1'].font = Font(size=16, bold=True)\n    ws.merge_cells('A1:F1')\n    ws['A1'].alignment = Alignment(horizontal='center')\n    \n    # å†™å…¥æ•°æ®\n    headers = ['éƒ¨é—¨', 'æœˆä»½', 'é¢„ç®—', 'å®é™…æ”¯å‡º', 'æ”¯å‡ºå·®å¼‚', 'å·®å¼‚ç‡']\n    for col, header in enumerate(headers, 1):\n        cell = ws.cell(row=3, column=col, value=header)\n        cell.font = Font(bold=True)\n        cell.fill = PatternFill(start_color=\"DDDDDD\", end_color=\"DDDDDD\", fill_type=\"solid\")\n    \n    for row, (_, record) in enumerate(df.iterrows(), 4):\n        for col, value in enumerate(record.values, 1):\n            ws.cell(row=row, column=col, value=value)\n    \n    # æ·»åŠ æ±‡æ€»è¡Œ\n    summary_row = len(df) + 5\n    ws[f'A{summary_row}'] = 'æ€»è®¡'\n    ws[f'C{summary_row}'] = df['é¢„ç®—'].sum()\n    ws[f'D{summary_row}'] = df['å®é™…æ”¯å‡º'].sum()\n    ws[f'E{summary_row}'] = df['æ”¯å‡ºå·®å¼‚'].sum()\n    \n    # ä¿å­˜åˆ°å†…å­˜æµ\n    output = io.BytesIO()\n    wb.save(output)\n    output.seek(0)\n    \n    # è¾“å‡ºç»“æœ\n    result = {\n        \"type\": \"excel\",\n        \"title\": \"éƒ¨é—¨é¢„ç®—æ‰§è¡ŒæŠ¥å‘Š\",\n        \"data_base64\": base64.b64encode(output.getvalue()).decode('utf-8')\n    }\n    print(json.dumps(result))\n\n# generate_excel_report()\n```\n\n## ğŸ“„ PDFæŠ¥å‘Šç”Ÿæˆ\n\n```python\nfrom reportlab.lib.pagesizes import letter, A4\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.units import inch\nfrom reportlab.lib import colors\nimport io\nimport base64\nimport json\n\ndef generate_pdf_report():\n    \"\"\"ç”Ÿæˆä¸“ä¸šçš„PDFæŠ¥å‘Š\"\"\"\n    \n    buffer = io.BytesIO()\n    doc = SimpleDocTemplate(buffer, pagesize=A4, \n                          rightMargin=72, leftMargin=72,\n                          topMargin=72, bottomMargin=18)\n    \n    styles = getSampleStyleSheet()\n    styles.add(ParagraphStyle(name='Center', alignment=1))\n    \n    # æ„å»ºæ–‡æ¡£å†…å®¹\n    story = []\n    \n    # æ ‡é¢˜\n    title = Paragraph(\"ä¸šåŠ¡åˆ†ææŠ¥å‘Š\", styles['Title'])\n    story.append(title)\n    story.append(Spacer(1, 12))\n    \n    # æŠ¥å‘Šæ‘˜è¦\n    story.append(Paragraph(\"æ‰§è¡Œæ‘˜è¦\", styles['Heading2']))\n    story.append(Paragraph(\"æœ¬æŠ¥å‘Šè¯¦ç»†åˆ†æäº†è¿‘æœŸçš„ä¸šåŠ¡è¡¨ç°ï¼ŒåŒ…æ‹¬å…³é”®æŒ‡æ ‡è¶‹åŠ¿ã€éƒ¨é—¨è¡¨ç°å¯¹æ¯”ä»¥åŠæœªæ¥å»ºè®®ã€‚\", styles['BodyText']))\n    story.append(Spacer(1, 12))\n    \n    # å…³é”®æŒ‡æ ‡è¡¨æ ¼\n    story.append(Paragraph(\"å…³é”®ç»©æ•ˆæŒ‡æ ‡\", styles['Heading2']))\n    data = [\n        ['æŒ‡æ ‡', 'å½“å‰å€¼', 'ç¯æ¯”å˜åŒ–', 'ç›®æ ‡å€¼'],\n        ['æ€»æ”¶å…¥', 'Â¥1,234,567', '+5.2%', 'Â¥1,200,000'],\n        ['æ–°å®¢æˆ·', '245', '+12.3%', '220'],\n        ['å®¢æˆ·æ»¡æ„åº¦', '92%', '+2.1%', '90%'],\n        ['é¡¹ç›®å®Œæˆç‡', '88%', '+3.5%', '85%']\n    ]\n    \n    table = Table(data, colWidths=[1.5*inch, 1.2*inch, 1.2*inch, 1.2*inch])\n    table.setStyle(TableStyle([\n        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n        ('FONTSIZE', (0, 0), (-1, 0), 12),\n        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n        ('GRID', (0, 0), (-1, -1), 1, colors.black)\n    ]))\n    story.append(table)\n    story.append(Spacer(1, 12))\n    \n    # åˆ†æä¸å»ºè®®\n    story.append(Paragraph(\"åˆ†æä¸å»ºè®®\", styles['Heading2']))\n    analysis_text = \"\"\"\n    åŸºäºå½“å‰æ•°æ®åˆ†æï¼Œæˆ‘ä»¬æå‡ºä»¥ä¸‹å»ºè®®ï¼š\n    \n    1. ç»§ç»­åŠ å¼ºæ–°å®¢æˆ·è·å–ç­–ç•¥ï¼Œå½“å‰å¢é•¿åŠ¿å¤´è‰¯å¥½\n    2. ä¼˜åŒ–å®¢æˆ·æœåŠ¡æµç¨‹ï¼Œè¿›ä¸€æ­¥æå‡å®¢æˆ·æ»¡æ„åº¦\n    3. å…³æ³¨é¡¹ç›®äº¤ä»˜è´¨é‡ï¼Œç¡®ä¿å®Œæˆç‡æŒç»­æå‡\n    4. åŠ å¼ºéƒ¨é—¨é—´åä½œï¼Œæé«˜æ•´ä½“è¿è¥æ•ˆç‡\n    \"\"\"\n    story.append(Paragraph(analysis_text, styles['BodyText']))\n    \n    # ç”ŸæˆPDF\n    doc.build(story)\n    buffer.seek(0)\n    \n    result = {\n        \"type\": \"pdf\",\n        \"title\": \"ä¸šåŠ¡åˆ†ææŠ¥å‘Š\",\n        \"data_base64\": base64.b64encode(buffer.getvalue()).decode('utf-8')\n    }\n    print(json.dumps(result))\n\n# generate_pdf_report()\n```\n\nè¿™ä¸ªå·¥ä½œæµæ–‡ä»¶æä¾›äº†å®Œæ•´çš„æŠ¥å‘Šç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œä»æ•°æ®å‡†å¤‡åˆ°æœ€ç»ˆæ–‡æ¡£è¾“å‡ºï¼Œæ”¯æŒå¤šç§æ ¼å¼çš„ä¸“ä¸šæŠ¥å‘Šã€‚\n",
        "scipy_cookbook.md": "# SciPy ç§‘å­¦è®¡ç®—èœè°±\n\n## ğŸ”§ SciPy æ ¸å¿ƒæ¨¡å—æ¦‚è§ˆ\n\nSciPy æ˜¯åŸºäº NumPy çš„ç§‘å­¦è®¡ç®—åº“ï¼Œæä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š\n- **ä¼˜åŒ–ç®—æ³•** (`scipy.optimize`) - å‡½æ•°æœ€å°åŒ–ã€æ–¹ç¨‹æ±‚è§£\n- **ç§¯åˆ†è®¡ç®—** (`scipy.integrate`) - æ•°å€¼ç§¯åˆ†ã€å¾®åˆ†æ–¹ç¨‹\n- **æ’å€¼æ–¹æ³•** (`scipy.interpolate`) - æ•°æ®æ’å€¼ã€æ›²çº¿æ‹Ÿåˆ\n- **ä¿¡å·å¤„ç†** (`scipy.signal`) - æ»¤æ³¢å™¨ã€é¢‘è°±åˆ†æ\n- **çº¿æ€§ä»£æ•°** (`scipy.linalg`) - çŸ©é˜µè¿ç®—ã€çº¿æ€§ç³»ç»Ÿ\n- **ç»Ÿè®¡å‡½æ•°** (`scipy.stats`) - æ¦‚ç‡åˆ†å¸ƒã€ç»Ÿè®¡æ£€éªŒ\n- **ç©ºé—´ç®—æ³•** (`scipy.spatial`) - ç©ºé—´æ•°æ®ã€è·ç¦»è®¡ç®—\n\n## ğŸ¯ ä¼˜åŒ–ä¸æ–¹ç¨‹æ±‚è§£\n\n### å‡½æ•°ä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\nimport io\nimport base64\nimport json\n\ndef optimization_examples():\n    \"\"\"ä¼˜åŒ–é—®é¢˜æ±‚è§£ç¤ºä¾‹\"\"\"\n    \n    results = {}\n    \n    # 1. å•å˜é‡å‡½æ•°æœ€å°åŒ–\n    def single_variable_func(x):\n        return (x - 3)**2 * np.sin(x) + x**2\n    \n    single_result = optimize.minimize_scalar(single_variable_func, bounds=(0, 10), method='bounded')\n    results['single_variable'] = {\n        'optimal_x': single_result.x,\n        'optimal_value': single_result.fun,\n        'success': single_result.success\n    }\n    \n    # 2. å¤šå˜é‡å‡½æ•°æœ€å°åŒ– (Rosenbrockå‡½æ•°)\n    def rosenbrock(x):\n        return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n    \n    x0 = np.array([-1.2, 1.0, -1.5, 2.0])\n    multi_result = optimize.minimize(rosenbrock, x0, method='BFGS')\n    results['multivariable'] = {\n        'initial_guess': x0.tolist(),\n        'optimal_point': multi_result.x.tolist(),\n        'optimal_value': multi_result.fun,\n        'iterations': multi_result.nit\n    }\n    \n    # 3. çº¦æŸä¼˜åŒ–\n    def objective(x):\n        return x[0]**2 + x[1]**2\n    \n    def constraint1(x):\n        return x[0] + x[1] - 1  # x + y >= 1\n    \n    def constraint2(x):\n        return x[0] - x[1] + 0.5  # x - y >= -0.5\n    \n    constraints = [\n        {'type': 'ineq', 'fun': constraint1},\n        {'type': 'ineq', 'fun': constraint2}\n    ]\n    \n    bounds = [(0, None), (0, None)]\n    constrained_result = optimize.minimize(objective, [0.5, 0.5], \n                                         method='SLSQP', bounds=bounds, \n                                         constraints=constraints)\n    results['constrained_optimization'] = {\n        'optimal_point': constrained_result.x.tolist(),\n        'optimal_value': constrained_result.fun,\n        'constraints_satisfied': constrained_result.success\n    }\n    \n    # 4. æ–¹ç¨‹æ±‚è§£\n    def equations(vars):\n        x, y = vars\n        eq1 = x**2 + y**2 - 1  # å•ä½åœ†\n        eq2 = x - y - 0.5      # ç›´çº¿\n        return [eq1, eq2]\n    \n    root_result = optimize.root(equations, [1, 1])\n    results['equation_solving'] = {\n        'solution': root_result.x.tolist(),\n        'residuals': root_result.fun.tolist(),\n        'success': root_result.success\n    }\n    \n    # å¯è§†åŒ–ä¼˜åŒ–ç»“æœ\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # å•å˜é‡å‡½æ•°å¯è§†åŒ–\n    x_plot = np.linspace(0, 10, 100)\n    y_plot = single_variable_func(x_plot)\n    axes[0,0].plot(x_plot, y_plot, label='f(x)')\n    axes[0,0].axvline(single_result.x, color='red', linestyle='--', label=f'æœ€ä¼˜è§£ x={single_result.x:.3f}')\n    axes[0,0].set_title('å•å˜é‡å‡½æ•°ä¼˜åŒ–')\n    axes[0,0].legend()\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # Rosenbrockå‡½æ•°ç­‰é«˜çº¿\n    x = np.linspace(-2, 2, 100)\n    y = np.linspace(-1, 3, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.zeros_like(X)\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            Z[i,j] = rosenbrock([X[i,j], Y[i,j]])\n    \n    contour = axes[0,1].contour(X, Y, Z, levels=50)\n    axes[0,1].clabel(contour, inline=True, fontsize=8)\n    axes[0,1].plot(multi_result.x[0], multi_result.x[1], 'ro', markersize=8, label='æœ€ä¼˜è§£')\n    axes[0,1].set_title('Rosenbrockå‡½æ•°ä¼˜åŒ–')\n    axes[0,1].legend()\n    \n    # çº¦æŸä¼˜åŒ–å¯è§†åŒ–\n    x_const = np.linspace(0, 2, 100)\n    y_const = np.linspace(0, 2, 100)\n    X_const, Y_const = np.meshgrid(x_const, y_const)\n    Z_const = objective([X_const, Y_const])\n    \n    axes[1,0].contourf(X_const, Y_const, Z_const, levels=20, alpha=0.6)\n    axes[1,0].contour(X_const, Y_const, Z_const, levels=10, colors='black', alpha=0.4)\n    \n    # ç»˜åˆ¶çº¦æŸæ¡ä»¶\n    y_constraint1 = 1 - x_const  # x + y = 1\n    y_constraint2 = x_const + 0.5  # x - y = -0.5\n    axes[1,0].plot(x_const, y_constraint1, 'r-', linewidth=2, label='x + y = 1')\n    axes[1,0].plot(x_const, y_constraint2, 'b-', linewidth=2, label='x - y = -0.5')\n    axes[1,0].fill_between(x_const, np.maximum(y_constraint1, y_constraint2), 2, alpha=0.3, color='gray')\n    \n    axes[1,0].plot(constrained_result.x[0], constrained_result.x[1], 'go', markersize=10, label='æœ€ä¼˜è§£')\n    axes[1,0].set_xlim(0, 2)\n    axes[1,0].set_ylim(0, 2)\n    axes[1,0].set_title('çº¦æŸä¼˜åŒ–')\n    axes[1,0].legend()\n    \n    # æ–¹ç¨‹æ±‚è§£å¯è§†åŒ–\n    theta = np.linspace(0, 2*np.pi, 100)\n    circle_x = np.cos(theta)\n    circle_y = np.sin(theta)\n    line_x = np.linspace(-1.5, 1.5, 100)\n    line_y = line_x - 0.5\n    \n    axes[1,1].plot(circle_x, circle_y, 'b-', label='xÂ² + yÂ² = 1')\n    axes[1,1].plot(line_x, line_y, 'r-', label='x - y = 0.5')\n    axes[1,1].plot(root_result.x[0], root_result.x[1], 'go', markersize=8, label='äº¤ç‚¹')\n    axes[1,1].set_xlim(-1.5, 1.5)\n    axes[1,1].set_ylim(-1.5, 1.5)\n    axes[1,1].set_title('æ–¹ç¨‹æ±‚è§£')\n    axes[1,1].legend()\n    axes[1,1].grid(True, alpha=0.3)\n    axes[1,1].set_aspect('equal')\n    \n    plt.tight_layout()\n    \n    # è¾“å‡ºå›¾è¡¨\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n    buf.seek(0)\n    chart_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    buf.close()\n    plt.close('all')\n    \n    final_result = {\n        \"type\": \"scipy_optimization\",\n        \"title\": \"SciPy ä¼˜åŒ–ä¸æ–¹ç¨‹æ±‚è§£\",\n        \"results\": results,\n        \"chart_preview\": chart_base64\n    }\n    print(json.dumps(final_result))\n\n# optimization_examples()\n```\n\n## ğŸ“ æ•°å€¼ç§¯åˆ†ä¸å¾®åˆ†æ–¹ç¨‹\n\n### ç§¯åˆ†è®¡ç®—\n```python\ndef integration_examples():\n    \"\"\"æ•°å€¼ç§¯åˆ†ç¤ºä¾‹\"\"\"\n    \n    from scipy import integrate\n    import numpy as np\n    \n    results = {}\n    \n    # 1. å®šç§¯åˆ†\n    def func1(x):\n        return np.exp(-x**2) * np.sin(x)\n    \n    integral1, error1 = integrate.quad(func1, 0, np.inf)\n    results['definite_integral'] = {\n        'integral_value': integral1,\n        'estimated_error': error1,\n        'function': 'e^(-xÂ²) * sin(x) from 0 to âˆ'\n    }\n    \n    # 2. äºŒé‡ç§¯åˆ†\n    def func2(x, y):\n        return np.exp(-x**2 - y**2)\n    \n    integral2, error2 = integrate.dblquad(func2, -np.inf, np.inf, lambda x: -np.inf, lambda x: np.inf)\n    results['double_integral'] = {\n        'integral_value': integral2,\n        'estimated_error': error2,\n        'function': 'e^(-xÂ²-yÂ²) over entire plane'\n    }\n    \n    # 3. å¸¸å¾®åˆ†æ–¹ç¨‹æ±‚è§£\n    def ode_system(t, y):\n        \"\"\"Lotka-Volterra æ•é£Ÿè€…-è¢«æ•é£Ÿè€…æ¨¡å‹\"\"\"\n        alpha, beta, delta, gamma = 1.0, 0.1, 0.075, 1.5\n        prey, predator = y\n        dprey_dt = alpha * prey - beta * prey * predator\n        dpredator_dt = delta * prey * predator - gamma * predator\n        return [dprey_dt, dpredator_dt]\n    \n    t_span = (0, 50)\n    y0 = [10, 5]  # åˆå§‹ç§ç¾¤æ•°é‡\n    t_eval = np.linspace(0, 50, 1000)\n    \n    solution = integrate.solve_ivp(ode_system, t_span, y0, t_eval=t_eval, method='RK45')\n    results['ode_solution'] = {\n        'time_points': len(solution.t),\n        'final_prey': solution.y[0, -1],\n        'final_predator': solution.y[1, -1],\n        'success': solution.success\n    }\n    \n    # å¯è§†åŒ–ç»“æœ\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # ç§¯åˆ†å‡½æ•°å¯è§†åŒ–\n    x_plot = np.linspace(0, 3, 100)\n    y_plot = func1(x_plot)\n    axes[0].plot(x_plot, y_plot, 'b-', linewidth=2, label='è¢«ç§¯å‡½æ•°')\n    axes[0].fill_between(x_plot, y_plot, alpha=0.3)\n    axes[0].set_xlabel('x')\n    axes[0].set_ylabel('f(x)')\n    axes[0].set_title('å®šç§¯åˆ†: e^(-xÂ²) * sin(x)')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # å¾®åˆ†æ–¹ç¨‹è§£å¯è§†åŒ–\n    axes[1].plot(solution.t, solution.y[0], 'g-', label='è¢«æ•é£Ÿè€…')\n    axes[1].plot(solution.t, solution.y[1], 'r-', label='æ•é£Ÿè€…')\n    axes[1].set_xlabel('æ—¶é—´')\n    axes[1].set_ylabel('ç§ç¾¤æ•°é‡')\n    axes[1].set_title('Lotka-Volterra æ¨¡å‹')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # è¾“å‡ºå›¾è¡¨\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n    buf.seek(0)\n    chart_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    buf.close()\n    plt.close('all')\n    \n    final_result = {\n        \"type\": \"scipy_integration\",\n        \"title\": \"SciPy æ•°å€¼ç§¯åˆ†ä¸å¾®åˆ†æ–¹ç¨‹\",\n        \"results\": results,\n        \"chart_preview\": chart_base64\n    }\n    print(json.dumps(final_result))\n\n# integration_examples()\n```\n\n## ğŸ“¡ ä¿¡å·å¤„ç†ä¸é¢‘è°±åˆ†æ\n\n### ä¿¡å·å¤„ç†\n```python\ndef signal_processing_examples():\n    \"\"\"ä¿¡å·å¤„ç†ä¸é¢‘è°±åˆ†æç¤ºä¾‹\"\"\"\n    \n    from scipy import signal\n    from scipy.fft import fft, fftfreq\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    results = {}\n    \n    # 1. ä¿¡å·ç”Ÿæˆ\n    t = np.linspace(0, 1, 1000, endpoint=False)\n    # åˆ›å»ºåŒ…å«å¤šä¸ªé¢‘ç‡æˆåˆ†çš„ä¿¡å·\n    original_signal = (np.sin(2 * np.pi * 5 * t) + \n                      0.5 * np.sin(2 * np.pi * 20 * t) + \n                      0.2 * np.sin(2 * np.pi * 50 * t))\n    \n    # æ·»åŠ å™ªå£°\n    noisy_signal = original_signal + 0.3 * np.random.normal(size=len(t))\n    \n    # 2. æ»¤æ³¢å™¨è®¾è®¡\n    # ä½é€šæ»¤æ³¢å™¨ï¼Œæˆªæ­¢é¢‘ç‡15Hz\n    nyquist = 500  # é‡‡æ ·é¢‘ç‡1000Hzï¼Œå¥ˆå¥æ–¯ç‰¹é¢‘ç‡500Hz\n    cutoff = 15 / nyquist  # å½’ä¸€åŒ–æˆªæ­¢é¢‘ç‡\n    b, a = signal.butter(4, cutoff, btype='low')\n    filtered_signal = signal.filtfilt(b, a, noisy_signal)\n    \n    # 3. é¢‘è°±åˆ†æ\n    fft_original = fft(original_signal)\n    fft_noisy = fft(noisy_signal)\n    fft_filtered = fft(filtered_signal)\n    freqs = fftfreq(len(t), t[1] - t[0])\n    \n    # æ‰¾åˆ°ä¸»è¦é¢‘ç‡æˆåˆ†\n    positive_freq_idx = np.where(freqs > 0)\n    original_peaks = []\n    noisy_peaks = []\n    filtered_peaks = []\n    \n    for i in range(3):  # æ‰¾å‰3ä¸ªä¸»è¦é¢‘ç‡\n        original_peak_idx = np.argmax(np.abs(fft_original[positive_freq_idx]))\n        original_peak_freq = freqs[positive_freq_idx][original_peak_idx]\n        original_peaks.append(original_peak_freq)\n        \n        noisy_peak_idx = np.argmax(np.abs(fft_noisy[positive_freq_idx]))\n        noisy_peak_freq = freqs[positive_freq_idx][noisy_peak_idx]\n        noisy_peaks.append(noisy_peak_freq)\n        \n        filtered_peak_idx = np.argmax(np.abs(fft_filtered[positive_freq_idx]))\n        filtered_peak_freq = freqs[positive_freq_idx][filtered_peak_idx]\n        filtered_peaks.append(filtered_peak_freq)\n    \n    results['signal_analysis'] = {\n        'original_frequencies': original_peaks,\n        'noisy_frequencies': noisy_peaks,\n        'filtered_frequencies': filtered_peaks,\n        'expected_frequencies': [5, 20, 50]\n    }\n    \n    # 4. è°±å›¾åˆ†æ\n    f, t_spec, Sxx = signal.spectrogram(original_signal, fs=1000, nperseg=100)\n    \n    results['spectrogram'] = {\n        'frequency_bins': len(f),\n        'time_segments': len(t_spec),\n        'spectral_shape': Sxx.shape\n    }\n    \n    # å¯è§†åŒ–ç»“æœ\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # æ—¶åŸŸä¿¡å·\n    axes[0,0].plot(t, original_signal, 'b-', alpha=0.7, label='åŸå§‹ä¿¡å·')\n    axes[0,0].plot(t, noisy_signal, 'r-', alpha=0.5, label='å¸¦å™ªå£°ä¿¡å·')\n    axes[0,0].plot(t, filtered_signal, 'g-', linewidth=2, label='æ»¤æ³¢åä¿¡å·')\n    axes[0,0].set_xlabel('æ—¶é—´ (s)')\n    axes[0,0].set_ylabel('å¹…åº¦')\n    axes[0,0].set_title('æ—¶åŸŸä¿¡å·')\n    axes[0,0].legend()\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # é¢‘åŸŸåˆ†æ\n    axes[0,1].plot(freqs[positive_freq_idx], np.abs(fft_original[positive_freq_idx]), 'b-', label='åŸå§‹é¢‘è°±')\n    axes[0,1].plot(freqs[positive_freq_idx], np.abs(fft_noisy[positive_freq_idx]), 'r-', alpha=0.5, label='å™ªå£°é¢‘è°±')\n    axes[0,1].plot(freqs[positive_freq_idx], np.abs(fft_filtered[positive_freq_idx]), 'g-', label='æ»¤æ³¢é¢‘è°±')\n    axes[0,1].set_xlabel('é¢‘ç‡ (Hz)')\n    axes[0,1].set_ylabel('å¹…åº¦')\n    axes[0,1].set_title('é¢‘åŸŸåˆ†æ')\n    axes[0,1].legend()\n    axes[0,1].grid(True, alpha=0.3)\n    axes[0,1].set_xlim(0, 100)\n    \n    # æ»¤æ³¢å™¨é¢‘ç‡å“åº”\n    w, h = signal.freqz(b, a)\n    axes[1,0].plot(w * nyquist / np.pi, 20 * np.log10(np.abs(h)), 'b-')\n    axes[1,0].axvline(15, color='red', linestyle='--', label='æˆªæ­¢é¢‘ç‡ 15Hz')\n    axes[1,0].set_xlabel('é¢‘ç‡ (Hz)')\n    axes[1,0].set_ylabel('å¹…åº¦ (dB)')\n    axes[1,0].set_title('æ»¤æ³¢å™¨é¢‘ç‡å“åº”')\n    axes[1,0].legend()\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # è°±å›¾\n    im = axes[1,1].pcolormesh(t_spec, f, 10 * np.log10(Sxx), shading='gouraud')\n    plt.colorbar(im, ax=axes[1,1], label='åŠŸç‡è°±å¯†åº¦ (dB)')\n    axes[1,1].set_xlabel('æ—¶é—´ (s)')\n    axes[1,1].set_ylabel('é¢‘ç‡ (Hz)')\n    axes[1,1].set_title('è°±å›¾åˆ†æ')\n    \n    plt.tight_layout()\n    \n    # è¾“å‡ºå›¾è¡¨\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n    buf.seek(0)\n    chart_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    buf.close()\n    plt.close('all')\n    \n    final_result = {\n        \"type\": \"scipy_signal_processing\",\n        \"title\": \"SciPy ä¿¡å·å¤„ç†ä¸é¢‘è°±åˆ†æ\",\n        \"results\": results,\n        \"chart_preview\": chart_base64\n    }\n    print(json.dumps(final_result))\n\n# signal_processing_examples()\n```\n\n## ğŸ§® çº¿æ€§ä»£æ•°ä¸ç©ºé—´ç®—æ³•\n\n### çº¿æ€§ä»£æ•°è¿ç®—\n```python\ndef linear_algebra_examples():\n    \"\"\"çº¿æ€§ä»£æ•°ä¸ç©ºé—´ç®—æ³•ç¤ºä¾‹\"\"\"\n    \n    from scipy import linalg, spatial\n    import numpy as np\n    \n    results = {}\n    \n    # 1. çŸ©é˜µè¿ç®—\n    A = np.array([[4, 2, 1], [2, 5, 3], [1, 3, 6]])\n    b = np.array([1, 2, 3])\n    \n    # çŸ©é˜µåˆ†è§£\n    lu, piv = linalg.lu_factor(A)\n    x_lu = linalg.lu_solve((lu, piv), b)\n    \n    # ç‰¹å¾å€¼åˆ†è§£\n    eigenvalues, eigenvectors = linalg.eig(A)\n    \n    # çŸ©é˜µæ±‚é€†\n    A_inv = linalg.inv(A)\n    \n    results['linear_algebra'] = {\n        'matrix_shape': A.shape,\n        'determinant': linalg.det(A),\n        'condition_number': linalg.cond(A),\n        'lu_solution': x_lu.tolist(),\n        'eigenvalues': eigenvalues.tolist(),\n        'matrix_invertible': not np.allclose(A_inv, 0)\n    }\n    \n    # 2. ç©ºé—´ç®—æ³• - è·ç¦»è®¡ç®—\n    points = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [0, 3]])\n    \n    # è®¡ç®—è·ç¦»çŸ©é˜µ\n    distance_matrix = spatial.distance_matrix(points, points)\n    \n    # æœ€è¿‘é‚»æœç´¢\n    tree = spatial.KDTree(points)\n    distances, indices = tree.query(points, k=2)  # æ¯ä¸ªç‚¹æ‰¾2ä¸ªæœ€è¿‘é‚»ï¼ˆåŒ…æ‹¬è‡ªèº«ï¼‰\n    \n    # å‡¸åŒ…è®¡ç®—\n    if len(points) >= 3:\n        hull = spatial.ConvexHull(points)\n        hull_volume = hull.volume\n        hull_area = hull.area\n    else:\n        hull_volume = 0\n        hull_area = 0\n    \n    results['spatial_algorithms'] = {\n        'num_points': len(points),\n        'distance_matrix_shape': distance_matrix.shape,\n        'nearest_neighbors': indices.tolist(),\n        'convex_hull_volume': hull_volume,\n        'convex_hull_area': hull_area\n    }\n    \n    # 3. æ’å€¼æ–¹æ³•\n    from scipy import interpolate\n    \n    x_known = np.linspace(0, 10, 10)\n    y_known = np.sin(x_known) + 0.1 * np.random.normal(size=len(x_known))\n    \n    # æ ·æ¡æ’å€¼\n    spline = interpolate.UnivariateSpline(x_known, y_known, s=0)\n    x_fine = np.linspace(0, 10, 100)\n    y_spline = spline(x_fine)\n    \n    # çº¿æ€§æ’å€¼\n    linear_interp = interpolate.interp1d(x_known, y_known, kind='linear')\n    y_linear = linear_interp(x_fine)\n    \n    results['interpolation'] = {\n        'known_points': len(x_known),\n        'interpolated_points': len(x_fine),\n        'spline_accuracy': np.mean((y_spline - np.sin(x_fine))**2),\n        'linear_accuracy': np.mean((y_linear - np.sin(x_fine))**2)\n    }\n    \n    # å¯è§†åŒ–ç»“æœ\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # çŸ©é˜µç‰¹å¾å‘é‡å¯è§†åŒ–\n    for i in range(min(3, len(eigenvalues))):\n        axes[0,0].arrow(0, 0, eigenvectors[0,i].real, eigenvectors[1,i].real, \n                       head_width=0.1, head_length=0.1, fc='blue', ec='blue',\n                       label=f'ç‰¹å¾å‘é‡ {i+1}' if i == 0 else \"\")\n    axes[0,0].set_xlim(-1, 1)\n    axes[0,0].set_ylim(-1, 1)\n    axes[0,0].set_title('çŸ©é˜µç‰¹å¾å‘é‡')\n    axes[0,0].legend()\n    axes[0,0].grid(True, alpha=0.3)\n    axes[0,0].set_aspect('equal')\n    \n    # ç©ºé—´ç‚¹ä¸å‡¸åŒ…\n    axes[0,1].scatter(points[:,0], points[:,1], c='red', s=50, label='æ•°æ®ç‚¹')\n    if len(points) >= 3:\n        for simplex in hull.simplices:\n            axes[0,1].plot(points[simplex, 0], points[simplex, 1], 'b-', linewidth=2)\n    axes[0,1].set_title('ç©ºé—´ç‚¹ä¸å‡¸åŒ…')\n    axes[0,1].legend()\n    axes[0,1].grid(True, alpha=0.3)\n    axes[0,1].set_aspect('equal')\n    \n    # æ’å€¼æ¯”è¾ƒ\n    axes[1,0].plot(x_known, y_known, 'ro', markersize=8, label='å·²çŸ¥ç‚¹')\n    axes[1,0].plot(x_fine, np.sin(x_fine), 'k-', alpha=0.5, label='çœŸå®å‡½æ•°')\n    axes[1,0].plot(x_fine, y_spline, 'b-', label='æ ·æ¡æ’å€¼')\n    axes[1,0].plot(x_fine, y_linear, 'g--', label='çº¿æ€§æ’å€¼')\n    axes[1,0].set_xlabel('x')\n    axes[1,0].set_ylabel('y')\n    axes[1,0].set_title('æ’å€¼æ–¹æ³•æ¯”è¾ƒ')\n    axes[1,0].legend()\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # è·ç¦»çŸ©é˜µçƒ­åŠ›å›¾\n    im = axes[1,1].imshow(distance_matrix, cmap='viridis', interpolation='nearest')\n    plt.colorbar(im, ax=axes[1,1], label='è·ç¦»')\n    axes[1,1].set_title('ç‚¹é—´è·ç¦»çŸ©é˜µ')\n    axes[1,1].set_xlabel('ç‚¹ç´¢å¼•')\n    axes[1,1].set_ylabel('ç‚¹ç´¢å¼•')\n    \n    plt.tight_layout()\n    \n    # è¾“å‡ºå›¾è¡¨\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n    buf.seek(0)\n    chart_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    buf.close()\n    plt.close('all')\n    \n    final_result = {\n        \"type\": \"scipy_linear_algebra\",\n        \"title\": \"SciPy çº¿æ€§ä»£æ•°ä¸ç©ºé—´ç®—æ³•\",\n        \"results\": results,\n        \"chart_preview\": chart_base64\n    }\n    print(json.dumps(final_result))\n\n# linear_algebra_examples()\n```\n\nè¿™ä¸ª SciPy cookbook æ–‡ä»¶æä¾›äº†ä»åŸºç¡€ç§‘å­¦è®¡ç®—åˆ°é«˜çº§ç®—æ³•åº”ç”¨çš„å®Œæ•´æŒ‡å—ï¼Œæ¶µç›–äº†ä¼˜åŒ–ã€ç§¯åˆ†ã€ä¿¡å·å¤„ç†ã€çº¿æ€§ä»£æ•°å’Œç©ºé—´ç®—æ³•ç­‰æ ¸å¿ƒé¢†åŸŸã€‚",
        "sympy_cookbook.md": "# SymPy ç¬¦å·æ•°å­¦èœè°±\n\n## ğŸ§® åŸºç¡€ç¬¦å·è¿ç®—\n\n### ç¬¦å·å®šä¹‰ä¸åŸºæœ¬è¿ç®—\n```python\nimport sympy as sp\nimport numpy as np\nimport json\n\ndef basic_symbolic_operations():\n    \"\"\"åŸºç¡€ç¬¦å·è¿ç®—ç¤ºä¾‹\"\"\"\n    \n    # å®šä¹‰ç¬¦å·\n    x, y, z = sp.symbols('x y z')\n    a, b, c = sp.symbols('a b c')\n    \n    # åŸºæœ¬è¡¨è¾¾å¼\n    expr1 = x**2 + 2*x + 1\n    expr2 = (x + 1)**2\n    \n    # è¡¨è¾¾å¼ç®€åŒ–\n    simplified = sp.simplify(expr1 - expr2)\n    \n    # ç»“æœè¾“å‡º\n    result = {\n        \"type\": \"symbolic_math\",\n        \"title\": \"åŸºç¡€ç¬¦å·è¿ç®—\",\n        \"operations\": {\n            \"expression_1\": str(expr1),\n            \"expression_2\": str(expr2),\n            \"simplified_difference\": str(simplified),\n            \"are_equal\": str(expr1.equals(expr2))\n        }\n    }\n    print(json.dumps(result))\n\n# basic_symbolic_operations()\n```\n\n## ğŸ¯ æ–¹ç¨‹æ±‚è§£å·¥ä½œæµ\n\n### ä»£æ•°æ–¹ç¨‹æ±‚è§£\n```python\ndef equation_solving_workflow():\n    \"\"\"å®Œæ•´çš„æ–¹ç¨‹æ±‚è§£å·¥ä½œæµ\"\"\"\n    \n    # å®šä¹‰ç¬¦å·\n    x, y, z = sp.symbols('x y z')\n    \n    # 1. ä¸€å…ƒæ–¹ç¨‹\n    equation1 = sp.Eq(x**2 - 5*x + 6, 0)\n    solution1 = sp.solve(equation1, x)\n    \n    # 2. å¤šå…ƒæ–¹ç¨‹ç»„\n    equations = [\n        sp.Eq(2*x + 3*y, 7),\n        sp.Eq(4*x - y, 1)\n    ]\n    solution2 = sp.solve(equations, (x, y))\n    \n    # 3. éçº¿æ€§æ–¹ç¨‹\n    equation3 = sp.Eq(sp.sin(x) - x/2, 0)\n    solution3 = sp.nsolve(equation3, x, 1)  # æ•°å€¼è§£\n    \n    result = {\n        \"type\": \"equation_solutions\",\n        \"title\": \"æ–¹ç¨‹æ±‚è§£ç»“æœ\",\n        \"solutions\": {\n            \"quadratic_equation\": {\n                \"equation\": str(equation1),\n                \"solutions\": [str(sol) for sol in solution1]\n            },\n            \"linear_system\": {\n                \"equations\": [str(eq) for eq in equations],\n                \"solutions\": {str(k): float(v) for k, v in solution2.items()}\n            },\n            \"nonlinear_equation\": {\n                \"equation\": str(equation3),\n                \"numerical_solution\": float(solution3)\n            }\n        }\n    }\n    print(json.dumps(result))\n\n# equation_solving_workflow()\n```\n\n## ğŸ“ å¾®ç§¯åˆ†è¿ç®—\n\n### å¾®åˆ†ä¸ç§¯åˆ†\n```python\ndef calculus_operations():\n    \"\"\"å¾®ç§¯åˆ†è¿ç®—ç¤ºä¾‹\"\"\"\n    \n    x = sp.symbols('x')\n    \n    # 1. å¯¼æ•°è®¡ç®—\n    f = x**3 + 2*x**2 + sp.sin(x)\n    derivative = sp.diff(f, x)\n    second_derivative = sp.diff(f, x, 2)\n    \n    # 2. ç§¯åˆ†è®¡ç®—\n    indefinite_integral = sp.integrate(f, x)\n    definite_integral = sp.integrate(f, (x, 0, sp.pi))\n    \n    # 3. æé™è®¡ç®—\n    limit_expr = (sp.sin(x) - x) / x**3\n    limit_result = sp.limit(limit_expr, x, 0)\n    \n    result = {\n        \"type\": \"calculus_results\",\n        \"title\": \"å¾®ç§¯åˆ†è¿ç®—ç»“æœ\",\n        \"operations\": {\n            \"function\": str(f),\n            \"first_derivative\": str(derivative),\n            \"second_derivative\": str(second_derivative),\n            \"indefinite_integral\": str(indefinite_integral),\n            \"definite_integral_0_to_pi\": float(definite_integral),\n            \"limit_sin_x_minus_x_over_x_cubed\": float(limit_result)\n        }\n    }\n    print(json.dumps(result))\n\n# calculus_operations()\n```\n\n## ğŸ” å…¬å¼è¯æ˜å·¥ä½œæµ\n\n### æ•°å­¦å…¬å¼è¯æ˜\n```python\ndef formula_proof_workflow():\n    \"\"\"æ•°å­¦å…¬å¼è¯æ˜å·¥ä½œæµ\"\"\"\n    \n    # å®šä¹‰ç¬¦å·\n    a, b, c, x = sp.symbols('a b c x')\n    \n    proofs = []\n    \n    # 1. è¯æ˜ (a+b)^2 = a^2 + 2ab + b^2\n    lhs1 = (a + b)**2\n    rhs1 = a**2 + 2*a*b + b**2\n    proof1 = sp.simplify(lhs1 - rhs1) == 0\n    \n    proofs.append({\n        \"theorem\": \"(a + b)Â² = aÂ² + 2ab + bÂ²\",\n        \"lhs\": str(lhs1),\n        \"rhs\": str(rhs1),\n        \"proof\": \"ç›´æ¥å±•å¼€éªŒè¯\",\n        \"verified\": bool(proof1)\n    })\n    \n    # 2. è¯æ˜ä¸‰è§’æ’ç­‰å¼ sinÂ²x + cosÂ²x = 1\n    lhs2 = sp.sin(x)**2 + sp.cos(x)**2\n    rhs2 = 1\n    proof2 = sp.simplify(lhs2 - rhs2) == 0\n    \n    proofs.append({\n        \"theorem\": \"sinÂ²x + cosÂ²x = 1\",\n        \"lhs\": str(lhs2),\n        \"rhs\": str(rhs2),\n        \"proof\": \"ä½¿ç”¨ä¸‰è§’æ’ç­‰å¼çš„å®šä¹‰\",\n        \"verified\": bool(proof2)\n    })\n    \n    # 3. è¯æ˜äºŒæ¬¡æ–¹ç¨‹æ±‚æ ¹å…¬å¼\n    # æ–¹ç¨‹: axÂ² + bx + c = 0\n    equation = sp.Eq(a*x**2 + b*x + c, 0)\n    solutions = sp.solve(equation, x)\n    \n    proofs.append({\n        \"theorem\": \"äºŒæ¬¡æ–¹ç¨‹æ±‚æ ¹å…¬å¼\",\n        \"equation\": str(equation),\n        \"solutions\": [str(sol) for sol in solutions],\n        \"proof\": \"é€šè¿‡é…æ–¹æ³•æ±‚è§£\",\n        \"verified\": True\n    })\n    \n    result = {\n        \"type\": \"mathematical_proofs\",\n        \"title\": \"æ•°å­¦å…¬å¼è¯æ˜\",\n        \"proofs\": proofs\n    }\n    print(json.dumps(result))\n\n# formula_proof_workflow()\n```\n\n## ğŸ§© çŸ©é˜µä¸çº¿æ€§ä»£æ•°\n\n### çŸ©é˜µè¿ç®—\n```python\ndef linear_algebra_operations():\n    \"\"\"çº¿æ€§ä»£æ•°è¿ç®—ç¤ºä¾‹\"\"\"\n    \n    # å®šä¹‰ç¬¦å·çŸ©é˜µ\n    A = sp.Matrix([[1, 2], [3, 4]])\n    B = sp.Matrix([[2, 0], [1, 2]])\n    \n    # åŸºæœ¬çŸ©é˜µè¿ç®—\n    matrix_sum = A + B\n    matrix_product = A * B\n    determinant_A = A.det()\n    inverse_A = A.inv()\n    eigenvalues_A = A.eigenvals()\n    \n    # è§£çº¿æ€§æ–¹ç¨‹ç»„\n    x1, x2 = sp.symbols('x1 x2')\n    equations = [\n        sp.Eq(2*x1 + 3*x2, 7),\n        sp.Eq(4*x1 + 5*x2, 13)\n    ]\n    solution = sp.solve(equations, (x1, x2))\n    \n    result = {\n        \"type\": \"linear_algebra\",\n        \"title\": \"çº¿æ€§ä»£æ•°è¿ç®—ç»“æœ\",\n        \"matrix_operations\": {\n            \"matrix_A\": str(A.tolist()),\n            \"matrix_B\": str(B.tolist()),\n            \"A_plus_B\": str(matrix_sum.tolist()),\n            \"A_times_B\": str(matrix_product.tolist()),\n            \"determinant_A\": float(determinant_A),\n            \"inverse_A\": str(inverse_A.tolist()),\n            \"eigenvalues_A\": {str(k): int(v) for k, v in eigenvalues_A.items()}\n        },\n        \"linear_system\": {\n            \"equations\": [str(eq) for eq in equations],\n            \"solution\": {str(k): float(v) for k, v in solution.items()}\n        }\n    }\n    print(json.dumps(result))\n\n# linear_algebra_operations()\n```\n\n## ğŸ“ˆ æ•°å€¼è®¡ç®—ä¸è¿‘ä¼¼\n\n### ç¬¦å·è®¡ç®—ä¸æ•°å€¼è¿‘ä¼¼\n```python\ndef numerical_approximations():\n    \"\"\"æ•°å€¼è®¡ç®—ä¸è¿‘ä¼¼\"\"\"\n    \n    x = sp.symbols('x')\n    \n    # 1. çº§æ•°å±•å¼€\n    sin_taylor = sp.sin(x).series(x, 0, 6)  # 6é˜¶æ³°å‹’å±•å¼€\n    exp_taylor = sp.exp(x).series(x, 0, 5)  # 5é˜¶æ³°å‹’å±•å¼€\n    \n    # 2. æ•°å€¼è¿‘ä¼¼\n    pi_approx = sp.N(sp.pi, 10)  # Ï€çš„10ä½ç²¾åº¦è¿‘ä¼¼\n    e_approx = sp.N(sp.E, 8)     # eçš„8ä½ç²¾åº¦è¿‘ä¼¼\n    \n    # 3. æ•°å€¼ç§¯åˆ†\n    numerical_integral = sp.N(sp.integrate(sp.sin(x), (x, 0, sp.pi/2)))\n    \n    # 4. æ•°å€¼æ±‚è§£æ–¹ç¨‹\n    equation = sp.Eq(x**3 - 2*x - 5, 0)\n    numerical_solution = sp.nsolve(equation, x, 2)  # ä»x=2å¼€å§‹æ±‚è§£\n    \n    result = {\n        \"type\": \"numerical_approximations\",\n        \"title\": \"æ•°å€¼è®¡ç®—ä¸è¿‘ä¼¼ç»“æœ\",\n        \"approximations\": {\n            \"sin_taylor_series\": str(sin_taylor),\n            \"exp_taylor_series\": str(exp_taylor),\n            \"pi_approximation\": float(pi_approx),\n            \"e_approximation\": float(e_approx),\n            \"numerical_integral_sin_0_to_pi_2\": float(numerical_integral),\n            \"equation_solution\": float(numerical_solution)\n        }\n    }\n    print(json.dumps(result))\n\n# numerical_approximations()\n```\n\n## ğŸ“ å¤æ‚æ•°å­¦é—®é¢˜è§£å†³\n\n### ç»¼åˆæ•°å­¦é—®é¢˜\n```python\ndef complex_math_problem():\n    \"\"\"è§£å†³å¤æ‚æ•°å­¦é—®é¢˜\"\"\"\n    \n    x, y, z = sp.symbols('x y z')\n    \n    # é—®é¢˜1: æ±‚å‡½æ•°æå€¼\n    f = x**3 - 6*x**2 + 9*x + 1\n    critical_points = sp.solve(sp.diff(f, x), x)\n    \n    # è®¡ç®—äºŒé˜¶å¯¼æ•°åˆ¤æ–­æå€¼ç±»å‹\n    second_deriv = sp.diff(f, x, 2)\n    extremum_types = {}\n    for point in critical_points:\n        second_deriv_val = second_deriv.subs(x, point)\n        if second_deriv_val > 0:\n            extremum_types[float(point)] = \"å±€éƒ¨æå°å€¼\"\n        elif second_deriv_val < 0:\n            extremum_types[float(point)] = \"å±€éƒ¨æå¤§å€¼\"\n        else:\n            extremum_types[float(point)] = \"æ‹ç‚¹\"\n    \n    # é—®é¢˜2: æ›²çº¿é•¿åº¦è®¡ç®—\n    curve_length = sp.integrate(sp.sqrt(1 + sp.diff(f, x)**2), (x, 0, 3))\n    \n    # é—®é¢˜3: æ—‹è½¬ä½“ä½“ç§¯\n    volume = sp.pi * sp.integrate(f**2, (x, 0, 3))\n    \n    result = {\n        \"type\": \"complex_math_solution\",\n        \"title\": \"å¤æ‚æ•°å­¦é—®é¢˜è§£å†³æ–¹æ¡ˆ\",\n        \"solutions\": {\n            \"function_analysis\": {\n                \"function\": str(f),\n                \"critical_points\": [float(p) for p in critical_points],\n                \"extremum_types\": extremum_types\n            },\n            \"curve_properties\": {\n                \"curve_length_0_to_3\": float(sp.N(curve_length)),\n                \"volume_of_revolution\": float(sp.N(volume))\n            }\n        }\n    }\n    print(json.dumps(result))\n\n# complex_math_problem()\n```\n\n## ğŸ’¡ æ•°å­¦è¯æ˜ç­–ç•¥\n\n### è‡ªåŠ¨è¯æ˜æ¡†æ¶\n```python\ndef automated_proof_framework(expression1, expression2, proof_method=\"simplify\"):\n    \"\"\"\n    è‡ªåŠ¨è¯æ˜æ¡†æ¶\n    proof_method: \"simplify\", \"expand\", \"factor\", \"trigsimp\"\n    \"\"\"\n    \n    x, y = sp.symbols('x y')\n    \n    # æ ¹æ®è¯æ˜æ–¹æ³•é€‰æ‹©ç­–ç•¥\n    if proof_method == \"simplify\":\n        proof = sp.simplify(expression1 - expression2)\n    elif proof_method == \"expand\":\n        proof = sp.expand(expression1 - expression2)\n    elif proof_method == \"factor\":\n        proof = sp.factor(expression1 - expression2)\n    elif proof_method == \"trigsimp\":\n        proof = sp.trigsimp(expression1 - expression2)\n    else:\n        proof = expression1 - expression2\n    \n    is_proven = (proof == 0)\n    \n    return {\n        \"expression1\": str(expression1),\n        \"expression2\": str(expression2),\n        \"proof_method\": proof_method,\n        \"proof_steps\": str(proof),\n        \"is_proven\": bool(is_proven)\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# expr1 = (x + y)**2\n# expr2 = x**2 + 2*x*y + y**2\n# result = automated_proof_framework(expr1, expr2, \"expand\")\n```\n\nè¿™ä¸ªSymPyèœè°±æ–‡ä»¶æä¾›äº†ä»åŸºç¡€ç¬¦å·è¿ç®—åˆ°å¤æ‚æ•°å­¦è¯æ˜çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œæ˜¯è§£å†³æ•°å­¦é—®é¢˜çš„å¼ºå¤§å·¥å…·ã€‚"
      }
    },
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\python_sandbox",
    "lastUpdated": "2025-11-16T07:45:22.499Z"
  },
  "stockfish_analyzer": {
    "metadata": {
      "name": "stockfish_analyzer",
      "description": "å›½é™…è±¡æ£‹å¼•æ“åˆ†æå·¥å…·ï¼Œæä¾›æœ€ä½³èµ°æ³•æ¨èã€å±€é¢è¯„ä¼°å’Œå¤šç§èµ°æ³•é€‰æ‹©åˆ†æã€‚æ”¯æŒFENå­—ç¬¦ä¸²ç›´æ¥è¾“å…¥åˆ†æã€‚",
      "tool_name": "stockfish_analyzer",
      "category": "chess",
      "priority": 6,
      "tags": [
        "chess",
        "analysis",
        "game",
        "strategy",
        "evaluation",
        "FEN",
        "SAN",
        "position",
        "move",
        "best-move",
        "top-moves",
        "chess-engine",
        "stockfish",
        "board",
        "æ£‹å±€",
        "èµ°æ³•",
        "è¯„ä¼°",
        "å±€é¢"
      ],
      "version": 1.1
    },
    "content": "# å›½é™…è±¡æ£‹AIåŠ©æ•™æŒ‡å—\n\nä½ æ˜¯ä¸€ä½é¡¶çº§çš„å›½é™…è±¡æ£‹AIåŠ©æ•™ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ä½œä¸ºç”¨æˆ·å’Œå¼ºå¤§çš„ \"stockfish_analyzer\" å·¥å…·ä¹‹é—´çš„æ™ºèƒ½æ¡¥æ¢ã€‚ä½  **ä¸è‡ªå·±ä¸‹æ£‹**ï¼Œè€Œæ˜¯ **è°ƒç”¨å·¥å…·** å¹¶ **è§£é‡Šç»“æœ**ã€‚\n\n## ğŸ¯ æ ¸å¿ƒå·¥ä½œæµç¨‹\n\n### 1. **è¯†åˆ«FENå­—ç¬¦ä¸²å’Œç”¨æˆ·æ„å›¾**\n- **FENå­—ç¬¦ä¸²ç‰¹å¾**: è¯†åˆ«å¦‚ `rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1` æ ¼å¼çš„å­—ç¬¦ä¸²\n- **è‡ªåŠ¨è§¦å‘**: å½“æ£€æµ‹åˆ°æœ‰æ•ˆçš„FENå­—ç¬¦ä¸²æ—¶ï¼Œè‡ªåŠ¨è°ƒç”¨åˆ†æå·¥å…·\n- **æ„å›¾åˆ†æ**: æ ¹æ®ç”¨æˆ·é—®é¢˜é€‰æ‹©åˆé€‚æ¨¡å¼ï¼š\n  - **æœ€ä½³èµ°æ³•**: \"æˆ‘è¯¥æ€ä¹ˆèµ°ï¼Ÿ\"ã€\"æœ€ä½³èµ°æ³•\"ã€\"ä¸‹ä¸€æ­¥\" â†’ `get_best_move`\n  - **å¤šç§é€‰æ‹©**: \"å‰ä¸‰æ­¥æ¨è\"ã€\"æœ‰å“ªäº›é€‰æ‹©\"ã€\"å‡ ä¸ªå¥½èµ°æ³•\" â†’ `get_top_moves`\n  - **å±€é¢è¯„ä¼°**: \"è°ä¼˜åŠ¿\"ã€\"å±€é¢å¦‚ä½•\"ã€\"è¯„ä¼°\" â†’ `evaluate_position`\n\n### 2. **è°ƒç”¨æ­£ç¡®å·¥å…·**\næ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©å¯¹åº”çš„åˆ†ææ¨¡å¼ã€‚\n\n### 3. **è§£é‡Šå·¥å…·ç»“æœ**\nå°†ä¸“ä¸šçš„å¼•æ“è¾“å‡ºè½¬åŒ–ä¸ºæ˜“æ‡‚çš„æ•™å­¦è¯­è¨€ã€‚\n\n## ğŸ“‹ å¿«é€Ÿä½¿ç”¨æŒ‡å—\n\n### åœºæ™¯1ï¼šç›´æ¥FENåˆ†æ\n**ç”¨æˆ·è¾“å…¥**: `rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1`\n**è‡ªåŠ¨å“åº”**: åˆ†æåˆå§‹å±€é¢ï¼Œæä¾›æœ€ä½³èµ°æ³•å’Œè¯„ä¼°\n\n### åœºæ™¯2ï¼šFEN + ç®€å•æŒ‡ä»¤  \n**ç”¨æˆ·è¾“å…¥**: `r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3` å‰ä¸‰æ­¥æ¨è\n**å·¥å…·è°ƒç”¨**: `get_top_moves` with `top_n: 3`\n\n### åœºæ™¯3ï¼šå±€é¢è¯„ä¼°è¯·æ±‚\n**ç”¨æˆ·è¾“å…¥**: `r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3` ç°åœ¨è°ä¼˜åŠ¿ï¼Ÿ\n**å·¥å…·è°ƒç”¨**: `evaluate_position`\n\n## ğŸ”§ å·¥å…·è°ƒç”¨è§„èŒƒ\n\n**é‡è¦æç¤º**: å½“ä½ å†³å®šè°ƒç”¨ `stockfish_analyzer` å·¥å…·æ—¶ï¼Œä½ çš„æ€è€ƒè¿‡ç¨‹åº”è¯¥ç”Ÿæˆä¸€ä¸ªåŒ…å« `tool_name` å’Œ `parameters` å­—æ®µçš„JSONå¯¹è±¡ã€‚`parameters` å­—æ®µçš„å€¼å¿…é¡»ä¸¥æ ¼éµå®ˆå·¥å…·çš„è¾“å…¥æ¨¡å¼ã€‚\n\n### âœ… æ­£ç¡®çš„è°ƒç”¨ç»“æ„\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\",\n  \"parameters\": {\n    \"fen\": \"<FENå­—ç¬¦ä¸²>\",\n    \"mode\": \"<åŠŸèƒ½æ¨¡å¼>\",\n    \"options\": {\n      \"<é€‰é¡¹å>\": \"<é€‰é¡¹å€¼>\"\n    }\n  }\n}\n```\n\n### åŠŸèƒ½æ¨¡å¼è¯¦è§£\n\n#### 1. è·å–æœ€ä½³èµ°æ³• (`get_best_move`)\n**é€‚ç”¨åœºæ™¯**: ç”¨æˆ·è¯¢é—®\"æœ€ä½³èµ°æ³•\"ã€\"ä¸‹ä¸€æ­¥æ€ä¹ˆèµ°\"\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\",\n  \"parameters\": {\n    \"fen\": \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\",\n    \"mode\": \"get_best_move\"\n  }\n}\n```\n\n#### 2. è·å–å¤šä¸ªèµ°æ³•é€‰é¡¹ (`get_top_moves`)\n**é€‚ç”¨åœºæ™¯**: ç”¨æˆ·è¯¢é—®\"å‰ä¸‰æ­¥\"ã€\"æœ‰å“ªäº›é€‰æ‹©\"ã€\"å‡ ä¸ªå¥½èµ°æ³•\"\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\", \n  \"parameters\": {\n    \"fen\": \"r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3\",\n    \"mode\": \"get_top_moves\",\n    \"options\": {\n      \"top_n\": 3\n    }\n  }\n}\n```\n\n#### 3. è¯„ä¼°å±€é¢ (`evaluate_position`)\n**é€‚ç”¨åœºæ™¯**: ç”¨æˆ·è¯¢é—®\"å±€é¢å¦‚ä½•\"ã€\"è°ä¼˜åŠ¿\"ã€\"è¯„ä¼°ä¸€ä¸‹\"\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\",\n  \"parameters\": {\n    \"fen\": \"r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3\", \n    \"mode\": \"evaluate_position\"\n  }\n}\n```\n\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\n\n- **ç¼ºå°‘ `fen` å‚æ•°**: `{\"tool_name\": \"stockfish_analyzer\", \"parameters\": {\"mode\": \"get_best_move\"}}`\n- **é”™è¯¯çš„ `mode` åç§°**: `{\"tool_name\": \"stockfish_analyzer\", \"parameters\": {\"fen\": \"...\", \"mode\": \"best_move\"}}` (åº”ä¸º \"get_best_move\")\n- **options æ ¼å¼é”™è¯¯**: `{\"tool_name\": \"stockfish_analyzer\", \"parameters\": {\"fen\": \"...\", \"mode\": \"get_top_moves\", \"options\": 3}}` (options å¿…é¡»æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚ `{\"top_n\": 3}`)\n\n## ğŸ’¡ ç»“æœè§£é‡ŠæŒ‡å—\n\n### è¯„ä¼°åˆ†æ•°è§£é‡Š\n- **å…µå€¼ä¼˜åŠ¿**: `\"evaluation\": {\"type\": \"cp\", \"value\": 250}` â†’ \"ç™½æ–¹æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œç›¸å½“äºå¤š2.5ä¸ªå…µ\"\n- **è½»å¾®ä¼˜åŠ¿**: `\"evaluation\": {\"type\": \"cp\", \"value\": -120}` â†’ \"é»‘æ–¹ç¨å ä¼˜ï¼Œä¼˜åŠ¿çº¦1.2ä¸ªå…µ\"  \n- **å°†æ­»å±€é¢**: `\"evaluation\": {\"type\": \"mate\", \"value\": 3}` â†’ \"ç™½æ–¹3æ­¥å†…å¯å°†æ­»å¯¹æ–¹\"\n\n### èµ°æ³•è§£é‡Š\n- **UCIè½¬SAN**: `\"best_move\": \"g1f3\"` â†’ \"æœ€ä½³èµ°æ³•æ˜¯ **Nf3**\"\n- **æˆ˜ç•¥æ„å›¾**: è§£é‡Šèµ°æ³•çš„ç›®çš„å’Œæˆ˜ç•¥æ„ä¹‰\n- **å¤šèµ°æ³•æ¯”è¾ƒ**: å½“æœ‰å¤šä¸ªé€‰é¡¹æ—¶ï¼Œåˆ†æå„è‡ªçš„ä¼˜ç¼ºç‚¹\n\n## ğŸš€ æ™ºèƒ½è¯†åˆ«å¢å¼º\n\n### FENå­—ç¬¦ä¸²ç‰¹å¾è¯†åˆ«\n- **æ ¼å¼ç‰¹å¾**: åŒ…å« `/` åˆ†éš”çš„è¡Œã€`w`/`b` èµ°å­æ–¹ã€æ˜“ä½æƒåˆ©ç­‰\n- **è‡ªåŠ¨æ£€æµ‹**: æ£€æµ‹åˆ°FENæ ¼å¼æ—¶è‡ªåŠ¨è§¦å‘åˆ†æ\n- **å®¹é”™å¤„ç†**: å¤„ç†å¸¸è§çš„FENæ ¼å¼å˜ä½“\n\n### ç”¨æˆ·æ„å›¾å…³é”®è¯\n- **æœ€ä½³èµ°æ³•ç±»**: \"æœ€ä½³\"ã€\"æœ€å¥½\"ã€\"æ€ä¹ˆèµ°\"ã€\"ä¸‹ä¸€æ­¥\"\n- **å¤šé€‰é¡¹ç±»**: \"å‡ ä¸ª\"ã€\"å“ªäº›\"ã€\"é€‰æ‹©\"ã€\"æ¨è\"ã€\"å‰ä¸‰\"  \n- **è¯„ä¼°ç±»**: \"è¯„ä¼°\"ã€\"ä¼˜åŠ¿\"ã€\"å±€é¢\"ã€\"è°å¥½\"\n- **ä¸­è‹±æ–‡æ··åˆ**: æ”¯æŒä¸­æ–‡æŒ‡ä»¤å¦‚\"æ£‹å±€\"ã€\"èµ°æ³•\"ã€\"è¯„ä¼°\"\n\n## âš ï¸ å¸¸è§é—®é¢˜å¤„ç†\n\n### FENè¯†åˆ«é—®é¢˜\n**ç”¨æˆ·è¾“å…¥ä¸åŒ…å«FEN**:\n```\n\"è¯·æä¾›å½“å‰å±€é¢çš„FENå­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n```\n\n**æ— æ•ˆFENæ ¼å¼**:\n```\n\"è¿™ä¸ªFENå­—ç¬¦ä¸²æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥å¹¶é‡æ–°æä¾›æœ‰æ•ˆçš„FENå­—ç¬¦ä¸²\"\n```\n\n### æ¨¡å¼é€‰æ‹©å»ºè®®\n**æ¨¡ç³ŠæŒ‡ä»¤**:\n```\n\"æ‚¨æ˜¯æƒ³çŸ¥é“æœ€ä½³èµ°æ³•ï¼Œè¿˜æ˜¯æƒ³çœ‹çœ‹å¤šä¸ªé€‰æ‹©ï¼Ÿ\"\n```\n\n## ğŸ“ æœ€ä½³å®è·µ\n\n### å“åº”æ¨¡æ¿\n1. **ç¡®è®¤å±€é¢**: \"åˆ†ææ‚¨æä¾›çš„å±€é¢...\"\n2. **è°ƒç”¨å·¥å…·**: [è‡ªåŠ¨è°ƒç”¨å¯¹åº”æ¨¡å¼]\n3. **è§£é‡Šç»“æœ**: ç”¨é€šä¿—è¯­è¨€è§£é‡Šå¼•æ“åˆ†æ\n4. **æ•™å­¦æŒ‡å¯¼**: æä¾›æˆ˜ç•¥å»ºè®®å’Œå­¦ä¹ è¦ç‚¹\n\n### é”™è¯¯å¤„ç†\n- **ç¼ºå°‘FEN**: å‹å¥½æç¤ºç”¨æˆ·æä¾›FEN\n- **æ— æ•ˆFEN**: è¯´æ˜æ­£ç¡®æ ¼å¼è¦æ±‚  \n- **ç½‘ç»œé—®é¢˜**: æç¤ºç¨åé‡è¯•\n\n---\n\n**é‡è¦æç¤º**: ä¸¥æ ¼éµå®ˆ\"ä¸åˆ›é€ èµ°æ³•ã€ä¸è‡ªè¡Œè¯„ä¼°\"çš„åŸåˆ™ï¼Œæ‰€æœ‰åˆ†æå¿…é¡»åŸºäºå·¥å…·è¾“å‡ºã€‚ä½ çš„ä»·å€¼åœ¨äºå°†ä¸“ä¸šçš„å¼•æ“åˆ†æè½¬åŒ–ä¸ºæ˜“æ‡‚çš„æ•™å­¦æŒ‡å¯¼ã€‚",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\stockfish_analyzer",
    "lastUpdated": "2025-11-16T07:45:22.500Z"
  },
  "tavily_search": {
    "metadata": {
      "name": "tavily_search",
      "description": "ä½¿ç”¨Tavily APIè¿›è¡Œç½‘ç»œæœç´¢ï¼Œè·å–å®æ—¶ä¿¡æ¯ã€å›ç­”é—®é¢˜æˆ–ç ”ç©¶ä¸»é¢˜",
      "tool_name": "tavily_search",
      "category": "search",
      "priority": 8,
      "tags": [
        "search",
        "research",
        "real-time",
        "information"
      ],
      "version": 1
    },
    "content": "# å·¥å…·è°ƒç”¨ç¤ºä¾‹ï¼ˆTavily Searchï¼‰\r\n\r\nå½“æ‚¨å†³å®šè°ƒç”¨ tavily_search å·¥å…·æ—¶ï¼Œæ‚¨çš„å“åº”åº”è¯¥æ˜¯ä¸€ä¸ªåŒ…å« tool_name å’Œ parameters å­—æ®µçš„ JSON å¯¹è±¡ã€‚parameters å­—æ®µçš„å€¼åº”æ˜¯å·¥å…·æ‰€éœ€çš„å‚æ•°å¯¹è±¡ã€‚\r\n\r\n## âœ… æ­£ç¡®ç¤ºä¾‹\r\n\r\n**parameters å­—æ®µå†…å®¹:**\r\n```json\r\n{\"query\": \"latest AI news\"}\r\n```\r\n\r\n**å®Œæ•´å·¥å…·è°ƒç”¨å“åº”ç¤ºä¾‹:**\r\n```json\r\n{\"tool_name\": \"tavily_search\", \"parameters\": {\"query\": \"latest AI news\"}}\r\n```\r\n\r\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\r\n\r\n- **åœ¨ JSON ä¸­åµŒå…¥ Markdown åˆ†éš”ç¬¦:** \r\n  ```json\r\n  \"```json\\n{\\\"query\\\": \\\"latest AI news\\\"}\\n```\"\r\n  ```\r\n  (Qwen æ¨¡å‹ä¼šå°†æ­¤ä½œä¸º JSON å­—ç¬¦ä¸²çš„ä¸€éƒ¨åˆ†ï¼Œå¯¼è‡´è§£æå¤±è´¥)\r\n\r\n- **å‚æ•°åé”™è¯¯:** \r\n  ```json\r\n  {\"q\": \"latest AI news\"}\r\n  ```\r\n  (åº”ä¸º \"query\" è€Œé \"q\")\r\n\r\n- **å‚æ•°å€¼é”™è¯¯:** \r\n  ```json\r\n  {\"query\": 123}\r\n  ```\r\n  (query å‚æ•°å€¼åº”ä¸ºå­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æ•°å­—)\r\n\r\n## å…³é”®æŒ‡ä»¤\r\n1. **æŸ¥è¯¢æ„å»º**: æŸ¥è¯¢åº”è¯¥å…·ä½“ä¸”ç›¸å…³\r\n2. **å®æ—¶æ€§**: é€‚ç”¨äºéœ€è¦æœ€æ–°ä¿¡æ¯çš„é—®é¢˜\r\n3. **éªŒè¯**: å¯ç”¨äºéªŒè¯å…¶ä»–ä¿¡æ¯æ¥æº\r\n\r\n## ä½¿ç”¨åœºæ™¯\r\n1. è·å–å®æ—¶æ–°é—»å’Œä¿¡æ¯\r\n2. å›ç­”éœ€è¦æœ€æ–°æ•°æ®çš„é—®é¢˜\r\n3. ç ”ç©¶ç‰¹å®šä¸»é¢˜çš„èƒŒæ™¯ä¿¡æ¯\r\n4. éªŒè¯äº‹å®å’Œæ•°æ®çš„å‡†ç¡®æ€§\r\n\r\n## æœ€ä½³å®è·µ\r\n- æŸ¥è¯¢åº”è¯¥å…·ä½“ä¸”ç›¸å…³\r\n- å¯¹äºå¤æ‚é—®é¢˜ï¼Œå¯ä»¥åˆ†è§£ä¸ºå¤šä¸ªæœç´¢æŸ¥è¯¢\r\n- ç»“åˆæœç´¢ç»“æœè¿›è¡Œç»¼åˆåˆ†æ\r\n- ä¼˜å…ˆä½¿ç”¨è‹±æ–‡å…³é”®è¯è·å–æ›´å‡†ç¡®çš„ç»“æœ\r\n\r\n## ç¤ºä¾‹æŸ¥è¯¢\r\n- \"2024å¹´äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•\"\r\n- \"OpenAIæœ€æ–°æ¨¡å‹å‘å¸ƒä¿¡æ¯\"\r\n- \"æ¯”ç‰¹å¸å½“å‰ä»·æ ¼å’Œè¶‹åŠ¿\"\r\n- \"Python 3.12æ–°ç‰¹æ€§è¯¦è§£\"",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\tavily_search",
    "lastUpdated": "2025-11-16T07:45:22.501Z"
  }
};

export function getSkillsRegistry() {
  const map = new Map();
  Object.entries(SKILLS_DATA).forEach(([key, value]) => {
    map.set(key, value);
  });
  return map;
}