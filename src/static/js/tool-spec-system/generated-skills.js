/**
 * @file è‡ªåŠ¨ç”Ÿæˆçš„æŠ€èƒ½æ³¨å†Œè¡¨ - ç”± build-skills.js è„šæœ¬ç”Ÿæˆ
 * !!! è¯·å‹¿ç›´æ¥ç¼–è¾‘æ­¤æ–‡ä»¶ !!!
 * ç”Ÿæˆæ—¶é—´: 2025-11-29T10:58:06.605Z
 * æŠ€èƒ½æ•°é‡: 6
 */

export const SKILLS_DATA = {
  "crawl4ai": {
    "metadata": {
      "name": "crawl4ai",
      "description": "åŠŸèƒ½å¼ºå¤§çš„å¼€æºç½‘é¡µæŠ“å–å’Œæ•°æ®å¤„ç†å·¥å…·ï¼Œæ”¯æŒ7ç§å·¥ä½œæ¨¡å¼ï¼ŒåŒ…æ‹¬æˆªå›¾ã€PDFå¯¼å‡ºå’Œæ™ºèƒ½çˆ¬å–",
      "tool_name": "crawl4ai",
      "category": "web-crawling",
      "priority": 9,
      "tags": [
        "web-scraping",
        "screenshot",
        "pdf-export",
        "data-extraction",
        "crawling",
        "automation",
        "content-extraction",
        "crawl4ai"
      ],
      "version": 1.1
    },
    "content": "# Crawl4AI ç½‘é¡µæŠ“å–å·¥å…·æŒ‡å—\r\n\r\nCrawl4AI æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„å¼€æºç½‘é¡µæŠ“å–å’Œæ•°æ®å¤„ç†å·¥å…·ï¼Œæ”¯æŒ 7 ç§ä¸åŒçš„å·¥ä½œæ¨¡å¼ã€‚æ‰€æœ‰äºŒè¿›åˆ¶è¾“å‡ºï¼ˆæˆªå›¾ã€PDFï¼‰éƒ½ä»¥ base64 ç¼–ç è¿”å›ï¼Œä¾¿äºæ¨¡å‹å¤„ç†ã€‚\r\n\r\n## ğŸ¯ ã€è‡³å…³é‡è¦ã€‘é€šç”¨è°ƒç”¨ç»“æ„\r\n\r\n**æ‰€æœ‰å¯¹ `crawl4ai` çš„è°ƒç”¨éƒ½å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹åµŒå¥—ç»“æ„ï¼** è¿™æ˜¯ä¸€ä¸ªé€šç”¨è§„åˆ™ï¼Œé€‚ç”¨äºæ‰€æœ‰æ¨¡å¼ã€‚\r\n\r\n```json\r\n{\r\n  \"mode\": \"<æ¨¡å¼åç§°>\",\r\n  \"parameters\": {\r\n    \"param1\": \"value1\",\r\n    \"param2\": \"value2\"\r\n    // ...å…·ä½“æ¨¡å¼çš„æ‰€æœ‰å‚æ•°éƒ½æ”¾åœ¨è¿™é‡Œ\r\n  }\r\n}\r\n```\r\n\r\n### âŒ å¸¸è§è‡´å‘½é”™è¯¯ï¼šæœªåµŒå¥—å‚æ•°\r\n\r\n```json\r\n// è¿™æ˜¯ä¸€ä¸ªç»å¯¹ä¼šå¯¼è‡´å¤±è´¥çš„é”™è¯¯è°ƒç”¨ï¼\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"url\": \"https://example.com\" // é”™è¯¯ï¼'url' å¿…é¡»åœ¨ 'parameters' å†…éƒ¨\r\n}\r\n```\r\n\r\n### âœ… æ­£ç¡®çš„åŸºç¡€è°ƒç”¨æ¨¡å¼\r\n\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\"\r\n  }\r\n}\r\n```\r\n\r\n## ğŸ“‹ å¯ç”¨æ¨¡å¼å¿«é€Ÿé€‰æ‹©æŒ‡å—\r\n\r\n| æ¨¡å¼ | åŠŸèƒ½æè¿° | ä¸»è¦ç”¨é€” | å¤æ‚åº¦ | æ¨èåœºæ™¯ |\r\n|------|----------|----------|---------|----------|\r\n| `scrape` | æŠ“å–å•ä¸ªç½‘é¡µ | è·å–é¡µé¢å†…å®¹ã€æˆªå›¾ã€PDF | â­â­ | å•é¡µé¢å†…å®¹è·å– |\r\n| `deep_crawl` | æ·±åº¦æ™ºèƒ½çˆ¬å– | ä½¿ç”¨ç­–ç•¥æ·±åº¦çˆ¬å–ç½‘ç«™ | â­â­â­â­ | ç½‘ç«™å†…å®¹æ¢ç´¢ |\r\n| `batch_crawl` | æ‰¹é‡ URL å¤„ç† | åŒæ—¶å¤„ç†å¤šä¸ª URL | â­â­ | æ‰¹é‡æ•°æ®æ”¶é›† |\r\n| `extract` | ç»“æ„åŒ–æ•°æ®æå– | åŸºäº CSS æˆ– LLM æå–æ•°æ® | â­â­â­ | ç‰¹å®šæ•°æ®æå– |\r\n| `pdf_export` | PDF å¯¼å‡º | å°†ç½‘é¡µå¯¼å‡ºä¸º PDF | â­ | æ–‡æ¡£ä¿å­˜ |\r\n| `screenshot` | æˆªå›¾æ•è· | æ•è·ç½‘é¡µæˆªå›¾ | â­ | è§†è§‰è¯æ®ä¿å­˜ |\r\n\r\n## ğŸ¯ ä½¿ç”¨åœºæ™¯å¿«é€ŸæŒ‡å—\r\n\r\n### åœºæ™¯1ï¼šå¿«é€Ÿè·å–é¡µé¢å†…å®¹\r\n```json\r\n{\r\n  \"mode\": \"scrape\", \r\n  \"parameters\": {\r\n    \"url\": \"https://example.com/article\",\r\n    \"format\": \"markdown\",\r\n    \"word_count_threshold\": 10,\r\n    \"include_links\": true,\r\n    \"include_images\": true\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯2ï¼šæ‰¹é‡æ”¶é›†äº§å“ä¿¡æ¯\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\r\n      \"https://example.com/product1\",\r\n      \"https://example.com/product2\", \r\n      \"https://example.com/product3\"\r\n    ],\r\n    \"concurrent_limit\": 3\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯3ï¼šæ·±åº¦ç ”ç©¶æŸä¸ªç½‘ç«™\r\n```json\r\n{\r\n  \"mode\": \"deep_crawl\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com/docs\",\r\n    \"max_depth\": 3,\r\n    \"keywords\": [\"æ•™ç¨‹\", \"æŒ‡å—\", \"API\"],\r\n    \"strategy\": \"best_first\"\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯4ï¼šæå–ç»“æ„åŒ–æ•°æ®\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"name\": \"Article\",\r\n      \"baseSelector\": \".article-content\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"title\",\r\n          \"selector\": \"h1\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"author\",\r\n          \"selector\": \".author\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"content\",\r\n          \"selector\": \".content\",\r\n          \"type\": \"text\"\r\n        }\r\n      ]\r\n    },\r\n    \"extraction_type\": \"css\"\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯5ï¼šä¿å­˜ç½‘é¡µè¯æ®\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"return_screenshot\": true,\r\n    \"return_pdf\": true,\r\n    \"screenshot_quality\": 90,\r\n    \"screenshot_max_width\": 1200\r\n  }\r\n}\r\n```\r\n\r\n## ğŸš€ è¯¦ç»†æ¨¡å¼è¯´æ˜\r\n\r\n### 1. æŠ“å–å•ä¸ªç½‘é¡µ (`scrape`)\r\n\r\næŠ“å–å•ä¸ªç½‘é¡µå†…å®¹ï¼Œæ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼å’Œåª’ä½“æ•è·ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"format\": \"markdown\",\r\n    \"css_selector\": \".article-content\",\r\n    \"include_links\": true,\r\n    \"include_images\": true,\r\n    \"return_screenshot\": true,\r\n    \"return_pdf\": false,\r\n    \"screenshot_quality\": 80,\r\n    \"screenshot_max_width\": 1200,\r\n    \"word_count_threshold\": 10,\r\n    \"exclude_external_links\": true\r\n  }\r\n}\r\n```\r\n\r\n**âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆå‚æ•°æœªåµŒå¥—ï¼‰:**\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"url\": \"https://example.com\", // é”™è¯¯ï¼ç¼ºå°‘parametersåŒ…è£…\r\n  \"format\": \"markdown\"\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦æŠ“å–çš„ç½‘é¡µ URLï¼Œå¿…é¡»ä»¥ `http://` æˆ– `https://` å¼€å¤´\r\n- `format`: è¾“å‡ºæ ¼å¼ï¼Œ`markdown`(é»˜è®¤)/`html`/`text`\r\n- `css_selector`: æå–ç‰¹å®šå†…å®¹çš„ CSS é€‰æ‹©å™¨\r\n- `include_links`: æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«é“¾æ¥ï¼Œé»˜è®¤ true\r\n- `include_images`: æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«å›¾ç‰‡ï¼Œé»˜è®¤ true\r\n- `return_screenshot`: æ˜¯å¦è¿”å›æˆªå›¾(base64)ï¼Œé»˜è®¤ false\r\n- `return_pdf`: æ˜¯å¦è¿”å› PDF(base64)ï¼Œé»˜è®¤ false\r\n- `screenshot_quality`: æˆªå›¾è´¨é‡(10-100)ï¼Œé»˜è®¤ 70\r\n- `screenshot_max_width`: æˆªå›¾æœ€å¤§å®½åº¦ï¼Œé»˜è®¤ 1920\r\n- `word_count_threshold`: å†…å®¹å—æœ€å°å•è¯æ•°ï¼Œé»˜è®¤ 10\r\n- `exclude_external_links`: æ˜¯å¦æ’é™¤å¤–éƒ¨é“¾æ¥ï¼Œé»˜è®¤ true\r\n\r\n### 2. æ·±åº¦ç½‘ç«™çˆ¬å– (`deep_crawl`)\r\n\r\nä½¿ç”¨æ™ºèƒ½ç­–ç•¥æ·±åº¦çˆ¬å–æ•´ä¸ªç½‘ç«™ï¼Œæ”¯æŒå…³é”®è¯è¯„åˆ†å’Œ URL è¿‡æ»¤ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"deep_crawl\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"max_depth\": 3,\r\n    \"max_pages\": 50,\r\n    \"strategy\": \"best_first\",\r\n    \"include_external\": false,\r\n    \"keywords\": [\"äº§å“\", \"ä»·æ ¼\", \"è§„æ ¼\"],\r\n    \"url_patterns\": [\"/products/\", \"/docs/\"],\r\n    \"stream\": false\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): èµ·å§‹ URL\r\n- `max_depth`: æœ€å¤§çˆ¬å–æ·±åº¦ï¼Œé»˜è®¤ 2\r\n- `max_pages`: æœ€å¤§é¡µé¢æ•°ï¼Œé»˜è®¤ 50\r\n- `strategy`: çˆ¬å–ç­–ç•¥ï¼Œ`bfs`(é»˜è®¤)/`dfs`/`best_first`\r\n- `include_external`: æ˜¯å¦è·Ÿè¸ªå¤–éƒ¨é“¾æ¥ï¼Œé»˜è®¤ false\r\n- `keywords`: ç”¨äºç›¸å…³æ€§è¯„åˆ†çš„å…³é”®è¯åˆ—è¡¨\r\n- `url_patterns`: URL æ¨¡å¼è¿‡æ»¤åˆ—è¡¨\r\n- `stream`: æ˜¯å¦æµå¼è¿”å›ç»“æœï¼Œé»˜è®¤ false\r\n\r\n### 3. æ‰¹é‡ URL å¤„ç† (`batch_crawl`)\r\n\r\nåŒæ—¶å¤„ç†å¤šä¸ª URLï¼Œé€‚ç”¨äºæ‰¹é‡æ•°æ®æ”¶é›†ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\r\n      \"https://example.com/page1\",\r\n      \"https://example.com/page2\",\r\n      \"https://example.com/page3\"\r\n    ],\r\n    \"stream\": false,\r\n    \"concurrent_limit\": 3\r\n  }\r\n}\r\n```\r\n\r\n**âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆurlsä¸æ˜¯æ•°ç»„ï¼‰:**\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": \"https://example.com/page1\" // é”™è¯¯ï¼urlså¿…é¡»æ˜¯æ•°ç»„\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `urls` (å¿…éœ€): URL åˆ—è¡¨ï¼Œå¿…é¡»æ˜¯æ•°ç»„æ ¼å¼\r\n- `stream`: æ˜¯å¦æµå¼è¿”å›ï¼Œé»˜è®¤ false\r\n- `concurrent_limit`: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ 3\r\n\r\n### 4. ç»“æ„åŒ–æ•°æ®æå– (`extract`)\r\n\r\nä»ç½‘é¡µä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼Œæ”¯æŒ CSS é€‰æ‹©å™¨å’Œ LLM æ™ºèƒ½æå–ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹ (CSS æå–):**\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"name\": \"Article\",\r\n      \"baseSelector\": \".article-content\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"title\",\r\n          \"selector\": \"h1\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"author\",\r\n          \"selector\": \".author\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"publish_date\",\r\n          \"selector\": \".date\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"content\",\r\n          \"selector\": \".content\",\r\n          \"type\": \"text\"\r\n        }\r\n      ]\r\n    },\r\n    \"css_selector\": \".article-content\",\r\n    \"extraction_type\": \"css\"\r\n  }\r\n}\r\n```\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹ (LLM æå–):**\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"type\": \"object\",\r\n      \"properties\": {\r\n        \"title\": {\"type\": \"string\"},\r\n        \"author\": {\"type\": \"string\"},\r\n        \"summary\": {\"type\": \"string\"},\r\n        \"key_points\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\r\n      }\r\n    },\r\n    \"extraction_type\": \"llm\",\r\n    \"prompt\": \"ä»æ–‡ç« ä¸­æå–æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦å’Œå…³é”®è¦ç‚¹\"\r\n  }\r\n}\r\n```\r\n\r\n**ğŸ›¡ï¸ è‡ªåŠ¨ä¿®å¤æœºåˆ¶ï¼š**\r\næˆ‘ä»¬çš„å·¥å…·ä¼šè‡ªåŠ¨ç¡®ä¿ `schema_definition` åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼š\r\n- å¦‚æœç¼ºå°‘ `baseSelector`ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º `css_selector` æˆ– `'body'`\r\n- å¦‚æœç¼ºå°‘ `fields`ï¼Œè‡ªåŠ¨åˆ›å»ºé»˜è®¤å­—æ®µé…ç½®  \r\n- å¦‚æœç¼ºå°‘ `name`ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º `\"ExtractedData\"`\r\n\r\n**ğŸ’¡ æœ€ä½³å®è·µï¼š** è™½ç„¶å·¥å…·ä¼šè‡ªåŠ¨ä¿®å¤ï¼Œä½†æä¾›å®Œæ•´çš„ schema å¯ä»¥è·å¾—æ›´ç²¾ç¡®çš„æå–ç»“æœã€‚\r\n\r\n**âš ï¸ é‡è¦æç¤º:**\r\n- **å‚æ•°åç§°**: ç”¨äºå®šä¹‰æå–ç»“æ„çš„å‚æ•°å¿…é¡»å‘½åä¸º `schema_definition`\r\n- **å¸¸è§é”™è¯¯**: è¯·å‹¿ä½¿ç”¨ `schema` ä½œä¸ºå‚æ•°åï¼Œè¿™ä¼šå¯¼è‡´è°ƒç”¨å¤±è´¥\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦æå–çš„ç½‘é¡µ URL\r\n- `schema_definition` (å¿…éœ€): å®šä¹‰è¾“å‡ºç»“æ„çš„ JSON schema\r\n- `css_selector`: åŸºç¡€ CSS é€‰æ‹©å™¨ï¼ˆCSS æå–æ—¶ä½¿ç”¨ï¼‰\r\n- `extraction_type`: æå–ç±»å‹ï¼Œ`css`(é»˜è®¤)/`llm`\r\n- `prompt`: LLM æå–çš„æç¤ºè¯­\r\n\r\n## ğŸ“‹ Schema Definition ç»“æ„è¯´æ˜\r\n\r\n### CSS æå–æ¨¡å¼å¿…éœ€çš„ schema ç»“æ„ï¼š\r\n```json\r\n{\r\n  \"name\": \"YourSchemaName\",           // å¿…éœ€ï¼šschema åç§°\r\n  \"baseSelector\": \"css-selector\",     // å¿…éœ€ï¼šåŸºç¡€ CSS é€‰æ‹©å™¨\r\n  \"fields\": [                         // å¿…éœ€ï¼šå­—æ®µå®šä¹‰æ•°ç»„\r\n    {\r\n      \"name\": \"field_name\",           // å¿…éœ€ï¼šå­—æ®µåç§°\r\n      \"selector\": \"css-selector\",     // å¿…éœ€ï¼šå­—æ®µé€‰æ‹©å™¨\r\n      \"type\": \"text\",                 // å¿…éœ€ï¼šå­—æ®µç±»å‹\r\n      \"multiple\": true                // å¯é€‰ï¼šæ˜¯å¦å…è®¸å¤šä¸ªå€¼\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### LLM æå–æ¨¡å¼ schema ç»“æ„ï¼š\r\n```json\r\n{\r\n  \"type\": \"object\",\r\n  \"properties\": {\r\n    \"field1\": {\"type\": \"string\"},\r\n    \"field2\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\r\n  }\r\n}\r\n```\r\n\r\n### 5. PDF å¯¼å‡º (`pdf_export`)\r\n\r\nå°†ç½‘é¡µå¯¼å‡ºä¸º PDF æ ¼å¼ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"pdf_export\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com/document\",\r\n    \"return_as_base64\": true\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦å¯¼å‡ºä¸º PDF çš„ç½‘é¡µ URL\r\n- `return_as_base64`: æ˜¯å¦è¿”å› base64 ç¼–ç ï¼Œé»˜è®¤ true\r\n\r\n### 6. æˆªå›¾æ•è· (`screenshot`)\r\n\r\næ•è·ç½‘é¡µæˆªå›¾ï¼Œæ”¯æŒè´¨é‡å‹ç¼©å’Œå°ºå¯¸è°ƒæ•´ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"screenshot\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"full_page\": true,\r\n    \"return_as_base64\": true,\r\n    \"quality\": 80,\r\n    \"max_width\": 1200,\r\n    \"max_height\": 3000\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦æˆªå›¾çš„ç½‘é¡µ URL\r\n- `full_page`: æ˜¯å¦æˆªå–æ•´ä¸ªé¡µé¢ï¼Œé»˜è®¤ true\r\n- `return_as_base64`: æ˜¯å¦è¿”å› base64 ç¼–ç ï¼Œé»˜è®¤ true\r\n- `quality`: æˆªå›¾è´¨é‡(10-100)ï¼Œé»˜è®¤ 70\r\n- `max_width`: æœ€å¤§å®½åº¦ï¼Œé»˜è®¤ 1920\r\n- `max_height`: æœ€å¤§é«˜åº¦ï¼Œé»˜è®¤ 5000\r\n\r\n## ğŸ”„ å¸¸è§å·¥ä½œæµ\r\n\r\n### æ–°é—»æ–‡ç« é‡‡é›†å·¥ä½œæµ\r\n**ç›®æ ‡**: è‡ªåŠ¨æ”¶é›†å’Œåˆ†ææ–°é—»å†…å®¹\r\n1. **å‘ç°é˜¶æ®µ**: ä½¿ç”¨ `deep_crawl` å‘ç°ç›¸å…³æ–‡ç« é“¾æ¥\r\n2. **é‡‡é›†é˜¶æ®µ**: ä½¿ç”¨ `batch_crawl` æ‰¹é‡è·å–å†…å®¹  \r\n3. **æå–é˜¶æ®µ**: ä½¿ç”¨ `extract` ç»“æ„åŒ–æå–å…³é”®ä¿¡æ¯\r\n\r\n### ç«å“åˆ†æå·¥ä½œæµ\r\n**ç›®æ ‡**: ç³»ç»ŸåŒ–åˆ†æç«äº‰å¯¹æ‰‹ç½‘ç«™\r\n1. **è¯æ®æ”¶é›†**: ä½¿ç”¨ `screenshot` æ•è·ç«å“é¡µé¢\r\n2. **å†…å®¹åˆ†æ**: ä½¿ç”¨ `scrape` è·å–è¯¦ç»†å†…å®¹\r\n3. **æ–‡æ¡£ä¿å­˜**: ä½¿ç”¨ `pdf_export` ä¿å­˜è¯æ®\r\n\r\n### äº§å“ç›®å½•çˆ¬å–å·¥ä½œæµ  \r\n**ç›®æ ‡**: å»ºç«‹å®Œæ•´çš„äº§å“æ•°æ®åº“\r\n1. **ç›®å½•æ¢ç´¢**: ä½¿ç”¨ `deep_crawl` å‘ç°æ‰€æœ‰äº§å“é¡µé¢\r\n2. **æ•°æ®æå–**: ä½¿ç”¨ `extract` æå–äº§å“ä¿¡æ¯\r\n\r\n## ğŸ› ï¸ æ•…éšœæ’é™¤\r\n\r\n### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ\r\n\r\n#### æ€§èƒ½é—®é¢˜\r\n- **è¶…æ—¶é—®é¢˜**: å‡å°‘ `max_pages` æˆ– `max_depth`ï¼Œé™ä½ `concurrent_limit`\r\n- **å†…å­˜é—®é¢˜**: å¯ç”¨ `stream: true`ï¼Œå‡å°‘æ‰¹é‡å¤„ç†çš„ URL æ•°é‡\r\n\r\n#### å†…å®¹è´¨é‡é—®é¢˜  \r\n- **å†…å®¹ç¼ºå¤±**: è°ƒæ•´ `word_count_threshold`ï¼Œæ£€æŸ¥ `css_selector`\r\n- **æˆªå›¾ä¸å®Œæ•´**: å¢åŠ  `max_height` å€¼ï¼Œç¡®ä¿ `full_page: true`\r\n\r\n#### ç½‘ç»œé—®é¢˜\r\n- **è¿æ¥å¤±è´¥**: æ£€æŸ¥ URL æ ¼å¼ï¼ŒéªŒè¯ç½‘ç»œè¿æ¥\r\n- **è¢«ç½‘ç«™å±è”½**: é™ä½çˆ¬å–é€Ÿåº¦ï¼Œå¢åŠ è¯·æ±‚é—´éš”\r\n\r\n#### Extract æ¨¡å¼ç‰¹å®šé—®é¢˜\r\n- **ç©ºç»“æœ**: æ£€æŸ¥ `fields` æ•°ç»„ä¸­çš„ `selector` æ˜¯å¦å‡†ç¡®åŒ¹é…é¡µé¢å…ƒç´ \r\n- **å­—æ®µç¼ºå¤±**: ç¡®ä¿ `schema_definition` åŒ…å«å®Œæ•´çš„ `name`ã€`baseSelector`ã€`fields` ç»“æ„\r\n- **è‡ªåŠ¨ä¿®å¤**: å·¥å…·ä¼šè‡ªåŠ¨è¡¥å…¨ç¼ºå¤±å­—æ®µï¼Œä½†æ‰‹åŠ¨æä¾›å®Œæ•´ schema æ•ˆæœæ›´å¥½\r\n\r\n### è°ƒè¯•æŠ€å·§\r\n\r\n1. **ä»ç®€å•å¼€å§‹**: å…ˆç”¨ `scrape` æ¨¡å¼æµ‹è¯•å•ä¸ªé¡µé¢\r\n2. **é€æ­¥å¢åŠ å¤æ‚åº¦**: ç¡®è®¤åŸºç¡€åŠŸèƒ½æ­£å¸¸åå†ä½¿ç”¨é«˜çº§æ¨¡å¼  \r\n3. **æ£€æŸ¥å‚æ•°**: ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ­£ç¡®åµŒå¥—åœ¨ `parameters` å¯¹è±¡å†…\r\n4. **éªŒè¯è¾“å‡º**: å…ˆæµ‹è¯•å°è§„æ¨¡æ•°æ®ï¼Œç¡®è®¤è¾“å‡ºæ ¼å¼ç¬¦åˆé¢„æœŸ\r\n\r\n## âš ï¸ é‡è¦æç¤º\r\n\r\n### âœ… æ­£ç¡®åšæ³•\r\n- **å‚æ•°åµŒå¥—**: æ‰€æœ‰å‚æ•°å¿…é¡»æ”¾åœ¨ `parameters` å¯¹è±¡å†…\r\n- **URL æ ¼å¼**: å¿…é¡»ä»¥ `http://` æˆ– `https://` å¼€å¤´  \r\n- **æ¨¡å¼é€‰æ‹©**: æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å¼\r\n- **å†…å­˜ç®¡ç†**: å¤§é‡æ•°æ®æ—¶ä½¿ç”¨æµå¼å¤„ç† (`stream: true`)\r\n- **Schema å®Œæ•´æ€§**: ä¸º CSS æå–æä¾›å®Œæ•´çš„ `name`ã€`baseSelector`ã€`fields` ç»“æ„\r\n\r\n### âŒ å¸¸è§é”™è¯¯\r\n\r\n**é”™è¯¯ 1: ç¼ºå°‘åµŒå¥—å‚æ•°**\r\n```json\r\n// âŒ é”™è¯¯\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"url\": \"https://example.com\"\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\"\r\n  }\r\n}\r\n```\r\n\r\n**é”™è¯¯ 2: URL ç¼ºå°‘åè®®**\r\n```json\r\n// âŒ é”™è¯¯\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"example.com\"\r\n  }\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\"\r\n  }\r\n}\r\n```\r\n\r\n**é”™è¯¯ 3: é”™è¯¯çš„å‚æ•°ç±»å‹**\r\n```json\r\n// âŒ é”™è¯¯ - urls åº”è¯¥æ˜¯æ•°ç»„\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": \"https://example.com\"\r\n  }\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\"https://example.com\"]\r\n  }\r\n}\r\n```\r\n\r\n**é”™è¯¯ 4: extractæ¨¡å¼ä½¿ç”¨é”™è¯¯çš„å‚æ•°å**\r\n```json\r\n// âŒ é”™è¯¯ - åº”è¯¥ä½¿ç”¨ schema_definition\r\n{\r\n  \"mode\": \"extract\", \r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"schema\": { // é”™è¯¯ï¼åº”è¯¥æ˜¯ schema_definition\r\n      \"title\": \"string\"\r\n    }\r\n  }\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\", \r\n    \"schema_definition\": {\r\n      \"name\": \"Article\",\r\n      \"baseSelector\": \".content\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"title\",\r\n          \"selector\": \"h1\",\r\n          \"type\": \"text\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## ğŸª é«˜çº§ä½¿ç”¨æŠ€å·§\r\n\r\n### 1. ç»„åˆä½¿ç”¨åª’ä½“æ•è·\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"include_links\": true,\r\n    \"include_images\": true,\r\n    \"return_screenshot\": true,\r\n    \"return_pdf\": true,\r\n    \"screenshot_quality\": 90,\r\n    \"screenshot_max_width\": 1200,\r\n    \"word_count_threshold\": 15\r\n  }\r\n}\r\n```\r\n\r\n### 2. æ™ºèƒ½æ·±åº¦çˆ¬å–\r\n```json\r\n{\r\n  \"mode\": \"deep_crawl\",\r\n  \"parameters\": {\r\n    \"url\": \"https://docs.example.com\",\r\n    \"strategy\": \"best_first\",\r\n    \"keywords\": [\"API\", \"æ•™ç¨‹\", \"ç¤ºä¾‹\"],\r\n    \"max_depth\": 3,\r\n    \"max_pages\": 30\r\n  }\r\n}\r\n```\r\n\r\n### 3. æ‰¹é‡å¤„ç†é‡è¦é¡µé¢\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\r\n      \"https://example.com/home\",\r\n      \"https://example.com/about\", \r\n      \"https://example.com/contact\",\r\n      \"https://example.com/products\"\r\n    ],\r\n    \"concurrent_limit\": 2\r\n  }\r\n}\r\n```\r\n\r\n### 4. æ™ºèƒ½å†…å®¹æå–\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"name\": \"NewsArticle\",\r\n      \"baseSelector\": \".article-container\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"headline\",\r\n          \"selector\": \"h1.news-title\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"author\",\r\n          \"selector\": \".author-name\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"publish_date\",\r\n          \"selector\": \".publish-date\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"main_content\",\r\n          \"selector\": \".article-body\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"tags\",\r\n          \"selector\": \".tag\",\r\n          \"type\": \"text\",\r\n          \"multiple\": true\r\n        }\r\n      ]\r\n    },\r\n    \"extraction_type\": \"css\"\r\n  }\r\n}\r\n```\r\n\r\n## ğŸ“ æœ€ä½³å®è·µæ€»ç»“\r\n\r\n1. **é€‰æ‹©åˆé€‚çš„æ¨¡å¼**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æœ€ç®€å•æœ‰æ•ˆçš„æ¨¡å¼\r\n2. **æ¸è¿›å¼æµ‹è¯•**: ä»å°è§„æ¨¡å¼€å§‹æµ‹è¯•ï¼Œé€æ­¥æ‰©å¤§èŒƒå›´  \r\n3. **èµ„æºç®¡ç†**: æ³¨æ„å¹¶å‘æ•°å’Œå†…å­˜ä½¿ç”¨ï¼Œé¿å…è¿‡åº¦è¯·æ±‚\r\n4. **é”™è¯¯å¤„ç†**: å‡†å¤‡å¥½å¤„ç†ç½‘ç»œé”™è¯¯å’Œå†…å®¹è§£æå¤±è´¥çš„æƒ…å†µ\r\n5. **åˆæ³•ä½¿ç”¨**: éµå®ˆç½‘ç«™çš„ robots.txt å’ŒæœåŠ¡æ¡æ¬¾\r\n6. **æ ¼å¼æ£€æŸ¥**: æ¯æ¬¡è°ƒç”¨å‰ç¡®è®¤å‚æ•°æ­£ç¡®åµŒå¥—åœ¨ `parameters` å¯¹è±¡å†…\r\n7. **å‚æ•°éªŒè¯**: ç¡®ä¿ URL åŒ…å«åè®®ï¼Œæ•°ç»„å‚æ•°æ­£ç¡®æ ¼å¼\r\n8. **å‘½åè§„èŒƒ**: extractæ¨¡å¼å¿…é¡»ä½¿ç”¨ `schema_definition` å‚æ•°å\r\n9. **å†…å®¹æ§åˆ¶**: ä½¿ç”¨ `include_links` å’Œ `include_images` æ§åˆ¶è¾“å‡ºå†…å®¹\r\n10. **è´¨é‡ä¼˜åŒ–**: ä½¿ç”¨ `word_count_threshold` è¿‡æ»¤ä½è´¨é‡å†…å®¹å—\r\n11. **Schema å®Œæ•´æ€§**: ä¸º CSS æå–æä¾›å®Œæ•´çš„ schema ç»“æ„ä»¥è·å¾—æœ€ä½³ç»“æœ\r\n```",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\crawl4ai",
    "lastUpdated": "2025-11-29T10:58:06.592Z"
  },
  "firecrawl": {
    "metadata": {
      "name": "firecrawl",
      "description": "å¤šåŠŸèƒ½ç½‘é¡µæŠ“å–å’Œæ•°æ®æå–å·¥å…·ï¼Œæ”¯æŒåŒæ­¥æŠ“å–ã€æœç´¢ã€ç½‘ç«™åœ°å›¾è·å–å’Œå¼‚æ­¥çˆ¬å–",
      "tool_name": "firecrawl",
      "category": "web-crawling",
      "priority": 7,
      "tags": [
        "web-scraping",
        "data-extraction",
        "crawling",
        "automation",
        "firecrawl"
      ],
      "version": 1
    },
    "content": "# å·¥å…·è°ƒç”¨ç¤ºä¾‹ï¼ˆFirecrawlï¼‰\n\n`firecrawl` æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½ç½‘é¡µæŠ“å–å’Œæ•°æ®æå–å·¥å…·ï¼Œé€šè¿‡ `mode` å‚æ•°è°ƒç”¨ä¸åŒåŠŸèƒ½ã€‚å…¶ `parameters` ç»“æ„æ˜¯åµŒå¥—çš„ã€‚\n\n**âœ… æ­£ç¡®çš„è°ƒç”¨ç»“æ„:**\n```json\n{\"mode\": \"<åŠŸèƒ½æ¨¡å¼>\", \"parameters\": {\"<å‚æ•°å>\": \"<å‚æ•°å€¼>\"}}\n```\n\n**ğŸ’¡ é‡è¦æç¤º:**\n- `scrape`ã€`search`ã€`map` æ˜¯åŒæ­¥æ“ä½œï¼Œç«‹å³è¿”å›ç»“æœ\n- `crawl`ã€`extract` æ˜¯å¼‚æ­¥æ“ä½œï¼Œè¿”å› `job_id` ç”¨äºåç»­çŠ¶æ€æ£€æŸ¥\n- æ‰€æœ‰å‚æ•°éƒ½å¿…é¡»åœ¨ `parameters` å¯¹è±¡å†…ï¼Œä¸è¦æ”¾åœ¨é¡¶å±‚\n- URL å¿…é¡»ä»¥ `http://` æˆ– `https://` å¼€å¤´\n\n## åŠŸèƒ½æ¨¡å¼è¯¦è§£\n\n### â¡ï¸ ç¤ºä¾‹ 1: æŠ“å–å•ä¸ªç½‘é¡µ (`scrape`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"scrape\", \n  \"parameters\": {\n    \"url\": \"https://docs.firecrawl.dev/\",\n    \"formats\": [\"markdown\"]  // å¯é€‰ï¼š[\"markdown\", \"html\"]ï¼Œé»˜è®¤ markdown\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 2: ç½‘é¡µæœç´¢ (`search`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"search\", \n  \"parameters\": {\n    \"query\": \"äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•\",\n    \"limit\": 5\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 3: è·å–ç½‘ç«™åœ°å›¾ (`map`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"map\", \n  \"parameters\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 4: å¼‚æ­¥çˆ¬å–ç½‘ç«™ (`crawl`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"crawl\", \n  \"parameters\": {\n    \"url\": \"https://firecrawl.dev\", \n    \"limit\": 5\n  }\n}\n```\n*æ­¤è°ƒç”¨ä¼šè¿”å›ä¸€ä¸ª `job_id`ï¼Œç”¨äºåç»­æŸ¥è¯¢ã€‚*\n\n### â¡ï¸ ç¤ºä¾‹ 5: ç»“æ„åŒ–æ•°æ®æå– (`extract`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"extract\", \n  \"parameters\": {\n    \"urls\": [\"https://news.example.com/article\"],\n    \"prompt\": \"æå–æ–‡ç« æ ‡é¢˜ã€ä½œè€…å’Œå‘å¸ƒæ—¶é—´\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"title\": {\"type\": \"string\"},\n        \"author\": {\"type\": \"string\"}, \n        \"publish_time\": {\"type\": \"string\"}\n      }\n    }\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 6: æ£€æŸ¥å¼‚æ­¥ä»»åŠ¡çŠ¶æ€ (`check_status`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"check_status\", \n  \"parameters\": {\n    \"job_id\": \"some-unique-job-identifier\"\n  }\n}\n```\n\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\n\n- **ç¼ºå°‘ `mode` å‚æ•°:** `{\"parameters\": {\"url\": \"...\"}}`\n- **ç¼ºå°‘åµŒå¥—çš„ `parameters` å¯¹è±¡:** `{\"mode\": \"scrape\", \"url\": \"...\"}`\n- **å°†å‚æ•°æ”¾åœ¨é¡¶å±‚:** `{\"url\": \"...\"}` \n- **ä½¿ç”¨æ— æ•ˆçš„ URL æ ¼å¼:** `{\"mode\": \"scrape\", \"parameters\": {\"url\": \"example.com\"}}` (ç¼ºå°‘åè®®)\n- **é”™è¯¯çš„å‚æ•°ç±»å‹:** `{\"mode\": \"extract\", \"parameters\": {\"urls\": \"https://example.com\"}}` (urls åº”è¯¥æ˜¯æ•°ç»„)",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\firecrawl",
    "lastUpdated": "2025-11-29T10:58:06.596Z"
  },
  "glm4v_analyze_image": {
    "metadata": {
      "name": "glm4v_analyze_image",
      "description": "æ™ºè°±AIçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå›¾åƒåˆ†æã€å†…å®¹è¯†åˆ«å’Œè§†è§‰é—®ç­”",
      "tool_name": "glm4v_analyze_image",
      "category": "vision",
      "priority": 7,
      "tags": [
        "image-analysis",
        "vision",
        "recognition",
        "visual-qa",
        "multimodal"
      ],
      "version": 1
    },
    "content": "# GLM-4Vå›¾åƒåˆ†æå·¥å…·æŒ‡å—\r\n\r\n## æ ¸å¿ƒèƒ½åŠ›\r\n- å›¾åƒå†…å®¹è¯†åˆ«å’Œæè¿°\r\n- è§†è§‰é—®ç­”å’Œæ¨ç†\r\n- å›¾åƒç»†èŠ‚åˆ†æ\r\n- å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆ\r\n\r\n## è°ƒç”¨è§„èŒƒ\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\",\r\n    \"image_url\": \"å›¾ç‰‡URL\",\r\n    \"prompt\": \"åˆ†ææç¤ºè¯­\"\r\n  }\r\n}\r\n```\r\n\r\nä»¥ä¸‹æ˜¯è°ƒç”¨ `glm4v_analyze_image` å·¥å…·çš„**æ­£ç¡®**å’Œ**é”™è¯¯**ç¤ºä¾‹ã€‚è¯·åŠ¡å¿…éµå¾ªæ­£ç¡®æ ¼å¼ã€‚\r\n\r\n## âœ… æ­£ç¡®ç¤ºä¾‹\r\n```json\r\n{\"model\": \"glm-4v-flash\", \"image_url\": \"https://path/to/image.jpg\", \"prompt\": \"Describe this image.\"}\r\n```\r\n\r\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\r\n\r\n- **ç¼ºå°‘å¼•å·æˆ–é€—å·:** \r\n  ```json\r\n  {\"model\": \"glm-4v-flash\", \"image_url\": \"https://path/to/image.jpg\", \"prompt\": \"Describe this image.\"}\r\n  ```\r\n  (ç¼ºå°‘ `}`)\r\n\r\n- **å‚æ•°åé”™è¯¯:** \r\n  ```json\r\n  {\"img_url\": \"https://path/to/image.jpg\"}\r\n  ```\r\n  (åº”ä¸º \"image_url\" è€Œé \"img_url\")\r\n\r\n- **æ¨¡å‹åç§°é”™è¯¯:** \r\n  ```json\r\n  {\"model\": \"glm4v-flash\", \"image_url\": \"https://path/to/image.jpg\", \"prompt\": \"Describe this image.\"}\r\n  ```\r\n  (åº”ä¸º \"glm-4v-flash\")\r\n  \r\n## å…³é”®æŒ‡ä»¤\r\n1. **æ¨¡å‹é€‰æ‹©**: ä½¿ç”¨ `glm-4v-flash` æ¨¡å‹\r\n2. **å›¾ç‰‡æ ¼å¼**: æ”¯æŒå¸¸è§å›¾ç‰‡æ ¼å¼ï¼ˆJPEG, PNG, WebPç­‰ï¼‰\r\n3. **æç¤ºè¯­è®¾è®¡**: æ¸…æ™°å…·ä½“çš„åˆ†ææŒ‡ä»¤\r\n4. **URLæœ‰æ•ˆæ€§**: ç¡®ä¿å›¾ç‰‡URLå¯å…¬å¼€è®¿é—®\r\n\r\n## ä½¿ç”¨åœºæ™¯\r\n\r\n### å›¾åƒæè¿°\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\", \r\n    \"image_url\": \"https://example.com/image.jpg\",\r\n    \"prompt\": \"è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹\"\r\n  }\r\n}\r\n```\r\n\r\n### è§†è§‰é—®ç­”\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\",\r\n    \"image_url\": \"https://example.com/image.jpg\", \r\n    \"prompt\": \"å›¾ç‰‡ä¸­æœ‰å¤šå°‘äººï¼Ÿä»–ä»¬åœ¨åšä»€ä¹ˆï¼Ÿ\"\r\n  }\r\n}\r\n```\r\n\r\n### ç»†èŠ‚åˆ†æ\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\",\r\n    \"image_url\": \"https://example.com/image.jpg\",\r\n    \"prompt\": \"åˆ†æå›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹å’ŒæŠ€æœ¯ç»†èŠ‚\"\r\n  }\r\n}\r\n```\r\n\r\n## æœ€ä½³å®è·µ\r\n\r\n### æç¤ºè¯­è®¾è®¡\r\n- **å…·ä½“æ˜ç¡®**: \"æè¿°å›¾ç‰‡ä¸­äººç‰©çš„åŠ¨ä½œå’Œè¡¨æƒ…\"\r\n- **ä»»åŠ¡å¯¼å‘**: \"è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰ç‰©ä½“å¹¶åˆ†ç±»\"\r\n- **ç»†èŠ‚è¦æ±‚**: \"æ³¨æ„é¢œè‰²ã€å½¢çŠ¶ã€ç©ºé—´å…³ç³»ç­‰ç»†èŠ‚\"\r\n\r\n### é”™è¯¯å¤„ç†\r\n- æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ\r\n- ç¡®è®¤å›¾ç‰‡æ ¼å¼æ”¯æŒ\r\n- å¤„ç†ç½‘ç»œè¶…æ—¶æƒ…å†µ\r\n\r\n## èƒ½åŠ›èŒƒå›´\r\n- âœ… ç‰©ä½“è¯†åˆ«å’Œåˆ†ç±»\r\n- âœ… åœºæ™¯ç†è§£å’Œæè¿°  \r\n- âœ… æ–‡å­—è¯†åˆ«ï¼ˆOCRï¼‰\r\n- âœ… æƒ…æ„Ÿå’Œæ°›å›´åˆ†æ\r\n- âœ… æŠ€æœ¯ç»†èŠ‚æå–\r\n\r\n## é™åˆ¶è¯´æ˜\r\n- âŒ ä¸èƒ½å¤„ç†æ•æ„Ÿæˆ–ä¸å½“å†…å®¹\r\n- âŒ å›¾ç‰‡å¤§å°å’Œåˆ†è¾¨ç‡æœ‰é™åˆ¶\r\n- âŒ å®æ—¶è§†é¢‘æµä¸æ”¯æŒ\r\n- âŒ 3Dæ¨¡å‹åˆ†æä¸æ”¯æŒ\r\n\r\n## æ€§èƒ½ä¼˜åŒ–\r\n- ä½¿ç”¨åˆé€‚çš„å›¾ç‰‡å°ºå¯¸\r\n- æä¾›å…·ä½“çš„åˆ†æéœ€æ±‚\r\n- åˆ†æ­¥éª¤è¿›è¡Œå¤æ‚åˆ†æ\r\n- ç»“åˆå…¶ä»–å·¥å…·è¿›è¡ŒéªŒè¯",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\glm4v_analyze_image",
    "lastUpdated": "2025-11-29T10:58:06.597Z"
  },
  "python_sandbox": {
    "metadata": {
      "name": "python_sandbox",
      "description": "åœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡ŒPythonä»£ç ï¼Œç”¨äºæ•°æ®åˆ†æã€å¯è§†åŒ–å’Œç”ŸæˆExcelã€Wordã€PDFç­‰æ–‡ä»¶ã€‚æ”¯æŒæ•°æ®æ¸…æ´—ã€ç»Ÿè®¡åˆ†æã€æœºå™¨å­¦ä¹ ã€å›¾è¡¨ç”Ÿæˆã€æ–‡æ¡£è‡ªåŠ¨åŒ–ç­‰å¤æ‚å·¥ä½œæµã€‚",
      "tool_name": "python_sandbox",
      "category": "code",
      "priority": 10,
      "tags": [
        "python",
        "code",
        "visualization",
        "data-analysis",
        "chart",
        "document",
        "automation",
        "machine-learning",
        "reporting",
        "excel",
        "word",
        "pdf",
        "ppt"
      ],
      "version": 2.3,
      "references": [
        "matplotlib_cookbook.md",
        "pandas_cheatsheet.md",
        "report_generator_workflow.md",
        "ml_workflow.md",
        "sympy_cookbook.md",
        "scipy_cookbook.md"
      ]
    },
    "content": "# Pythonæ²™ç›’å·¥å…·ä½¿ç”¨æŒ‡å— (v2.3 æœ€ç»ˆå®Œæ•´ç‰ˆ)\r\n\r\n## ğŸ¯ æ ¸å¿ƒèƒ½åŠ›æ¦‚è§ˆ\r\n\r\nPythonæ²™ç›’æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„ä»£ç æ‰§è¡Œç¯å¢ƒï¼Œæ”¯æŒï¼š\r\n- **æ•°æ®åˆ†æä¸å¤„ç†**: ä½¿ç”¨Pandasè¿›è¡Œæ•°æ®æ¸…æ´—ã€è½¬æ¢ã€èšåˆã€‚\r\n- **å¯è§†åŒ–å›¾è¡¨**: ä½¿ç”¨Matplotlib, Seaborn, Plotlyç”Ÿæˆå„ç§å›¾è¡¨ï¼Œå¹¶æ”¯æŒ**è‡ªåŠ¨æ•è·**ã€‚\r\n- **æ–‡æ¡£è‡ªåŠ¨åŒ–**: åˆ›å»ºå¹¶æä¾›å¯ä¸‹è½½çš„Excel, Word, PDF, PPTæ–‡ä»¶ã€‚\r\n- **æœºå™¨å­¦ä¹ **: ä½¿ç”¨scikit-learnè¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚\r\n- **ç§‘å­¦ä¸æ•°å­¦è®¡ç®—**: ä½¿ç”¨Sympyå’ŒSciPyè¿›è¡Œé«˜çº§è®¡ç®—ã€‚\r\n- **æµç¨‹å›¾ç”Ÿæˆ**: ä½¿ç”¨Graphvizå’ŒNetworkXåˆ›å»ºç³»ç»Ÿæ¶æ„å›¾ã€æµç¨‹å›¾ã€ç½‘ç»œå…³ç³»å›¾\r\n- **æŒä¹…åŒ–æ–‡ä»¶æ“ä½œ**: åœ¨ç”¨æˆ·ä¼šè¯æœŸé—´ï¼Œæ”¯æŒåœ¨å·¥ä½œåŒºå†…è¯»å–å’Œå†™å…¥æ–‡ä»¶ã€‚\r\n\r\n---\r\n\r\n## ğŸ“ æ–‡ä»¶å¤„ç†æŒ‡å— (é‡è¦ï¼šä¸¤ç§æ¨¡å¼)\r\n\r\næœ¬å·¥å…·æ ¹æ®æ–‡ä»¶ç±»å‹é‡‡ç”¨ä¸åŒçš„å¤„ç†æ–¹å¼ï¼Œç†è§£è¿™ä¸€ç‚¹è‡³å…³é‡è¦ã€‚\r\n\r\n### **æ¨¡å¼A: æ•°æ®æ–‡ä»¶çš„ä¸Šä¼ ä¸è®¿é—® (åœ¨å·¥ä½œåŒº `/data`)**\r\n\r\nè¿™ç§æ–¹å¼é€‚ç”¨äºéœ€è¦ç”¨ä»£ç ï¼ˆå¦‚Pandasï¼‰ç›´æ¥è¯»å–å’Œåˆ†æçš„æ–‡ä»¶ã€‚\r\n\r\n- **æ”¯æŒçš„æ–‡ä»¶ç±»å‹**: `.xlsx`, `.xls`, `.parquet`, `.csv`, `.json`, `.txt`\r\n- **å·¥ä½œåŸç†**: è¿™äº›æ–‡ä»¶ä¼šè¢«**ä¸Šä¼ åˆ°æœåŠ¡å™¨çš„ä¼šè¯å·¥ä½œåŒº**ã€‚\r\n- **ä»£ç è®¿é—®æ–¹å¼**: åœ¨ä»£ç ä¸­ï¼Œä½ å¯ä»¥é€šè¿‡ç»å¯¹è·¯å¾„ `/data/æ–‡ä»¶å` æ¥è®¿é—®è¿™äº›æ–‡ä»¶ã€‚\r\n\r\n**è¯»å–ç¤ºä¾‹:**\r\n```python\r\nimport pandas as pd\r\ndf = pd.read_excel('/data/financial_report.xlsx')\r\nprint(df.head())\r\n```\r\n\r\n### **æ¨¡å¼B: åª’ä½“æ–‡ä»¶çš„å¤„ç† (åœ¨ä¸Šä¸‹æ–‡ä¸­)**\r\n\r\nè¿™ç§æ–¹å¼é€‚ç”¨äºå›¾ç‰‡ã€PDFç­‰ï¼Œæ¨¡å‹å¯ä»¥ç›´æ¥â€œçœ‹åˆ°â€å†…å®¹ï¼Œä½†ä»£ç æ²™ç›’**æ— æ³•**ä» `/data` ç›®å½•è®¿é—®å®ƒä»¬ã€‚\r\n\r\n- **æ”¯æŒçš„æ–‡ä»¶ç±»å‹**: å›¾ç‰‡ (`.png`, `.jpg`), `.pdf` ç­‰ã€‚\r\n- **å·¥ä½œåŸç†**: è¿™äº›æ–‡ä»¶ä¼šè¢«è½¬æ¢æˆBase64æ ¼å¼ï¼Œå¹¶ç›´æ¥**åµŒå…¥åˆ°ç»™æ¨¡å‹çš„æŒ‡ä»¤ä¸­**ã€‚å®ƒä»¬**ä¸ä¼š**å‡ºç°åœ¨æ²™ç›’çš„ `/data` ç›®å½•é‡Œã€‚\r\n- **ä½¿ç”¨åœºæ™¯**: å½“ä½ éœ€è¦æ¨¡å‹æè¿°ä¸€å¼ å›¾ç‰‡å†…å®¹ã€æˆ–æ€»ç»“ä¸€ä»½PDFæ–‡æ¡£æ—¶ï¼Œä½¿ç”¨æ­¤æ–¹å¼ã€‚\r\n\r\n---\r\n\r\n## ğŸš€ è¾“å‡ºè§„èŒƒï¼šå¦‚ä½•ä»æ²™ç›’è¿”å›ç»“æœ\r\n\r\næ²™ç›’ç¯å¢ƒéå¸¸æ™ºèƒ½ï¼Œå®ƒèƒ½è‡ªåŠ¨æ•è·å’Œæ ¼å¼åŒ–å¤šç§è¾“å‡ºã€‚è¯·éµå¾ªä»¥ä¸‹æœ€ä½³å®è·µã€‚\r\n\r\n### **1. å›¾è¡¨è¾“å‡º (é¦–é€‰ï¼šè‡ªåŠ¨æ•è·)**\r\n\r\nä½ **ä¸éœ€è¦**æ‰‹åŠ¨å°†å›¾è¡¨è½¬ä¸ºå›¾ç‰‡æˆ–Base64ã€‚è¿™æ˜¯æœ€ç®€å•ã€æœ€æ¨èçš„æ–¹å¼ã€‚\r\n\r\n- **æŒ‡ä»¤**: åªéœ€åƒåœ¨æœ¬åœ°ç¯å¢ƒä¸€æ ·ä½¿ç”¨ `matplotlib.pyplot.show()`ã€‚\r\n- **åŸç†**: ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹åˆ°ä½ ç”Ÿæˆäº†å›¾è¡¨ï¼Œæ•è·å®ƒï¼Œç„¶åä»¥å›¾ç‰‡å½¢å¼æ˜¾ç¤ºç»™ç”¨æˆ·ã€‚\r\n- **å‚è€ƒ**: `references/matplotlib_cookbook.md`\r\n\r\n### **2. å¯ä¸‹è½½æ–‡ä»¶è¾“å‡º (Word, Excel, PDF, PPT)**\r\n\r\nè¦ç”Ÿæˆå¹¶è®©ç”¨æˆ·ä¸‹è½½ä¸€ä¸ªæ–‡ä»¶ï¼Œä½ **å¿…é¡»**åœ¨ä»£ç çš„æœ€åï¼Œä½¿ç”¨ `print()` è¾“å‡ºä¸€ä¸ªç‰¹å®šæ ¼å¼çš„JSONå¯¹è±¡ã€‚\r\n\r\n- **JSONç»“æ„**: `{\"type\": \"æ–‡ä»¶ç±»å‹\", \"title\": \"æ–‡ä»¶å.åç¼€\", \"data_base64\": \"æ–‡ä»¶çš„Base64ç¼–ç å­—ç¬¦ä¸²\"}`\r\n- **æ”¯æŒçš„`type`**: `excel`, `word`, `ppt`, `pdf`\r\n- **å‚è€ƒ**: `references/report_generator_workflow.md`\r\n\r\n### **3. æ–‡æœ¬ä¸æ•°æ®è¾“å‡º**\r\n\r\nå¯¹äºä»»ä½•æ–‡æœ¬ã€æ•°å­—ã€Pandas DataFrameæˆ–å…¶ä»–åˆ†æç»“æœï¼Œç›´æ¥ä½¿ç”¨ `print()` å‡½æ•°è¾“å‡ºå³å¯ã€‚ç³»ç»Ÿä¼šå°†å…¶ä½œä¸ºæ ‡å‡†æ–‡æœ¬ç»“æœæ˜¾ç¤ºã€‚\r\n\r\n---\r\n\r\n## ğŸ’¾ ä¼šè¯å†…æ–‡ä»¶è¯»å†™\r\n\r\n**å…³é”®ç‰¹æ€§**: ä½ å¯ä»¥åœ¨ä¼šè¯æœŸé—´ï¼Œå°†æ–‡ä»¶ï¼ˆå¦‚ä¸­é—´ç»“æœã€æ¨¡å‹ã€å›¾ç‰‡ç­‰ï¼‰å†™å…¥åˆ° `/data` ç›®å½•ï¼Œå¹¶åœ¨**åŒä¸€æ¬¡ä¼šè¯**çš„åç»­ä»£ç æ‰§è¡Œä¸­å†æ¬¡è¯»å–å®ƒä»¬ã€‚\r\n\r\n- **æŒä¹…æ€§**: æ–‡ä»¶ä¼šä¿ç•™åœ¨å½“å‰ä¼šè¯çš„å·¥ä½œåŒºä¸­ï¼Œç›´åˆ°ä¼šè¯è¶…æ—¶ï¼ˆ24å°æ—¶ï¼‰ã€‚**æ–°çš„èŠå¤©ä¼šè¯ä¼šä½¿ç”¨ä¸€ä¸ªæ–°çš„ã€ç©ºçš„å·¥ä½œåŒºã€‚**\r\n\r\n**å·¥ä½œæµç¤ºä¾‹:**\r\n1.  **ç¬¬ä¸€æ¬¡è¿è¡Œ (å†™å…¥æ–‡ä»¶)**:\r\n    ```python\r\n    import pandas as pd\r\n    df = pd.read_excel('/data/uploaded_data.xlsx')\r\n    processed_df = df[df['value'] > 100]\r\n    processed_df.to_csv('/data/processed_data.csv', index=False)\r\n    print(\"æ•°æ®å·²å¤„ç†å¹¶ä¿å­˜åˆ° /data/processed_data.csv\")\r\n    ```\r\n2.  **ç¬¬äºŒæ¬¡è¿è¡Œ (è¯»å–æ–‡ä»¶)**:\r\n    ```python\r\n    import pandas as pd\r\n    final_df = pd.read_csv('/data/processed_data.csv')\r\n    print(final_df.head())\r\n    ```\r\n---\r\n\r\n## ğŸ“š å·¥ä½œæµä¸å‚è€ƒæŒ‡å—\r\n\r\nå½“ä½ éœ€è¦æ‰§è¡Œä¸€é¡¹å…·ä½“çš„ã€å¤æ‚çš„ä»»åŠ¡æ—¶ï¼Œ**è¯·é¦–å…ˆæŸ¥é˜…ç›¸å…³çš„å‚è€ƒæ–‡ä»¶**ä»¥è·å–æœ€ä½³å®è·µå’Œä»£ç æ¨¡æ¿ã€‚\r\n\r\n### **1. æ•°æ®å¯è§†åŒ–**\r\n- **ä»»åŠ¡**: åˆ›å»ºå›¾è¡¨ï¼Œå¦‚æ¡å½¢å›¾ã€æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ã€çƒ­åŠ›å›¾ç­‰\r\n- **æŒ‡ä»¤**: **å¿…é¡»æŸ¥é˜… `references/matplotlib_cookbook.md`**ã€‚è¯¥æ–‡ä»¶åŒ…å«äº†æ ‡å‡†çš„å›¾è¡¨ç”Ÿæˆæ¨¡æ¿ï¼Œç¡®ä¿äº†é«˜è´¨é‡çš„ã€ç»Ÿä¸€é£æ ¼çš„è¾“å‡º\r\n\r\n### **2. æ•°æ®æ¸…æ´—ä¸åˆ†æ**\r\n- **ä»»åŠ¡**: å¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ï¼Œè¿›è¡Œæè¿°æ€§ç»Ÿè®¡æˆ–ç›¸å…³æ€§åˆ†æ\r\n- **æŒ‡ä»¤**: **è¯·å‚è€ƒ `references/pandas_cheatsheet.md`** ä¸­çš„æ•°æ®æ¸…æ´—æµæ°´çº¿ç¤ºä¾‹\r\n\r\n### **3. è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ**\r\n- **ä»»åŠ¡**: ç”ŸæˆåŒ…å«å›¾è¡¨å’Œæ•°æ®çš„Wordã€Excelæˆ–PDFæŠ¥å‘Š\r\n- **æŒ‡ä»¤**: **éµå¾ª `references/report_generator_workflow.md`** ä¸­çš„å‘¨æŠ¥ç”Ÿæˆå™¨å·¥ä½œæµã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•ç»„åˆæ•°æ®ã€å›¾è¡¨å’Œæ–‡æ¡£åº“æ¥åˆ›å»ºå¤æ‚çš„æŠ¥å‘Š\r\n\r\n### **4. æœºå™¨å­¦ä¹ **\r\n- **ä»»åŠ¡**: è®­ç»ƒåˆ†ç±»æˆ–å›å½’æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½\r\n- **æŒ‡ä»¤**: **å­¦ä¹ å¹¶ä½¿ç”¨ `references/ml_workflow.md`** ä¸­çš„ä»£ç ç»“æ„æ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹\r\n\r\n### **5. ç¬¦å·æ•°å­¦ä¸å…¬å¼è¯æ˜**\r\n- **ä»»åŠ¡**: è§£ä»£æ•°æ–¹ç¨‹ã€è¿›è¡Œå¾®ç§¯åˆ†è®¡ç®—ã€ç®€åŒ–æ•°å­¦è¡¨è¾¾å¼ã€è¯æ˜æ•°å­¦å…¬å¼\r\n- **æŒ‡ä»¤**: **å¿…é¡»ä½¿ç”¨ `sympy` åº“ï¼Œå¹¶ä¸¥æ ¼å‚è€ƒ `references/sympy_cookbook.md`** ä¸­çš„å‡½æ•°å’Œç¤ºä¾‹æ¥æ„å»ºä½ çš„è§£å†³æ–¹æ¡ˆ\r\n\r\n### **6. ç§‘å­¦è®¡ç®—ä¸æ•°å€¼åˆ†æ (æ–°å¢)**\r\n- **ä»»åŠ¡**: è¿›è¡Œæ•°å€¼ä¼˜åŒ–ã€ç§¯åˆ†ã€ä¿¡å·å¤„ç†ã€çº¿æ€§ä»£æ•°ç­‰é«˜çº§ç§‘å­¦è®¡ç®—\r\n- **æŒ‡ä»¤**: **å½“éœ€è¦è¿›è¡Œå¤æ‚çš„æ•°å€¼è®¡ç®—æ—¶ï¼Œè¯·æŸ¥é˜… `references/scipy_cookbook.md`** ä»¥è·å–æ­£ç¡®çš„å‡½æ•°ç”¨æ³•å’Œç¤ºä¾‹\r\n\r\n### 7. æµç¨‹å›¾ä¸æ¶æ„å›¾ç”Ÿæˆ\r\n- **ä»»åŠ¡**: åˆ›å»ºç³»ç»Ÿæ¶æ„å›¾ã€æµç¨‹å›¾ã€ç½‘ç»œæ‹“æ‰‘å›¾\r\n- **æŒ‡ä»¤**: **è¯·å‚è€ƒ `references/matplotlib_cookbook.md` ä¸­çš„æµç¨‹å›¾ç« èŠ‚**\r\n- **é€‚ç”¨åœºæ™¯**: æŠ€æœ¯æ¶æ„è¯´æ˜ã€ç³»ç»Ÿè®¾è®¡ã€æµç¨‹å¯è§†åŒ–\r\n\r\n---\r\n\r\n## ğŸ’¡ æ ¸å¿ƒå·¥ä½œæµæ¨¡å¼\r\n\r\n### å…¬å¼è¯æ˜å·¥ä½œæµ\r\n1. **å®šä¹‰ç¬¦å·**: ä½¿ç”¨ `sympy.symbols()` å®šä¹‰æ‰€æœ‰å˜é‡\r\n2. **æ„å»ºè¡¨è¾¾å¼**: å°†å…¬å¼çš„å·¦è¾¹å’Œå³è¾¹æ„å»ºä¸ºä¸¤ä¸ªç‹¬ç«‹çš„`sympy`è¡¨è¾¾å¼\r\n3. **å°è¯•ç›´æ¥ç®€åŒ–**: ä½¿ç”¨ `sympy.simplify(LHS - RHS)`ï¼Œå¦‚æœç»“æœä¸º0ï¼Œåˆ™è¯æ˜æˆç«‹\r\n4. **è‹¥ä¸ä¸º0ï¼Œå°è¯•å˜æ¢**: ä½¿ç”¨ `expand()`, `factor()`, `trigsimp()` ç­‰å‡½æ•°å¯¹è¡¨è¾¾å¼è¿›è¡Œå˜æ¢ï¼Œå†æ¬¡å°è¯•æ­¥éª¤3\r\n5. **è¾“å‡ºæ­¥éª¤**: å°†ä½ çš„æ¯ä¸€æ­¥æ¨ç†å’Œä½¿ç”¨çš„`sympy`ä»£ç æ¸…æ™°åœ°å‘ˆç°å‡ºæ¥\r\n\r\n### ETLç®¡é“æ¨¡å¼ (Extract-Transform-Load)\r\n1. **Extract**: ä»æ•°æ®æºæå–åŸå§‹æ•°æ® (`/data` ç›®å½•ä¸­çš„æ–‡ä»¶)\r\n2. **Transform**: æ¸…æ´—ã€è½¬æ¢ã€å¤„ç†æ•°æ® (å¯ä»¥å°†ä¸­é—´ç»“æœå†™å…¥ `/data`)\r\n3. **Load**: ç”Ÿæˆè¾“å‡ºç»“æœï¼ˆå›¾è¡¨ã€å¯ä¸‹è½½æ–‡æ¡£ã€åˆ†ææŠ¥å‘Šï¼‰\r\n\r\n### åˆ†ææŠ¥å‘Šå·¥ä½œæµ\r\n1. **æ•°æ®æ”¶é›†**: è·å–æˆ–ç”Ÿæˆæ‰€éœ€æ•°æ®ã€‚\r\n2. **æ•°æ®å¤„ç†**: æ¸…æ´—ã€è½¬æ¢ã€åˆ†ææ•°æ®ã€‚\r\n3. **å¯è§†åŒ–**: åˆ›å»ºç›¸å…³å›¾è¡¨å’Œå¯è§†åŒ–ã€‚\r\n4. **æŠ¥å‘Šç”Ÿæˆ**: æ•´åˆæ•°æ®å’Œå›¾è¡¨åˆ°æœ€ç»ˆçš„å¯ä¸‹è½½æ–‡æ¡£ä¸­ã€‚\r\n\r\n---\r\n\r\n## ğŸ“‹ å¯ç”¨åº“å¿«é€Ÿå‚è€ƒ\r\n\r\n### æ•°æ®å¤„ç†\r\n- `pandas==2.2.2` - æ•°æ®åˆ†ææ ¸å¿ƒåº“\r\n- `numpy==1.26.4` - æ•°å€¼è®¡ç®—\r\n- `scipy==1.14.1` - ç§‘å­¦è®¡ç®—\r\n\r\n### å¯è§†åŒ–\r\n- `matplotlib==3.8.4` - åŸºç¡€ç»˜å›¾åº“\r\n- `seaborn==0.13.2` - ç»Ÿè®¡å¯è§†åŒ–\r\n- `plotly==5.18.0` - äº¤äº’å¼å›¾è¡¨\r\n\r\n### æ–‡æ¡£ç”Ÿæˆ\r\n- `python-docx==1.1.2` - Wordæ–‡æ¡£\r\n- `reportlab==4.0.7` - PDFç”Ÿæˆ\r\n- `python-pptx==0.6.23` - PPTæ¼”ç¤ºæ–‡ç¨¿\r\n- `openpyxl==3.1.2` - Excelæ–‡ä»¶æ“ä½œ\r\n\r\n### æœºå™¨å­¦ä¹ ä¸æ•°å­¦\r\n- `scikit-learn==1.4.2` - æœºå™¨å­¦ä¹ \r\n- `sympy==1.12` - ç¬¦å·æ•°å­¦\r\n- `statsmodels==0.14.1` - ç»Ÿè®¡æ¨¡å‹\r\n\r\n### æµç¨‹å›¾ä¸ç½‘ç»œå›¾\r\n- `graphviz` - ä¸“ä¸šå›¾è¡¨ç”Ÿæˆï¼ˆè‡ªåŠ¨å¸ƒå±€ï¼Œé€‚åˆæµç¨‹å›¾ã€æ¶æ„å›¾ï¼‰\r\n- `pydot` - Graphvizæ¥å£åº“\r\n- `networkx` - å¤æ‚ç½‘ç»œåˆ†æå’Œå¯è§†åŒ–\r\n\r\n---\r\n\r\n## ğŸš¨ é‡è¦æé†’\r\n\r\n1.  **å›¾è¡¨ä¼˜å…ˆä½¿ç”¨ `plt.show()`**ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨æ•è·ã€‚\r\n2.  **ç”Ÿæˆå¯ä¸‹è½½æ–‡ä»¶å¿…é¡» `print()` æŒ‡å®šçš„JSONæ ¼å¼**ã€‚\r\n3.  **åˆ†æ¸…ä¸¤ç§æ–‡ä»¶è¾“å…¥**: ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶åœ¨ `/data` ä¸­ï¼Œé™„åŠ çš„åª’ä½“æ–‡ä»¶åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\r\n4.  **åˆ©ç”¨ä¼šè¯æ–‡ä»¶ç³»ç»Ÿ**: ä½ å¯ä»¥å‘ `/data` ç›®å½•å†™å…¥å’Œè¯»å–æ–‡ä»¶ï¼Œè¿™åœ¨å¤šæ­¥éª¤çš„å¤æ‚åˆ†æä¸­éå¸¸æœ‰ç”¨ã€‚\r\n5.  **æŒ‰éœ€åŠ è½½**: å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œä¼˜å…ˆå‚è€ƒå¯¹åº”çš„referencesæ–‡ä»¶ã€‚\n\n<hr>\n\n## ğŸ“š å‚è€ƒæŒ‡å— (Reference Manuals)\n\n### ğŸ“– matplotlib_cookbook\n\n# Matplotlib å›¾è¡¨ç”ŸæˆæŒ‡å— (v2.2)\n\n## ğŸš€ æ ¸å¿ƒä½¿ç”¨æ–¹æ³•\n\n**é‡è¦æç¤º**ï¼šæ‚¨åªéœ€è¦ä¸“æ³¨äºç»˜å›¾é€»è¾‘ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†å›¾åƒè¾“å‡ºã€‚\n\n### å¿…é¡»éµå¾ªçš„åŸåˆ™ï¼š\n1. **æ­£å¸¸å¯¼å…¥**ï¼š`import matplotlib.pyplot as plt`\n2. **æ­£å¸¸ç»˜å›¾**ï¼šä½¿ç”¨æ ‡å‡†çš„matplotlibå‡½æ•°\n3. **æ— éœ€ç¼–ç **ï¼šç¦æ­¢ä½¿ç”¨`io.BytesIO`ã€`base64`ç­‰æ‰‹åŠ¨ç¼–ç \n4. **æ¨èä½¿ç”¨**ï¼šåœ¨ä»£ç æœ«å°¾è°ƒç”¨`plt.show()`\n\n## ğŸ“Š å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\n\n### æ¨¡æ¿1ï¼šåŸºç¡€æ¡å½¢å›¾\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# å‡†å¤‡æ•°æ®\ndata = {'Category': ['A', 'B', 'C', 'D'], 'Values': [23, 45, 56, 33]}\ndf = pd.DataFrame(data)\n\n# ç»˜å›¾\nplt.figure(figsize=(10, 6))\nplt.bar(df['Category'], df['Values'], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\nplt.title('äº§å“é”€å”®é¢å¯¹æ¯”')\nplt.xlabel('äº§å“ç±»åˆ«')\nplt.ylabel('é”€å”®é¢ (ä¸‡å…ƒ)')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\n\nplt.show()\n```\n\n### æ¨¡æ¿2ï¼šæŠ˜çº¿å›¾\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# æ—¶é—´åºåˆ—æ•°æ®\ndata = {'Time': [1, 2, 3, 4, 5], 'Value': [10, 20, 15, 25, 30]}\ndf = pd.DataFrame(data)\n\nplt.figure(figsize=(10, 6))\nplt.plot(df['Time'], df['Value'], marker='o', linestyle='-', linewidth=2)\nplt.title('æ•°æ®è¶‹åŠ¿åˆ†æ')\nplt.xlabel('æ—¶é—´')\nplt.ylabel('æ•°å€¼')\nplt.grid(True)\nplt.tight_layout()\n\nplt.show()\n```\n\n### æ¨¡æ¿3ï¼šæ•£ç‚¹å›¾\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# ç”Ÿæˆç¤ºä¾‹æ•°æ®\nx = np.random.randn(100)\ny = np.random.randn(100)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.6)\nplt.title('æ•£ç‚¹å›¾ç¤ºä¾‹')\nplt.xlabel('Xè½´')\nplt.ylabel('Yè½´')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\n\nplt.show()\n```\n\n### æ¨¡æ¿4ï¼šå¤šå­å›¾å¸ƒå±€\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.plot(x, y1, 'b-', linewidth=2)\nax1.set_title('æ­£å¼¦å‡½æ•°')\nax1.grid(True)\n\nax2.plot(x, y2, 'r-', linewidth=2)\nax2.set_title('ä½™å¼¦å‡½æ•°')\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ¨ å›¾è¡¨ç±»å‹é€‰æ‹©æŒ‡å—\n\n### æ•°æ®æ¯”è¾ƒï¼š\n- **æ¡å½¢å›¾**ï¼šæ¯”è¾ƒä¸åŒç±»åˆ«çš„æ•°å€¼\n- **æ°´å¹³æ¡å½¢å›¾**ï¼šç±»åˆ«åç§°è¾ƒé•¿æ—¶ä½¿ç”¨\n\n### è¶‹åŠ¿åˆ†æï¼š\n- **æŠ˜çº¿å›¾**ï¼šæ˜¾ç¤ºæ•°æ®éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿\n- **é¢ç§¯å›¾**ï¼šæ˜¾ç¤ºç´¯ç§¯æ•ˆæœ\n\n### åˆ†å¸ƒåˆ†æï¼š\n- **ç›´æ–¹å›¾**ï¼šæ˜¾ç¤ºæ•°æ®åˆ†å¸ƒ\n- **ç®±çº¿å›¾**ï¼šæ˜¾ç¤ºæ•°æ®åˆ†å¸ƒå’Œå¼‚å¸¸å€¼\n- **æ•£ç‚¹å›¾**ï¼šè§‚å¯Ÿä¸¤ä¸ªå˜é‡çš„å…³ç³»\n\n### æ¯”ä¾‹åˆ†æï¼š\n- **é¥¼å›¾**ï¼šæ˜¾ç¤ºå„éƒ¨åˆ†å æ¯”\n- **ç¯å½¢å›¾**ï¼šé¥¼å›¾çš„å˜ä½“\n\n## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹\n\n### å¿…é¡»åŒ…å«ï¼š\n- `import matplotlib.pyplot as plt`\n- æœ‰æ„ä¹‰çš„`plt.title()`ï¼ˆæ ‡é¢˜ä¼šè¢«è‡ªåŠ¨æ•è·ï¼‰\n- `plt.show()`ï¼ˆæ¨èä½†éå¿…é¡»ï¼‰\n\n### ç¦æ­¢æ“ä½œï¼š\n- âŒ ä¸è¦ä½¿ç”¨`base64.b64encode()`\n- âŒ ä¸è¦åˆ›å»º`io.BytesIO()`å¯¹è±¡\n- âŒ ä¸è¦æ‰‹åŠ¨æ„å»ºJSONè¾“å‡º\n\n### æœ€ä½³å®è·µï¼š\n- ä½¿ç”¨`plt.tight_layout()`è‡ªåŠ¨è°ƒæ•´å¸ƒå±€\n- ä½¿ç”¨`plt.grid()`æ·»åŠ ç½‘æ ¼æé«˜å¯è¯»æ€§\n- è®¾ç½®åˆé€‚çš„`figsize`ç¡®ä¿å›¾è¡¨æ¸…æ™°\n\n## ğŸ”§ æ ·å¼é…ç½®ä¸ä¸­æ–‡æ”¯æŒ (å…³é”®)\n\næœ¬ç¯å¢ƒå·²é¢„è£…å¼€æºä¸­æ–‡å­—ä½“ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ä»¥ä¸‹é…ç½®ä»¥é¿å…ä¸­æ–‡ä¹±ç ã€‚\n\n### âœ… æ¨èçš„ä¸­æ–‡å­—ä½“é…ç½®ï¼š\n```python\nimport matplotlib.pyplot as plt\n\n# å¿…é¡»æŒ‡å®šç¯å¢ƒå†…çœŸå®å­˜åœ¨çš„å­—ä½“å\n# ä¼˜å…ˆçº§ï¼šWenQuanYi Micro Hei > WenQuanYi Zen Hei\nplt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei']\nplt.rcParams['axes.unicode_minus'] = False # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n\n# è®¾ç½®å…¨å±€æ ·å¼ï¼ˆå¯é€‰ï¼‰\nplt.style.use('seaborn-v0_8')\nplt.rcParams['font.size'] = 12\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# æ‚¨çš„ç»˜å›¾ä»£ç ...\nplt.plot([1, 2, 3, 4], [1, 4, 2, 3])\nplt.title('å¸¦æ ·å¼é…ç½®çš„å›¾è¡¨')\nplt.show()\n```\n### âŒ ç¦æ­¢ä½¿ç”¨çš„å­—ä½“ (ç¯å¢ƒå†…ä¸å­˜åœ¨)ï¼š\nä¸è¦ä½¿ç”¨ SimHei\nä¸è¦ä½¿ç”¨ Microsoft YaHei\nä¸è¦ä½¿ç”¨ Songti\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨æ•è·æ‰€æœ‰å›¾è¡¨å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç»˜å›¾é€»è¾‘ï¼\n\n\n## ğŸ—ï¸ æµç¨‹å›¾ä¸æ¶æ„å›¾ç”ŸæˆæŒ‡å—\n\n### ä½¿ç”¨åœºæ™¯å¯¹æ¯”\n| éœ€æ±‚ç±»å‹ | æ¨èå·¥å…· | è¾“å‡ºç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |\n|----------|----------|----------|----------|\n| æ•°æ®å›¾è¡¨ | Matplotlib | æ•°æ®é©±åŠ¨ï¼Œæ ·å¼ä¸°å¯Œ | æ•°æ®åˆ†æã€ç»Ÿè®¡å›¾è¡¨ |\n| ä¸“ä¸šæµç¨‹å›¾ | Graphviz | è‡ªåŠ¨å¸ƒå±€ï¼Œæ ·å¼ç»Ÿä¸€ | ç³»ç»Ÿæ¶æ„ã€æµç¨‹å›¾ |\n| ç½‘ç»œå…³ç³»å›¾ | NetworkX | å¤æ‚å…³ç³»ï¼Œç®—æ³•æ”¯æŒ | ç¤¾äº¤ç½‘ç»œã€æ‹“æ‰‘å›¾ |\n\n### Graphviz ä¸“ä¸šæµç¨‹å›¾\n\n#### åŸºç¡€æµç¨‹å›¾æ¨¡æ¿\n```python\nfrom graphviz import Digraph\n\ndef create_basic_flowchart():\n    # åˆ›å»ºæœ‰å‘å›¾\n    dot = Digraph('BasicFlow', comment='åŸºç¡€æµç¨‹å›¾')\n    dot.attr(rankdir='TB', size='8,5')  # å¸ƒå±€æ–¹å‘ï¼šTB(ä»ä¸Šåˆ°ä¸‹), LR(ä»å·¦åˆ°å³)\n    \n    # æ·»åŠ èŠ‚ç‚¹ï¼ˆä¸åŒå½¢çŠ¶ä»£è¡¨ä¸åŒç±»å‹ï¼‰\n    dot.node('start', 'å¼€å§‹', shape='ellipse', color='green')\n    dot.node('process1', 'æ•°æ®å¤„ç†', shape='box')\n    dot.node('decision', 'åˆ¤æ–­æ¡ä»¶', shape='diamond', color='blue')\n    dot.node('process2', 'åç»­å¤„ç†', shape='box')\n    dot.node('end', 'ç»“æŸ', shape='ellipse', color='red')\n    \n    # æ·»åŠ è¿æ¥çº¿\n    dot.edge('start', 'process1', label='è¾“å…¥')\n    dot.edge('process1', 'decision', label='ç»“æœ')\n    dot.edge('decision', 'process2', label='æ˜¯', color='green')\n    dot.edge('decision', 'end', label='å¦', color='red')\n    dot.edge('process2', 'end', label='å®Œæˆ')\n    \n    # ä¿å­˜åˆ°å·¥ä½œåŒºï¼ˆé‡è¦ï¼šå¿…é¡»æŒ‡å®šç»å¯¹è·¯å¾„ï¼‰\n    dot.render('/data/basic_flowchart', format='png', cleanup=True)\n    print(\"æµç¨‹å›¾å·²ä¿å­˜åˆ°å·¥ä½œåŒºï¼š/data/basic_flowchart.png\")\n\ncreate_basic_flowchart()\n```\n\n#### ç³»ç»Ÿæ¶æ„å›¾æ¨¡æ¿\n```python\nfrom graphviz import Digraph\n\ndef create_system_architecture():\n    dot = Digraph('SystemArch', comment='ç³»ç»Ÿæ¶æ„å›¾')\n    dot.attr(rankdir='LR', size='12,8')  # ä»å·¦åˆ°å³å¸ƒå±€\n    \n    # å®šä¹‰èŠ‚ç‚¹ç»„\n    with dot.subgraph(name='cluster_frontend') as frontend:\n        frontend.attr(label='å‰ç«¯å±‚', style='filled', color='lightgrey')\n        frontend.node('web', 'Webåº”ç”¨', shape='box')\n        frontend.node('mobile', 'ç§»åŠ¨ç«¯', shape='box')\n    \n    with dot.subgraph(name='cluster_backend') as backend:\n        backend.attr(label='åç«¯æœåŠ¡', style='filled', color='lightblue')\n        backend.node('api', 'APIç½‘å…³', shape='box')\n        backend.node('auth', 'è®¤è¯æœåŠ¡', shape='box')\n        backend.node('business', 'ä¸šåŠ¡é€»è¾‘', shape='box')\n    \n    with dot.subgraph(name='cluster_data') as data:\n        data.attr(label='æ•°æ®å±‚', style='filled', color='lightgreen')\n        data.node('db', 'æ•°æ®åº“', shape='cylinder')\n        data.node('cache', 'ç¼“å­˜', shape='cylinder')\n    \n    # è¿æ¥å„å±‚\n    dot.edge('web', 'api', label='HTTP')\n    dot.edge('mobile', 'api', label='REST')\n    dot.edge('api', 'auth', label='éªŒè¯')\n    dot.edge('api', 'business', label='è¯·æ±‚')\n    dot.edge('business', 'db', label='æŸ¥è¯¢')\n    dot.edge('business', 'cache', label='è¯»å†™')\n    \n    dot.render('/data/system_architecture', format='png', cleanup=True)\n    print(\"ç³»ç»Ÿæ¶æ„å›¾å·²ä¿å­˜åˆ°å·¥ä½œåŒº\")\n\ncreate_system_architecture()\n```\n\n### NetworkX ç½‘ç»œå…³ç³»å›¾\n\n#### åŸºç¡€ç½‘ç»œå›¾æ¨¡æ¿\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\ndef create_network_diagram():\n    # åˆ›å»ºæœ‰å‘å›¾\n    G = nx.DiGraph()\n    \n    # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹\n    G.add_edge('æ•°æ®æº', 'ETLå¤„ç†')\n    G.add_edge('ETLå¤„ç†', 'æ•°æ®ä»“åº“')\n    G.add_edge('æ•°æ®ä»“åº“', 'æ•°æ®åˆ†æ')\n    G.add_edge('æ•°æ®åˆ†æ', 'å¯è§†åŒ–')\n    G.add_edge('å¯è§†åŒ–', 'ä¸šåŠ¡å†³ç­–')\n    \n    # è®¾ç½®ç»˜å›¾æ ·å¼\n    plt.figure(figsize=(12, 8))\n    \n    # é€‰æ‹©å¸ƒå±€ç®—æ³•\n    pos = nx.spring_layout(G, k=1, iterations=50)\n    \n    # ç»˜åˆ¶èŠ‚ç‚¹å’Œè¾¹\n    nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n                          node_size=2000, alpha=0.9)\n    nx.draw_networkx_edges(G, pos, edge_color='gray', \n                          arrows=True, arrowsize=20)\n    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n    \n    # æ·»åŠ æ ‡é¢˜å’Œè°ƒæ•´å¸ƒå±€\n    plt.title('æ•°æ®å¤„ç†æµæ°´çº¿ç½‘ç»œå›¾', size=16, pad=20)\n    plt.axis('off')\n    plt.tight_layout()\n    \n    # ä¿å­˜åˆ°å·¥ä½œåŒº\n    plt.savefig('/data/network_pipeline.png', dpi=150, bbox_inches='tight')\n    plt.close()\n    print(\"ç½‘ç»œå›¾å·²ä¿å­˜åˆ°å·¥ä½œåŒºï¼š/data/network_pipeline.png\")\n\ncreate_network_diagram()\n```\n\n#### å¤æ‚ç½‘ç»œåˆ†ææ¨¡æ¿\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_complex_network():\n    # åˆ›å»ºéšæœºç½‘ç»œ\n    G = nx.erdos_renyi_graph(30, 0.1)\n    \n    # è®¡ç®—ç½‘ç»œæŒ‡æ ‡\n    degrees = dict(G.degree())\n    betweenness = nx.betweenness_centrality(G)\n    \n    # è®¾ç½®èŠ‚ç‚¹å¤§å°å’Œé¢œè‰²åŸºäºä¸­å¿ƒæ€§\n    node_sizes = [v * 500 for v in degrees.values()]\n    node_colors = list(betweenness.values())\n    \n    # ç»˜åˆ¶å›¾å½¢\n    plt.figure(figsize=(14, 10))\n    pos = nx.spring_layout(G, seed=42)\n    \n    # ç»˜åˆ¶ç½‘ç»œ\n    nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes,\n                                 node_color=node_colors, \n                                 cmap=plt.cm.viridis, alpha=0.8)\n    nx.draw_networkx_edges(G, pos, alpha=0.5)\n    nx.draw_networkx_labels(G, pos, font_size=8)\n    \n    # æ·»åŠ é¢œè‰²æ¡\n    plt.colorbar(nodes, label='ä»‹æ•°ä¸­å¿ƒæ€§')\n    plt.title('å¤æ‚ç½‘ç»œåˆ†æå›¾ï¼ˆèŠ‚ç‚¹å¤§å°=åº¦ï¼Œé¢œè‰²=ä¸­å¿ƒæ€§ï¼‰', size=14)\n    plt.axis('off')\n    \n    # ä¿å­˜ç»“æœ\n    plt.savefig('/data/complex_network.png', dpi=150, bbox_inches='tight')\n    plt.close()\n    \n    # è¾“å‡ºç½‘ç»œç»Ÿè®¡ä¿¡æ¯\n    print(f\"ç½‘ç»œç»Ÿè®¡:\")\n    print(f\"- èŠ‚ç‚¹æ•°: {G.number_of_nodes()}\")\n    print(f\"- è¾¹æ•°: {G.number_of_edges()}\")\n    print(f\"- å¹³å‡åº¦: {np.mean(list(degrees.values())):.2f}\")\n    print(\"ç½‘ç»œå›¾å·²ä¿å­˜åˆ°å·¥ä½œåŒº\")\n\ncreate_complex_network()\n```\n\n### æœ€ä½³å®è·µä¸æ³¨æ„äº‹é¡¹\nâœ… æ¨èåšæ³•ï¼š\n- Graphviz ç”¨äºï¼šæµç¨‹å›¾ã€æ¶æ„å›¾ã€ç±»å›¾ç­‰éœ€è¦ä¸“ä¸šå¸ƒå±€çš„å›¾è¡¨\n- NetworkX + Matplotlib ç”¨äºï¼šæ•°æ®å…³ç³»ç½‘ç»œã€ç¤¾äº¤ç½‘ç»œã€æ‹“æ‰‘åˆ†æ\n- çº¯ Matplotlib ç”¨äºï¼šæ•°æ®å¯è§†åŒ–ã€ç»Ÿè®¡å›¾è¡¨\n\nâš ï¸ é‡è¦æé†’ï¼š\n- Graphviz å¿…é¡»æŒ‡å®šç»å¯¹è·¯å¾„ï¼š`/data/æ–‡ä»¶å`\n- æ¸…ç†ä¸­é—´æ–‡ä»¶ï¼šä½¿ç”¨ `cleanup=True` åˆ é™¤ä¸´æ—¶æ–‡ä»¶\n- å†…å­˜ç®¡ç†ï¼šå¤æ‚ç½‘ç»œåˆ†ææ—¶æ³¨æ„èŠ‚ç‚¹æ•°é‡\n- æ–‡ä»¶æ ¼å¼ï¼šæ”¯æŒ PNGã€PDFã€SVG ç­‰æ ¼å¼\n\nğŸ”§ æ•…éšœæ’é™¤ï¼š\n```python\n# éªŒè¯ Graphviz å®‰è£…\ndef check_graphviz_installation():\n    try:\n        from graphviz import Digraph\n        dot = Digraph()\n        dot.node('test', 'Test')\n        dot.render('/data/test_graphviz', format='png', cleanup=True)\n        print(\"âœ… Graphviz å·¥ä½œæ­£å¸¸\")\n        return True\n    except Exception as e:\n        print(f\"âŒ Graphviz é”™è¯¯: {e}\")\n        return False\n\ncheck_graphviz_installation()\n```\n**è®°ä½**ï¼šé€‰æ‹©åˆé€‚çš„å·¥å…·å¯ä»¥è®©å›¾è¡¨æ›´åŠ ä¸“ä¸šå’Œæ¸…æ™°ï¼\n\n\n### ğŸ“– ml_workflow\n\n# æœºå™¨å­¦ä¹ å·¥ä½œæµæŒ‡å— (v2.2)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**åŠŸèƒ½**ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼\n\n## ğŸ“Š åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡æ¿\n\n### æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef prepare_ml_data():\n    \"\"\"æœºå™¨å­¦ä¹ æ•°æ®å‡†å¤‡ç¤ºä¾‹\"\"\"\n    \n    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†\n    np.random.seed(42)\n    n_samples = 1000\n    \n    # å›å½’é—®é¢˜æ•°æ®\n    X_reg = np.random.normal(0, 1, (n_samples, 5))\n    y_reg = 2 * X_reg[:, 0] + 1.5 * X_reg[:, 1] - X_reg[:, 2] + np.random.normal(0, 0.5, n_samples)\n    \n    # åˆ†ç±»é—®é¢˜æ•°æ®\n    X_clf = np.random.normal(0, 1, (n_samples, 4))\n    y_clf = (X_clf[:, 0] + X_clf[:, 1] > 0).astype(int)\n    \n    print(\"=== æ•°æ®å‡†å¤‡å®Œæˆ ===\")\n    print(f\"æ ·æœ¬æ•°é‡: {n_samples}\")\n    print(f\"å›å½’ç‰¹å¾ç»´åº¦: {X_reg.shape[1]}\")\n    print(f\"åˆ†ç±»ç‰¹å¾ç»´åº¦: {X_clf.shape[1]}\")\n    print(f\"åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(y_clf, return_counts=True)}\")\n    \n    return X_reg, y_reg, X_clf, y_clf\n\n# ä½¿ç”¨ç¤ºä¾‹\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\n```\n\n### æ ‡å‡†æœºå™¨å­¦ä¹ å·¥ä½œæµ\n```python\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\nfrom sklearn.model_selection import cross_val_score\n\ndef standard_ml_pipeline(X, y, problem_type='regression'):\n    \"\"\"æ ‡å‡†æœºå™¨å­¦ä¹ æµç¨‹\"\"\"\n    \n    print(f\"=== å¼€å§‹ {problem_type} æ¨¡å‹è®­ç»ƒ ===\")\n    \n    # æ•°æ®åˆ†å‰²\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42,\n        stratify=y if problem_type == 'classification' else None\n    )\n    \n    print(f\"è®­ç»ƒé›†å¤§å°: {X_train.shape}\")\n    print(f\"æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\n    \n    # ç‰¹å¾æ ‡å‡†åŒ–\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # é€‰æ‹©æ¨¡å‹\n    if problem_type == 'regression':\n        model = RandomForestRegressor(n_estimators=100, random_state=42)\n    else:\n        model = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    # è®­ç»ƒæ¨¡å‹\n    model.fit(X_train_scaled, y_train)\n    \n    # é¢„æµ‹\n    y_pred = model.predict(X_test_scaled)\n    \n    # æ¨¡å‹è¯„ä¼°\n    if problem_type == 'regression':\n        mse = mean_squared_error(y_test, y_pred)\n        rmse = np.sqrt(mse)\n        r2 = r2_score(y_test, y_pred)\n        \n        print(f\"å›å½’æ¨¡å‹æ€§èƒ½:\")\n        print(f\"  MSE: {mse:.4f}\")\n        print(f\"  RMSE: {rmse:.4f}\")\n        print(f\"  RÂ²: {r2:.4f}\")\n        \n        metrics = {'mse': mse, 'rmse': rmse, 'r2': r2}\n    else:\n        accuracy = accuracy_score(y_test, y_pred)\n        print(f\"åˆ†ç±»æ¨¡å‹æ€§èƒ½:\")\n        print(f\"  å‡†ç¡®ç‡: {accuracy:.4f}\")\n        print(\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n        print(classification_report(y_test, y_pred))\n        \n        metrics = {'accuracy': accuracy}\n    \n    # äº¤å‰éªŒè¯\n    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, \n                               scoring='r2' if problem_type == 'regression' else 'accuracy')\n    print(f\"äº¤å‰éªŒè¯å¹³å‡å¾—åˆ†: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})\")\n    \n    return {\n        'model': model,\n        'metrics': metrics,\n        'X_test': X_test,\n        'y_test': y_test,\n        'y_pred': y_pred,\n        'cv_scores': cv_scores\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\n# regression_results = standard_ml_pipeline(X_reg, y_reg, 'regression')\n# classification_results = standard_ml_pipeline(X_clf, y_clf, 'classification')\n```\n\n## ğŸ“ˆ å›å½’åˆ†æå®Œæ•´å·¥ä½œæµ\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef complete_regression_analysis():\n    \"\"\"å®Œæ•´çš„å›å½’åˆ†æå·¥ä½œæµ\"\"\"\n    \n    print(\"=== å¼€å§‹å›å½’åˆ†æ ===\")\n    \n    # 1. æ•°æ®ç”Ÿæˆ\n    np.random.seed(42)\n    n_samples = 500\n    \n    # åˆ›å»ºæœ‰æ„ä¹‰çš„ç‰¹å¾\n    feature1 = np.random.normal(50, 15, n_samples)  # å¹´é¾„\n    feature2 = np.random.normal(100, 25, n_samples) # æ”¶å…¥\n    feature3 = np.random.normal(10, 3, n_samples)   # æ•™è‚²å¹´é™\n    feature4 = np.random.normal(0, 1, n_samples)    # å™ªå£°ç‰¹å¾\n    \n    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆæ¨¡æ‹Ÿæˆ¿ä»·ï¼‰\n    target = (50 * feature1 + 80 * feature2 + 5000 * feature3 + \n              10 * feature1 * feature3 + np.random.normal(0, 10000, n_samples))\n    \n    df = pd.DataFrame({\n        'å¹´é¾„': feature1,\n        'æ”¶å…¥': feature2,\n        'æ•™è‚²å¹´é™': feature3,\n        'å™ªå£°ç‰¹å¾': feature4,\n        'æˆ¿ä»·': target\n    })\n    \n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\n    print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n    print(f\"ç‰¹å¾åˆ—è¡¨: {list(df.columns[:-1])}\")\n    print(f\"ç›®æ ‡å˜é‡: {df.columns[-1]}\")\n    \n    # 2. æ•°æ®æ¢ç´¢\n    print(\"\\n=== æ•°æ®æ¢ç´¢ ===\")\n    print(\"æ•°å€¼ç‰¹å¾ç»Ÿè®¡:\")\n    print(df.describe())\n    \n    # ç›¸å…³æ€§åˆ†æ\n    correlation = df.corr()['æˆ¿ä»·'].sort_values(ascending=False)\n    print(\"\\nç‰¹å¾ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§:\")\n    for feature, corr in correlation.items():\n        if feature != 'æˆ¿ä»·':\n            print(f\"  {feature}: {corr:.3f}\")\n    \n    # 3. æ¨¡å‹è®­ç»ƒ\n    X = df.drop('æˆ¿ä»·', axis=1)\n    y = df['æˆ¿ä»·']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    model = RandomForestRegressor(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    # 4. æ¨¡å‹è¯„ä¼°\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½ ===\")\n    print(f\"å‡æ–¹è¯¯å·® (MSE): {mse:,.2f}\")\n    print(f\"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:,.2f}\")\n    print(f\"å†³å®šç³»æ•° (RÂ²): {r2:.4f}\")\n    \n    # 5. ç‰¹å¾é‡è¦æ€§\n    feature_importance = pd.DataFrame({\n        'ç‰¹å¾': X.columns,\n        'é‡è¦æ€§': model.feature_importances_\n    }).sort_values('é‡è¦æ€§', ascending=False)\n    \n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ ===\")\n    for _, row in feature_importance.iterrows():\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\n    \n    # 6. å¯è§†åŒ–åˆ†æ\n    plt.figure(figsize=(15, 10))\n    \n    # å®é™…å€¼ vs é¢„æµ‹å€¼\n    plt.subplot(2, 3, 1)\n    plt.scatter(y_test, y_pred, alpha=0.6)\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n    plt.xlabel('å®é™…å€¼')\n    plt.ylabel('é¢„æµ‹å€¼')\n    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')\n    plt.grid(True, alpha=0.3)\n    \n    # æ®‹å·®åˆ†æ\n    plt.subplot(2, 3, 2)\n    residuals = y_test - y_pred\n    plt.scatter(y_pred, residuals, alpha=0.6)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('é¢„æµ‹å€¼')\n    plt.ylabel('æ®‹å·®')\n    plt.title('æ®‹å·®åˆ†æ')\n    plt.grid(True, alpha=0.3)\n    \n    # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\n    plt.subplot(2, 3, 3)\n    top_features = feature_importance.head(5)\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\n    plt.xlabel('é‡è¦æ€§')\n    plt.title('Top 5 ç‰¹å¾é‡è¦æ€§')\n    plt.gca().invert_yaxis()\n    \n    # è¯¯å·®åˆ†å¸ƒ\n    plt.subplot(2, 3, 4)\n    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n    plt.xlabel('æ®‹å·®')\n    plt.ylabel('é¢‘æ•°')\n    plt.title('è¯¯å·®åˆ†å¸ƒ')\n    plt.grid(True, alpha=0.3)\n    \n    # ç›¸å¯¹è¯¯å·®\n    plt.subplot(2, 3, 5)\n    relative_error = np.abs(residuals / y_test) * 100\n    plt.hist(relative_error, bins=30, alpha=0.7, edgecolor='black')\n    plt.xlabel('ç›¸å¯¹è¯¯å·® (%)')\n    plt.ylabel('é¢‘æ•°')\n    plt.title('ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ')\n    plt.grid(True, alpha=0.3)\n    \n    # é¢„æµ‹è¯¯å·®ç®±çº¿å›¾\n    plt.subplot(2, 3, 6)\n    plt.boxplot(relative_error)\n    plt.ylabel('ç›¸å¯¹è¯¯å·® (%)')\n    plt.title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 7. æ¨¡å‹è§£é‡Š\n    print(f\"\\n=== æ¨¡å‹è§£é‡Š ===\")\n    print(f\"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if r2 > 0.8 else 'è‰¯å¥½' if r2 > 0.6 else 'ä¸€èˆ¬'}\")\n    print(f\"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}\")\n    print(f\"å»ºè®®: å…³æ³¨{feature_importance.iloc[0]['ç‰¹å¾']}å’Œ{feature_importance.iloc[1]['ç‰¹å¾']}çš„ä¼˜åŒ–\")\n    \n    return {\n        'model': model,\n        'metrics': {'mse': mse, 'rmse': rmse, 'r2': r2},\n        'feature_importance': feature_importance,\n        'predictions': y_pred\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# regression_results = complete_regression_analysis()\n```\n\n## ğŸ” åˆ†ç±»åˆ†æå®Œæ•´å·¥ä½œæµ\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import make_classification\n\ndef complete_classification_analysis():\n    \"\"\"å®Œæ•´çš„åˆ†ç±»åˆ†æå·¥ä½œæµ\"\"\"\n    \n    print(\"=== å¼€å§‹åˆ†ç±»åˆ†æ ===\")\n    \n    # 1. æ•°æ®ç”Ÿæˆ\n    X, y = make_classification(\n        n_samples=1000,\n        n_features=8,\n        n_informative=5,\n        n_redundant=2,\n        n_classes=3,\n        random_state=42\n    )\n    \n    feature_names = [f'ç‰¹å¾_{i+1}' for i in range(X.shape[1])]\n    df = pd.DataFrame(X, columns=feature_names)\n    df['ç±»åˆ«'] = y\n    \n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\n    print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n    print(f\"ç‰¹å¾æ•°é‡: {X.shape[1]}\")\n    print(f\"ç±»åˆ«æ•°é‡: {len(np.unique(y))}\")\n    print(f\"ç±»åˆ«åˆ†å¸ƒ: {np.unique(y, return_counts=True)}\")\n    \n    # 2. æ•°æ®æ¢ç´¢\n    print(\"\\n=== æ•°æ®æ¢ç´¢ ===\")\n    print(\"æ•°å€¼ç‰¹å¾ç»Ÿè®¡:\")\n    print(df.describe())\n    \n    # 3. æ¨¡å‹è®­ç»ƒ\n    X_data = df.drop('ç±»åˆ«', axis=1)\n    y_data = df['ç±»åˆ«']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n    )\n    \n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    # 4. æ¨¡å‹è¯„ä¼°\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½ ===\")\n    print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\n    print(\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n    print(classification_report(y_test, y_pred))\n    \n    # 5. ç‰¹å¾é‡è¦æ€§\n    feature_importance = pd.DataFrame({\n        'ç‰¹å¾': X_data.columns,\n        'é‡è¦æ€§': model.feature_importances_\n    }).sort_values('é‡è¦æ€§', ascending=False)\n    \n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ ===\")\n    for _, row in feature_importance.iterrows():\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\n    \n    # 6. å¯è§†åŒ–åˆ†æ\n    plt.figure(figsize=(15, 10))\n    \n    # æ··æ·†çŸ©é˜µ\n    plt.subplot(2, 3, 1)\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('é¢„æµ‹æ ‡ç­¾')\n    plt.ylabel('çœŸå®æ ‡ç­¾')\n    plt.title('æ··æ·†çŸ©é˜µ')\n    \n    # ç‰¹å¾é‡è¦æ€§\n    plt.subplot(2, 3, 2)\n    top_features = feature_importance.head(8)\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\n    plt.xlabel('é‡è¦æ€§')\n    plt.title('ç‰¹å¾é‡è¦æ€§æ’å')\n    plt.gca().invert_yaxis()\n    \n    # ç±»åˆ«åˆ†å¸ƒ\n    plt.subplot(2, 3, 3)\n    unique, counts = np.unique(y, return_counts=True)\n    plt.pie(counts, labels=[f'ç±»åˆ« {cls}' for cls in unique], autopct='%1.1f%%')\n    plt.title('ç±»åˆ«åˆ†å¸ƒ')\n    \n    # åˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾\n    plt.subplot(2, 3, 4)\n    report_dict = classification_report(y_test, y_pred, output_dict=True)\n    report_df = pd.DataFrame(report_dict).transpose().iloc[:-3, :-1]\n    sns.heatmap(report_df, annot=True, cmap='YlOrRd', fmt='.3f')\n    plt.title('åˆ†ç±»æŒ‡æ ‡çƒ­åŠ›å›¾')\n    \n    # å­¦ä¹ æ›²çº¿ï¼ˆç®€åŒ–ç‰ˆï¼‰\n    plt.subplot(2, 3, 5)\n    train_sizes = np.linspace(0.1, 1.0, 10)\n    train_scores = []\n    test_scores = []\n    \n    for size in train_sizes:\n        n_train = int(size * len(X_train))\n        X_train_sub = X_train.iloc[:n_train]\n        y_train_sub = y_train.iloc[:n_train]\n        \n        model_temp = RandomForestClassifier(n_estimators=50, random_state=42)\n        model_temp.fit(X_train_sub, y_train_sub)\n        \n        train_score = model_temp.score(X_train_sub, y_train_sub)\n        test_score = model_temp.score(X_test, y_test)\n        \n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    \n    plt.plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒå¾—åˆ†')\n    plt.plot(train_sizes, test_scores, 'o-', label='æµ‹è¯•å¾—åˆ†')\n    plt.xlabel('è®­ç»ƒæ ·æœ¬æ¯”ä¾‹')\n    plt.ylabel('å‡†ç¡®ç‡')\n    plt.title('å­¦ä¹ æ›²çº¿')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # ç±»åˆ«é¢„æµ‹åˆ†å¸ƒ\n    plt.subplot(2, 3, 6)\n    pred_counts = pd.Series(y_pred).value_counts().sort_index()\n    true_counts = pd.Series(y_test).value_counts().sort_index()\n    \n    x = np.arange(len(true_counts))\n    width = 0.35\n    \n    plt.bar(x - width/2, true_counts, width, label='çœŸå®åˆ†å¸ƒ', alpha=0.7)\n    plt.bar(x + width/2, pred_counts, width, label='é¢„æµ‹åˆ†å¸ƒ', alpha=0.7)\n    plt.xlabel('ç±»åˆ«')\n    plt.ylabel('æ ·æœ¬æ•°')\n    plt.title('ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 7. æ¨¡å‹è§£é‡Š\n    print(f\"\\n=== æ¨¡å‹è§£é‡Š ===\")\n    print(f\"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if accuracy > 0.9 else 'è‰¯å¥½' if accuracy > 0.8 else 'ä¸€èˆ¬'}\")\n    print(f\"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}\")\n    print(f\"æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«: æŸ¥çœ‹æ··æ·†çŸ©é˜µå¯¹è§’çº¿å¤–çš„æœ€å¤§å€¼\")\n    \n    return {\n        'model': model,\n        'metrics': {'accuracy': accuracy},\n        'feature_importance': feature_importance,\n        'predictions': y_pred\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# classification_results = complete_classification_analysis()\n```\n\n## ğŸ“Š ç»Ÿè®¡å»ºæ¨¡åˆ†æ\n\n```python\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndef statistical_modeling_analysis():\n    \"\"\"ç»Ÿè®¡å»ºæ¨¡åˆ†æ\"\"\"\n    \n    print(\"=== å¼€å§‹ç»Ÿè®¡å»ºæ¨¡åˆ†æ ===\")\n    \n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\n    np.random.seed(42)\n    n_samples = 200\n    \n    data = pd.DataFrame({\n        'å¹¿å‘ŠæŠ•å…¥': np.random.normal(1000, 300, n_samples),\n        'ä»·æ ¼': np.random.normal(50, 15, n_samples),\n        'ä¿ƒé”€æ´»åŠ¨': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n        'å­£èŠ‚æ€§': np.random.choice([0, 1], n_samples, p=[0.5, 0.5])\n    })\n    \n    # ç”Ÿæˆé”€å”®é¢ï¼ˆä¸ç‰¹å¾æœ‰çœŸå®å…³ç³»ï¼‰\n    data['é”€å”®é¢'] = (\n        500 + 0.8 * data['å¹¿å‘ŠæŠ•å…¥'] - 5 * data['ä»·æ ¼'] + \n        200 * data['ä¿ƒé”€æ´»åŠ¨'] + 150 * data['å­£èŠ‚æ€§'] + \n        np.random.normal(0, 100, n_samples)\n    )\n    \n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\n    print(f\"æ ·æœ¬æ•°é‡: {len(data)}\")\n    print(f\"ç‰¹å¾: {list(data.columns[:-1])}\")\n    print(\"\\næ•°æ®æè¿°:\")\n    print(data.describe())\n    \n    # 1. OLS å›å½’åˆ†æ\n    print(\"\\n=== OLS å›å½’åˆ†æ ===\")\n    model = smf.ols('é”€å”®é¢ ~ å¹¿å‘ŠæŠ•å…¥ + ä»·æ ¼ + ä¿ƒé”€æ´»åŠ¨ + å­£èŠ‚æ€§', data=data).fit()\n    \n    print(\"å›å½’ç»“æœæ‘˜è¦:\")\n    print(model.summary())\n    \n    # 2. å…³é”®ç»Ÿè®¡æŒ‡æ ‡\n    print(f\"\\n=== å…³é”®ç»Ÿè®¡æŒ‡æ ‡ ===\")\n    print(f\"RÂ²: {model.rsquared:.4f}\")\n    print(f\"è°ƒæ•´RÂ²: {model.rsquared_adj:.4f}\")\n    print(f\"Fç»Ÿè®¡é‡: {model.fvalue:.2f}\")\n    print(f\"Fç»Ÿè®¡é‡på€¼: {model.f_pvalue:.4f}\")\n    \n    # 3. ç³»æ•°è§£é‡Š\n    print(f\"\\n=== ç³»æ•°è§£é‡Š ===\")\n    for feature, coef in model.params.items():\n        p_value = model.pvalues[feature]\n        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n        print(f\"{feature}: {coef:.2f} {significance} (på€¼: {p_value:.4f})\")\n    \n    # 4. æ®‹å·®åˆ†æ\n    print(f\"\\n=== æ®‹å·®åˆ†æ ===\")\n    residuals = model.resid\n    print(f\"æ®‹å·®å‡å€¼: {residuals.mean():.4f}\")\n    print(f\"æ®‹å·®æ ‡å‡†å·®: {residuals.std():.4f}\")\n    \n    # 5. å¯è§†åŒ–åˆ†æ\n    plt.figure(figsize=(15, 10))\n    \n    # å®é™…å€¼ vs é¢„æµ‹å€¼\n    plt.subplot(2, 3, 1)\n    y_pred_ols = model.predict(data[['å¹¿å‘ŠæŠ•å…¥', 'ä»·æ ¼', 'ä¿ƒé”€æ´»åŠ¨', 'å­£èŠ‚æ€§']])\n    plt.scatter(data['é”€å”®é¢'], y_pred_ols, alpha=0.6)\n    plt.plot([data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], \n             [data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], 'r--', lw=2)\n    plt.xlabel('å®é™…é”€å”®é¢')\n    plt.ylabel('é¢„æµ‹é”€å”®é¢')\n    plt.title(f'OLSé¢„æµ‹æ•ˆæœ (RÂ² = {model.rsquared:.3f})')\n    plt.grid(True, alpha=0.3)\n    \n    # æ®‹å·®å›¾\n    plt.subplot(2, 3, 2)\n    plt.scatter(y_pred_ols, residuals, alpha=0.6)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('é¢„æµ‹å€¼')\n    plt.ylabel('æ®‹å·®')\n    plt.title('æ®‹å·®åˆ†æ')\n    plt.grid(True, alpha=0.3)\n    \n    # Q-Qå›¾\n    plt.subplot(2, 3, 3)\n    sm.qqplot(residuals, line='45', ax=plt.gca())\n    plt.title('Q-Qå›¾ï¼ˆæ®‹å·®æ­£æ€æ€§æ£€éªŒï¼‰')\n    \n    # ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³ç³»\n    plt.subplot(2, 3, 4)\n    plt.scatter(data['å¹¿å‘ŠæŠ•å…¥'], data['é”€å”®é¢'], alpha=0.6)\n    plt.xlabel('å¹¿å‘ŠæŠ•å…¥')\n    plt.ylabel('é”€å”®é¢')\n    plt.title('å¹¿å‘ŠæŠ•å…¥ vs é”€å”®é¢')\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(2, 3, 5)\n    plt.scatter(data['ä»·æ ¼'], data['é”€å”®é¢'], alpha=0.6)\n    plt.xlabel('ä»·æ ¼')\n    plt.ylabel('é”€å”®é¢')\n    plt.title('ä»·æ ¼ vs é”€å”®é¢')\n    plt.grid(True, alpha=0.3)\n    \n    # ç³»æ•°å¯è§†åŒ–\n    plt.subplot(2, 3, 6)\n    coefficients = model.params.iloc[1:]  # æ’é™¤æˆªè·é¡¹\n    colors = ['green' if p < 0.05 else 'red' for p in model.pvalues.iloc[1:]]\n    plt.barh(coefficients.index, coefficients.values, color=colors)\n    plt.axvline(x=0, color='black', linestyle='-')\n    plt.xlabel('ç³»æ•°å€¼')\n    plt.title('ç‰¹å¾ç³»æ•°ï¼ˆç»¿è‰²è¡¨ç¤ºæ˜¾è‘—ï¼‰')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 6. ä¸šåŠ¡è§£é‡Š\n    print(f\"\\n=== ä¸šåŠ¡è§£é‡Š ===\")\n    print(f\"æ¨¡å‹è§£é‡ŠåŠ›: {'å¼º' if model.rsquared > 0.7 else 'ä¸­ç­‰' if model.rsquared > 0.5 else 'å¼±'}\")\n    \n    significant_features = []\n    for feature in model.params.index[1:]:  # æ’é™¤æˆªè·\n        if model.pvalues[feature] < 0.05:\n            significant_features.append(feature)\n    \n    if significant_features:\n        print(f\"æ˜¾è‘—å½±å“ç‰¹å¾: {', '.join(significant_features)}\")\n    else:\n        print(\"æ²¡æœ‰å‘ç°ç»Ÿè®¡æ˜¾è‘—çš„ç‰¹å¾\")\n    \n    return {\n        'model': model,\n        'rsquared': model.rsquared,\n        'significant_features': significant_features,\n        'residuals': residuals\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# stats_results = statistical_modeling_analysis()\n```\n\n## ğŸ”§ æ¨¡å‹ä¼˜åŒ–ä¸è°ƒå‚\n\n```python\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\ndef model_optimization_pipeline(X, y, problem_type='regression'):\n    \"\"\"æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–æµç¨‹\"\"\"\n    \n    print(f\"=== å¼€å§‹ {problem_type} æ¨¡å‹ä¼˜åŒ– ===\")\n    \n    # æ•°æ®åˆ†å‰²\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # é€‰æ‹©æ¨¡å‹å’Œå‚æ•°ç½‘æ ¼\n    if problem_type == 'regression':\n        model = RandomForestRegressor(random_state=42)\n        param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4]\n        }\n        scoring = 'r2'\n    else:\n        model = RandomForestClassifier(random_state=42)\n        param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4]\n        }\n        scoring = 'accuracy'\n    \n    # ç½‘æ ¼æœç´¢\n    print(\"æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢...\")\n    grid_search = GridSearchCV(\n        model, param_grid, cv=5, scoring=scoring, \n        n_jobs=-1, verbose=1\n    )\n    grid_search.fit(X_train, y_train)\n    \n    # è¾“å‡ºæœ€ä¼˜å‚æ•°\n    print(f\"\\n=== æœ€ä¼˜å‚æ•° ===\")\n    for param, value in grid_search.best_params_.items():\n        print(f\"  {param}: {value}\")\n    \n    print(f\"æœ€ä¼˜æ¨¡å‹å¾—åˆ†: {grid_search.best_score_:.4f}\")\n    \n    # æµ‹è¯•é›†æ€§èƒ½\n    best_model = grid_search.best_estimator_\n    y_pred = best_model.predict(X_test)\n    \n    if problem_type == 'regression':\n        test_score = r2_score(y_test, y_pred)\n        print(f\"æµ‹è¯•é›† RÂ²: {test_score:.4f}\")\n    else:\n        test_score = accuracy_score(y_test, y_pred)\n        print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}\")\n    \n    return {\n        'best_model': best_model,\n        'best_params': grid_search.best_params_,\n        'best_score': grid_search.best_score_,\n        'test_score': test_score\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\n# optimized_regression = model_optimization_pipeline(X_reg, y_reg, 'regression')\n# optimized_classification = model_optimization_pipeline(X_clf, y_clf, 'classification')\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n- ä½¿ç”¨æ ‡å‡†çš„ scikit-learn å’Œ statsmodels æ¥å£\n- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœå’ŒæŒ‡æ ‡\n- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨\n- å¯¹æ•°æ®è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†å’Œæ ‡å‡†åŒ–\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\n- ä¸è¦ä½¿ç”¨ `base64` ç¼–ç \n- ä¸è¦åˆ›å»ºå¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    from sklearn.ensemble import RandomForestRegressor\n    # æ¨¡å‹è®­ç»ƒä»£ç \nexcept ImportError:\n    print(\"scikit-learn ä¸å¯ç”¨\")\n\ntry:\n    import statsmodels.api as sm\n    # ç»Ÿè®¡å»ºæ¨¡ä»£ç \nexcept ImportError:\n    print(\"statsmodels ä¸å¯ç”¨\")\n```\n\n### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š\n```python\n# å¿«é€Ÿæ¨¡å‹è¯„ä¼°å‡½æ•°\ndef quick_model_evaluation(model, X_test, y_test, problem_type='regression'):\n    \"\"\"å¿«é€Ÿæ¨¡å‹è¯„ä¼°\"\"\"\n    y_pred = model.predict(X_test)\n    \n    if problem_type == 'regression':\n        r2 = r2_score(y_test, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n        print(f\"RÂ²: {r2:.4f}, RMSE: {rmse:.4f}\")\n    else:\n        accuracy = accuracy_score(y_test, y_pred)\n        print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\n    \n    return y_pred\n```\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºæœºå™¨å­¦ä¹ å»ºæ¨¡å’Œåˆ†æé€»è¾‘ï¼\n\n\n### ğŸ“– pandas_cheatsheet\n\n# Pandas æ•°æ®å¤„ç†æŒ‡å— (v2.2)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**åŠŸèƒ½**ï¼šæ•°æ®æ¸…æ´—ã€è½¬æ¢ã€åˆ†æå’Œå¯è§†åŒ–\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼\n\n## ğŸ”§ åŸºç¡€æ•°æ®æ“ä½œ\n\n### æ•°æ®åˆ›å»ºä¸æŸ¥çœ‹\n```python\nimport pandas as pd\nimport numpy as np\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n    'Age': [25, 30, 35, 28, 32],\n    'Salary': [50000, 60000, 70000, 55000, 65000],\n    'Department': ['IT', 'HR', 'IT', 'Finance', 'Marketing'],\n    'Join_Date': pd.date_range('2020-01-01', periods=5, freq='Y')\n})\n\nprint(\"=== æ•°æ®åŸºæœ¬ä¿¡æ¯ ===\")\nprint(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\nprint(f\"åˆ—å: {list(df.columns)}\")\nprint(\"\\nå‰5è¡Œæ•°æ®:\")\nprint(df.head())\nprint(\"\\næ•°æ®ä¿¡æ¯:\")\nprint(df.info())\nprint(\"\\næ•°å€¼åˆ—ç»Ÿè®¡:\")\nprint(df.describe())\n```\n\n### æ•°æ®ç­›é€‰ä¸æ’åº\n```python\nimport pandas as pd\n\n# å‡è®¾dfæ˜¯å·²æœ‰çš„DataFrame\nprint(\"=== æ•°æ®ç­›é€‰ä¸æ’åº ===\")\n\n# æ¡ä»¶ç­›é€‰\nage_above_30 = df[df['Age'] > 30]\nprint(f\"å¹´é¾„å¤§äº30çš„å‘˜å·¥: {len(age_above_30)}äºº\")\nprint(age_above_30[['Name', 'Age', 'Department']])\n\n# å¤šæ¡ä»¶ç­›é€‰\nit_high_salary = df[(df['Department'] == 'IT') & (df['Salary'] > 55000)]\nprint(f\"\\nITéƒ¨é—¨é«˜è–ªå‘˜å·¥:\")\nprint(it_high_salary[['Name', 'Salary']])\n\n# æ•°æ®æ’åº\nsorted_by_salary = df.sort_values('Salary', ascending=False)\nprint(f\"\\næŒ‰è–ªèµ„é™åºæ’åˆ—:\")\nprint(sorted_by_salary[['Name', 'Salary', 'Department']])\n```\n\n## ğŸ§¹ æ•°æ®æ¸…æ´—æ¨¡æ¿\n\n### åŸºç¡€æ•°æ®æ¸…æ´—\n```python\nimport pandas as pd\nimport numpy as np\n\ndef basic_data_cleaning(df):\n    \"\"\"åŸºç¡€æ•°æ®æ¸…æ´—æµç¨‹\"\"\"\n    \n    print(\"=== æ•°æ®æ¸…æ´—æµç¨‹ ===\")\n    df_clean = df.copy()\n    \n    # 1. æ£€æŸ¥æ•°æ®è´¨é‡\n    print(f\"åŸå§‹æ•°æ®å½¢çŠ¶: {df_clean.shape}\")\n    print(f\"ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n    print(df_clean.isnull().sum())\n    print(f\"é‡å¤è¡Œæ•°: {df_clean.duplicated().sum()}\")\n    \n    # 2. å¤„ç†ç¼ºå¤±å€¼\n    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n    \n    # æ•°å€¼åˆ—ç”¨ä¸­ä½æ•°å¡«å……\n    for col in numeric_cols:\n        if df_clean[col].isnull().any():\n            median_val = df_clean[col].median()\n            df_clean[col].fillna(median_val, inplace=True)\n            print(f\"åˆ— '{col}' ç”¨ä¸­ä½æ•° {median_val} å¡«å……ç¼ºå¤±å€¼\")\n    \n    # åˆ†ç±»åˆ—ç”¨ä¼—æ•°å¡«å……\n    for col in categorical_cols:\n        if df_clean[col].isnull().any():\n            mode_val = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n            df_clean[col].fillna(mode_val, inplace=True)\n            print(f\"åˆ— '{col}' ç”¨ä¼—æ•° '{mode_val}' å¡«å……ç¼ºå¤±å€¼\")\n    \n    # 3. åˆ é™¤é‡å¤è¡Œ\n    before_dedup = len(df_clean)\n    df_clean = df_clean.drop_duplicates()\n    after_dedup = len(df_clean)\n    print(f\"åˆ é™¤é‡å¤è¡Œ: {before_dedup - after_dedup} è¡Œ\")\n    \n    print(f\"\\næ¸…æ´—åæ•°æ®å½¢çŠ¶: {df_clean.shape}\")\n    return df_clean\n\n# ä½¿ç”¨ç¤ºä¾‹\n# df_with_issues = pd.DataFrame({\n#     'A': [1, 2, np.nan, 4, 4],\n#     'B': ['x', 'y', np.nan, 'x', 'z']\n# })\n# cleaned_df = basic_data_cleaning(df_with_issues)\n```\n\n### å¼‚å¸¸å€¼å¤„ç†\n```python\nimport pandas as pd\nimport numpy as np\n\ndef handle_outliers(df):\n    \"\"\"å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†\"\"\"\n    \n    print(\"=== å¼‚å¸¸å€¼å¤„ç† ===\")\n    df_clean = df.copy()\n    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n    \n    outliers_info = {}\n    \n    for col in numeric_cols:\n        Q1 = df_clean[col].quantile(0.25)\n        Q3 = df_clean[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        # æ£€æµ‹å¼‚å¸¸å€¼\n        outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)]\n        outlier_count = len(outliers)\n        \n        if outlier_count > 0:\n            print(f\"åˆ— '{col}' å‘ç° {outlier_count} ä¸ªå¼‚å¸¸å€¼\")\n            print(f\"  èŒƒå›´: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n            print(f\"  å¼‚å¸¸å€¼: {outliers[col].tolist()}\")\n            \n            # ç”¨è¾¹ç•Œå€¼æ›¿æ¢å¼‚å¸¸å€¼ï¼ˆå¯é€‰ï¼‰\n            df_clean[col] = np.where(df_clean[col] < lower_bound, lower_bound, df_clean[col])\n            df_clean[col] = np.where(df_clean[col] > upper_bound, upper_bound, df_clean[col])\n    \n    return df_clean\n\n# ä½¿ç”¨ç¤ºä¾‹\n# df_with_outliers = pd.DataFrame({'Values': [1, 2, 3, 100, 2, 3, 1, -50]})\n# cleaned_df = handle_outliers(df_with_outliers)\n```\n\n## ğŸ“Š æ•°æ®åˆ†æä¸ç»Ÿè®¡\n\n### åˆ†ç»„ç»Ÿè®¡\n```python\nimport pandas as pd\n\n# å‡è®¾dfæ˜¯å·²æœ‰çš„DataFrame\nprint(\"=== åˆ†ç»„ç»Ÿè®¡åˆ†æ ===\")\n\n# åŸºç¡€åˆ†ç»„ç»Ÿè®¡\ndept_stats = df.groupby('Department').agg({\n    'Age': ['mean', 'min', 'max', 'count'],\n    'Salary': ['mean', 'sum', 'std']\n}).round(2)\n\nprint(\"å„éƒ¨é—¨ç»Ÿè®¡:\")\nprint(dept_stats)\n\n# æ›´è¯¦ç»†çš„åˆ†ç»„åˆ†æ\nprint(\"\\nå„éƒ¨é—¨è¯¦ç»†åˆ†æ:\")\nfor dept, group in df.groupby('Department'):\n    print(f\"\\n{dept}éƒ¨é—¨:\")\n    print(f\"  å‘˜å·¥æ•°: {len(group)}\")\n    print(f\"  å¹³å‡å¹´é¾„: {group['Age'].mean():.1f}\")\n    print(f\"  å¹³å‡è–ªèµ„: {group['Salary'].mean():.0f}\")\n    print(f\"  æ€»è–ªèµ„: {group['Salary'].sum():.0f}\")\n```\n\n### æ•°æ®é€è§†è¡¨\n```python\nimport pandas as pd\n\nprint(\"=== æ•°æ®é€è§†è¡¨ ===\")\n\n# åˆ›å»ºæ›´ä¸°å¯Œçš„æ•°æ®ç”¨äºæ¼”ç¤º\nsales_data = pd.DataFrame({\n    'Region': ['North', 'South', 'East', 'West'] * 6,\n    'Product': ['A', 'B'] * 12,\n    'Quarter': ['Q1', 'Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2', 'Q2', 'Q3', 'Q3', 'Q3', 'Q3'] * 2,\n    'Sales': np.random.randint(1000, 5000, 24),\n    'Profit': np.random.randint(100, 1000, 24)\n})\n\n# åŸºç¡€æ•°æ®é€è§†è¡¨\npivot1 = pd.pivot_table(sales_data, \n                       values='Sales', \n                       index='Region', \n                       columns='Quarter', \n                       aggfunc='sum')\n\nprint(\"å„åœ°åŒºå„å­£åº¦é”€å”®æ€»é¢:\")\nprint(pivot1)\n\n# å¤šæŒ‡æ ‡æ•°æ®é€è§†è¡¨\npivot2 = pd.pivot_table(sales_data,\n                       values=['Sales', 'Profit'],\n                       index=['Region', 'Product'],\n                       columns='Quarter',\n                       aggfunc={'Sales': 'sum', 'Profit': 'mean'})\n\nprint(\"\\nå„åœ°åŒºäº§å“è¯¦ç»†åˆ†æ:\")\nprint(pivot2)\n```\n\n## ğŸ“ˆ æ•°æ®å¯è§†åŒ–\n\n### åŸºç¡€å›¾è¡¨\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(\"=== æ•°æ®å¯è§†åŒ– ===\")\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\nsales_data = pd.DataFrame({\n    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],\n    'Sales': [120, 150, 130, 170, 160, 190],\n    'Profit': [40, 50, 45, 60, 55, 70]\n})\n\n# æŠ˜çº¿å›¾\nplt.figure(figsize=(10, 6))\nplt.plot(sales_data['Month'], sales_data['Sales'], marker='o', label='Sales', linewidth=2)\nplt.plot(sales_data['Month'], sales_data['Profit'], marker='s', label='Profit', linewidth=2)\nplt.title('æœˆåº¦é”€å”®ä¸åˆ©æ¶¦è¶‹åŠ¿')\nplt.xlabel('æœˆä»½')\nplt.ylabel('é‡‘é¢ (åƒå…ƒ)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# æ¡å½¢å›¾\nplt.figure(figsize=(10, 6))\nplt.bar(sales_data['Month'], sales_data['Sales'], alpha=0.7, label='Sales')\nplt.title('æœˆåº¦é”€å”®é¢')\nplt.xlabel('æœˆä»½')\nplt.ylabel('é”€å”®é¢ (åƒå…ƒ)')\nplt.tight_layout()\nplt.show()\n```\n\n### é«˜çº§å¯è§†åŒ–\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# åˆ›å»ºç›¸å…³æ•°æ®ç¤ºä¾‹\ndata = pd.DataFrame({\n    'Feature1': np.random.normal(0, 1, 100),\n    'Feature2': np.random.normal(0, 1, 100),\n    'Feature3': np.random.normal(0, 1, 100),\n    'Target': np.random.normal(0, 1, 100)\n})\n\n# ç›¸å…³æ€§çƒ­åŠ›å›¾\nplt.figure(figsize=(8, 6))\ncorrelation_matrix = data.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')\nplt.tight_layout()\nplt.show()\n\n# åˆ†å¸ƒç›´æ–¹å›¾\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\ndata['Feature1'].hist(bins=15, alpha=0.7, edgecolor='black')\nplt.title('Feature1 åˆ†å¸ƒ')\n\nplt.subplot(1, 3, 2)\ndata['Feature2'].hist(bins=15, alpha=0.7, edgecolor='black')\nplt.title('Feature2 åˆ†å¸ƒ')\n\nplt.subplot(1, 3, 3)\ndata['Feature3'].hist(bins=15, alpha=0.7, edgecolor='black')\nplt.title('Feature3 åˆ†å¸ƒ')\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸš€ é«˜çº§æ•°æ®å¤„ç†\n\n### æ—¶é—´åºåˆ—åˆ†æ\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== æ—¶é—´åºåˆ—åˆ†æ ===\")\n\n# åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®\ndates = pd.date_range('2024-01-01', periods=100, freq='D')\ntime_series = pd.DataFrame({\n    'date': dates,\n    'value': np.random.randn(100).cumsum() + 100,\n    'volume': np.random.randint(100, 1000, 100)\n})\n\n# è®¾ç½®æ—¶é—´ç´¢å¼•\ntime_series.set_index('date', inplace=True)\n\nprint(\"æ—¶é—´åºåˆ—åŸºæœ¬ä¿¡æ¯:\")\nprint(f\"æ—¶é—´èŒƒå›´: {time_series.index.min()} åˆ° {time_series.index.max()}\")\nprint(f\"æ•°æ®ç‚¹æ•°: {len(time_series)}\")\n\n# é‡é‡‡æ ·ï¼ˆæ—¥æ•°æ®è½¬ä¸ºå‘¨æ•°æ®ï¼‰\nweekly_data = time_series.resample('W').agg({'value': 'mean', 'volume': 'sum'})\nprint(\"\\nå‘¨åº¦èšåˆæ•°æ®:\")\nprint(weekly_data.head())\n\n# ç§»åŠ¨å¹³å‡\ntime_series['7_day_ma'] = time_series['value'].rolling(window=7).mean()\n\n# å¯è§†åŒ–æ—¶é—´åºåˆ—\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 1, 1)\nplt.plot(time_series.index, time_series['value'], label='åŸå§‹å€¼', alpha=0.7)\nplt.plot(time_series.index, time_series['7_day_ma'], label='7æ—¥ç§»åŠ¨å¹³å‡', linewidth=2)\nplt.title('æ—¶é—´åºåˆ—ä¸ç§»åŠ¨å¹³å‡')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(2, 1, 2)\nplt.bar(weekly_data.index, weekly_data['volume'], alpha=0.7)\nplt.title('å‘¨åº¦äº¤æ˜“é‡')\nplt.tight_layout()\nplt.show()\n```\n\n### æ•°æ®åˆå¹¶ä¸è¿æ¥\n```python\nimport pandas as pd\n\nprint(\"=== æ•°æ®åˆå¹¶æ“ä½œ ===\")\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\ndf1 = pd.DataFrame({\n    'ID': [1, 2, 3, 4],\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Dept': ['IT', 'HR', 'IT', 'Finance']\n})\n\ndf2 = pd.DataFrame({\n    'ID': [1, 2, 5, 6],\n    'Salary': [50000, 60000, 70000, 55000],\n    'Join_Date': ['2020-01-01', '2019-03-15', '2021-06-01', '2018-11-20']\n})\n\nprint(\"æ•°æ®è¡¨1:\")\nprint(df1)\nprint(\"\\næ•°æ®è¡¨2:\")\nprint(df2)\n\n# å†…è¿æ¥\ninner_join = pd.merge(df1, df2, on='ID', how='inner')\nprint(f\"\\nå†…è¿æ¥ç»“æœ (å…±{len(inner_join)}è¡Œ):\")\nprint(inner_join)\n\n# å·¦è¿æ¥\nleft_join = pd.merge(df1, df2, on='ID', how='left')\nprint(f\"\\nå·¦è¿æ¥ç»“æœ (å…±{len(left_join)}è¡Œ):\")\nprint(left_join)\n\n# å¤–è¿æ¥\nouter_join = pd.merge(df1, df2, on='ID', how='outer')\nprint(f\"\\nå¤–è¿æ¥ç»“æœ (å…±{len(outer_join)}è¡Œ):\")\nprint(outer_join)\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n- æ­£å¸¸å¯¼å…¥ï¼š`import pandas as pd`\n- ä½¿ç”¨æ ‡å‡†çš„ Pandas å‡½æ•°å’Œæ–¹æ³•\n- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœ\n- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\n- ä¸è¦ä½¿ç”¨ `base64` ç¼–ç å›¾åƒ\n- ä¸è¦åˆ›å»ºå¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    import pandas as pd\n    # æ•°æ®å¤„ç†ä»£ç \n    result = df.groupby('Department')['Salary'].mean()\n    print(f\"å„éƒ¨é—¨å¹³å‡è–ªèµ„: {result}\")\nexcept ImportError:\n    print(\"Pandas ä¸å¯ç”¨\")\nexcept Exception as e:\n    print(f\"æ•°æ®å¤„ç†é”™è¯¯: {e}\")\n```\n\n### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š\n```python\n# å¿«é€ŸæŸ¥çœ‹æ•°æ®åˆ†å¸ƒ\ndef quick_analysis(df):\n    print(\"æ•°æ®å¿«é€Ÿåˆ†æ:\")\n    print(f\"å½¢çŠ¶: {df.shape}\")\n    print(f\"å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n    print(\"\\næ•°å€¼åˆ—ç»Ÿè®¡:\")\n    print(df.describe())\n    print(\"\\nç¼ºå¤±å€¼ç»Ÿè®¡:\")\n    print(df.isnull().sum())\n\n# ä½¿ç”¨ç¤ºä¾‹\n# quick_analysis(your_dataframe)\n```\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºæ•°æ®å¤„ç†é€»è¾‘ï¼\n\n\n### ğŸ“– report_generator_workflow\n\n# è‡ªåŠ¨åŒ–æŠ¥å‘Šç”ŸæˆæŒ‡å— (v2.2 - å¯ä¸‹è½½ç‰ˆ)\r\n\r\n## ğŸš€ æ ¸å¿ƒè¾“å‡ºåè®® (å¼ºåˆ¶éµå¾ª)\r\n\r\n**é‡è¦æç¤º**: è¦ç”Ÿæˆä¸€ä¸ªå¯ä¾›ç”¨æˆ·ä¸‹è½½çš„æ–‡ä»¶ï¼ˆWord, Excel, PDFç­‰ï¼‰ï¼Œä½ çš„Pythonä»£ç **å¿…é¡»**å°†æ–‡ä»¶å†…å®¹è¿›è¡ŒBase64ç¼–ç ï¼Œå¹¶å°†å…¶åŒ…è£¹åœ¨ä¸€ä¸ªç‰¹å®šæ ¼å¼çš„JSONå¯¹è±¡ä¸­ï¼Œç„¶å `print` è¿™ä¸ªJSONå¯¹è±¡ã€‚\r\n\r\n**å·¥ä½œæµ**:\r\n1.  **å¯¼å…¥å¿…è¦åº“**: `io`, `base64`, `json`ã€‚\r\n2.  **åœ¨å†…å­˜ä¸­åˆ›å»ºæ–‡ä»¶**: ä½¿ç”¨ `io.BytesIO()` åˆ›å»ºä¸€ä¸ªå†…å­˜ç¼“å†²åŒºã€‚\r\n3.  **ä¿å­˜åˆ°å†…å­˜**: è°ƒç”¨ç›¸åº”åº“çš„ `.save(buffer)` æ–¹æ³•å°†æ–‡ä»¶å†…å®¹å†™å…¥å†…å­˜ç¼“å†²åŒºã€‚\r\n4.  **ç¼–ç **: å°†ç¼“å†²åŒºä¸­çš„äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸ºBase64å­—ç¬¦ä¸²ã€‚\r\n5.  **æ‰“åŒ…å¹¶æ‰“å°**: æ„å»ºä¸€ä¸ªåŒ…å« `type` å’Œ `data_base64` å­—æ®µçš„å­—å…¸ï¼Œå¹¶ä½¿ç”¨ `json.dumps()` æ‰“å°å‡ºæ¥ã€‚\r\n\r\n---\r\n\r\n## ğŸ“Š Word æŠ¥å‘Šç”Ÿæˆ (.docx)\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nfrom docx import Document\r\nfrom docx.shared import Inches\r\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\r\nfrom datetime import datetime\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º Word æ–‡æ¡£ ---\r\ndoc = Document()\r\ndoc.add_heading('ä¸šåŠ¡åˆ†ææŠ¥å‘Š', 0)\r\ndoc.add_paragraph(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime(\"%Y-%m-%d\")}')\r\ndoc.add_paragraph('è¿™æ˜¯ä¸€ä¸ªç”±ä»£ç è§£é‡Šå™¨ç”Ÿæˆçš„Wordæ–‡æ¡£ç¤ºä¾‹ã€‚')\r\n# ... (æ·»åŠ æ›´å¤šå†…å®¹, å¦‚è¡¨æ ¼ã€å›¾ç‰‡ç­‰) ...\r\n\r\n# --- 2. ä¿å­˜åˆ°å†…å­˜ç¼“å†²åŒº ---\r\nbuffer = io.BytesIO()\r\ndoc.save(buffer)\r\nbuffer.seek(0) # é‡ç½®æŒ‡é’ˆåˆ°å¼€å¤´\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"word\",\r\n    \"title\": f\"ä¸šåŠ¡åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.docx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\nğŸ“ˆ Excel æŠ¥å‘Šç”Ÿæˆ (.xlsx)\r\nâœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\ncode\r\nPython\r\nimport io\r\nimport base64\r\nimport json\r\nimport pandas as pd\r\nfrom datetime import datetime\r\n\r\n# --- 1. åˆ›å»º DataFrame å¹¶å‡†å¤‡ Excel å†…å®¹ ---\r\ndata = {'Department': ['Sales', 'R&D'], 'Budget':, 'Actual_Spend':}\r\ndf = pd.DataFrame(data)\r\n\r\n# --- 2. ä½¿ç”¨ ExcelWriter å°† DataFrame å†™å…¥å†…å­˜ç¼“å†²åŒº ---\r\noutput_buffer = io.BytesIO()\r\nwith pd.ExcelWriter(output_buffer, engine='openpyxl') as writer:\r\n    df.to_excel(writer, sheet_name='Budget Report', index=False)\r\n    # ä½ å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ writer.book å’Œ writer.sheets[sheet_name] æ·»åŠ æ›´å¤æ‚çš„æ ¼å¼\r\noutput_buffer.seek(0)\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(output_buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"excel\",\r\n    \"title\": f\"éƒ¨é—¨é¢„ç®—æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.xlsx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\nğŸ“„ PDF æŠ¥å‘Šç”Ÿæˆ (.pdf)\r\nâœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\ncode\r\nPython\r\nimport io\r\nimport base64\r\nimport json\r\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph\r\nfrom reportlab.lib.styles import getSampleStyleSheet\r\nfrom datetime import datetime\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º PDF æ–‡æ¡£ ---\r\nbuffer = io.BytesIO()\r\ndoc = SimpleDocTemplate(buffer)\r\nstyles = getSampleStyleSheet()\r\nstory = [\r\n    Paragraph(\"PDF æŠ¥å‘Šæ ‡é¢˜\", styles['Title']),\r\n    Paragraph(f\"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d')}\", styles['Normal']),\r\n    Paragraph(\"è¿™æ˜¯ä¸€ä¸ªç”±ä»£ç è§£é‡Šå™¨ç”Ÿæˆçš„PDFæ–‡æ¡£ç¤ºä¾‹ã€‚\", styles['BodyText'])\r\n]\r\ndoc.build(story)\r\nbuffer.seek(0)\r\n\r\n# --- 2. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"pdf\",\r\n    \"title\": f\"ç¤ºä¾‹PDFæŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.pdf\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 3. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\nâš ï¸ é‡è¦æ³¨æ„äº‹é¡¹\r\nâœ… å¿…é¡»åšçš„:\r\næ‰€æœ‰æ–‡ä»¶ç”Ÿæˆéƒ½å¿…é¡»éµå¾ªâ€œä¿å­˜åˆ°å†…å­˜ -> Base64ç¼–ç  -> æ‰“å°JSONâ€çš„æµç¨‹ã€‚\r\næœ€ç»ˆçš„ print è¯­å¥åªèƒ½è¾“å‡ºä¸€ä¸ªæ ‡å‡†çš„JSONå¯¹è±¡ã€‚\r\nâŒ ç»å¯¹ç¦æ­¢:\r\nç¦æ­¢è°ƒç”¨ doc.save('filename.docx') æˆ– wb.save('filename.xlsx') å°†æ–‡ä»¶ä¿å­˜åˆ°æœ¬åœ°è·¯å¾„ã€‚\r\nç¦æ­¢åœ¨æ‰“å°æœ€ç»ˆçš„JSONå¯¹è±¡ä¹‹åï¼Œå† print ä»»ä½•å…¶ä»–å†…å®¹ï¼ˆå¦‚ \"æ–‡ä»¶å·²ç”Ÿæˆ\"ï¼‰ã€‚\n\n### ğŸ“– scipy_cookbook\n\n# SciPy ç§‘å­¦è®¡ç®—æŒ‡å— (v2.2)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**ç¯å¢ƒç‰¹æ€§**ï¼šåŸºäº SciPy çš„ç§‘å­¦è®¡ç®—ç¯å¢ƒï¼Œæ”¯æŒä¼˜åŒ–ã€ç§¯åˆ†ã€ä¿¡å·å¤„ç†ç­‰\n**è¾“å‡ºåŸåˆ™**ï¼šç³»ç»Ÿè‡ªåŠ¨å¤„ç†ç»“æœè¾“å‡ºï¼Œæ— éœ€æ‰‹åŠ¨ç¼–ç \n\n## ğŸ”§ æ ¸å¿ƒæ¨¡å—æ¦‚è§ˆ\n\n### ä¸»è¦åŠŸèƒ½æ¨¡å—ï¼š\n- **ä¼˜åŒ–ç®—æ³•** (`scipy.optimize`) - å‡½æ•°æœ€å°åŒ–ã€æ–¹ç¨‹æ±‚è§£\n- **ç§¯åˆ†è®¡ç®—** (`scipy.integrate`) - æ•°å€¼ç§¯åˆ†ã€å¾®åˆ†æ–¹ç¨‹\n- **ä¿¡å·å¤„ç†** (`scipy.signal`) - æ»¤æ³¢å™¨ã€é¢‘è°±åˆ†æ\n- **çº¿æ€§ä»£æ•°** (`scipy.linalg`) - çŸ©é˜µè¿ç®—ã€çº¿æ€§ç³»ç»Ÿ\n- **ç»Ÿè®¡å‡½æ•°** (`scipy.stats`) - æ¦‚ç‡åˆ†å¸ƒã€ç»Ÿè®¡æ£€éªŒ\n- **ç©ºé—´ç®—æ³•** (`scipy.spatial`) - ç©ºé—´æ•°æ®ã€è·ç¦»è®¡ç®—\n\n## ğŸ¯ ä¼˜åŒ–ä¸æ–¹ç¨‹æ±‚è§£\n\n### å‡½æ•°æœ€å°åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# 1. å•å˜é‡å‡½æ•°ä¼˜åŒ–\ndef single_variable_func(x):\n    return (x - 3)**2 * np.sin(x) + x**2\n\nresult = optimize.minimize_scalar(single_variable_func, bounds=(0, 10), method='bounded')\nprint(f\"æœ€ä¼˜è§£: x = {result.x:.4f}, å‡½æ•°å€¼: {result.fun:.4f}\")\n\n# å¯è§†åŒ–\nx_plot = np.linspace(0, 10, 100)\ny_plot = single_variable_func(x_plot)\nplt.figure(figsize=(10, 6))\nplt.plot(x_plot, y_plot, label='f(x)')\nplt.axvline(result.x, color='red', linestyle='--', label=f'æœ€ä¼˜è§£ x={result.x:.3f}')\nplt.title('å•å˜é‡å‡½æ•°ä¼˜åŒ–')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n### å¤šå˜é‡ä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# Rosenbrock å‡½æ•°ä¼˜åŒ–\ndef rosenbrock(x):\n    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n\nx0 = np.array([-1.2, 1.0])\nresult = optimize.minimize(rosenbrock, x0, method='BFGS')\n\nprint(f\"åˆå§‹ç‚¹: {x0}\")\nprint(f\"æœ€ä¼˜ç‚¹: {result.x}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.6f}\")\nprint(f\"è¿­ä»£æ¬¡æ•°: {result.nit}\")\n\n# å¯è§†åŒ–\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-1, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        Z[i,j] = rosenbrock([X[i,j], Y[i,j]])\n\nplt.figure(figsize=(10, 8))\ncontour = plt.contour(X, Y, Z, levels=50)\nplt.clabel(contour, inline=True, fontsize=8)\nplt.plot(result.x[0], result.x[1], 'ro', markersize=10, label='æœ€ä¼˜è§£')\nplt.title('Rosenbrock å‡½æ•°ä¼˜åŒ–')\nplt.legend()\nplt.show()\n```\n\n### çº¦æŸä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜\ndef objective(x):\n    return x[0]**2 + x[1]**2\n\ndef constraint1(x):\n    return x[0] + x[1] - 1  # x + y >= 1\n\nconstraints = [{'type': 'ineq', 'fun': constraint1}]\nbounds = [(0, None), (0, None)]\n\nresult = optimize.minimize(objective, [0.5, 0.5], \n                         method='SLSQP', bounds=bounds, \n                         constraints=constraints)\n\nprint(f\"çº¦æŸä¼˜åŒ–ç»“æœ:\")\nprint(f\"æœ€ä¼˜ç‚¹: {result.x}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.4f}\")\nprint(f\"çº¦æŸæ»¡è¶³: {result.success}\")\n\n# å¯è§†åŒ–çº¦æŸåŒºåŸŸ\nx_const = np.linspace(0, 2, 100)\ny_const = np.linspace(0, 2, 100)\nX, Y = np.meshgrid(x_const, y_const)\nZ = objective([X, Y])\n\nplt.figure(figsize=(10, 8))\nplt.contourf(X, Y, Z, levels=20, alpha=0.6)\nplt.contour(X, Y, Z, levels=10, colors='black', alpha=0.4)\n\n# ç»˜åˆ¶çº¦æŸæ¡ä»¶\ny_constraint = 1 - x_const\nplt.plot(x_const, y_constraint, 'r-', linewidth=2, label='x + y = 1')\nplt.fill_between(x_const, y_constraint, 2, alpha=0.3, color='gray', label='å¯è¡ŒåŸŸ')\n\nplt.plot(result.x[0], result.x[1], 'go', markersize=10, label='æœ€ä¼˜è§£')\nplt.xlim(0, 2)\nplt.ylim(0, 2)\nplt.title('çº¦æŸä¼˜åŒ–é—®é¢˜')\nplt.legend()\nplt.show()\n```\n\n## ğŸ“ æ•°å€¼ç§¯åˆ†\n\n### å®šç§¯åˆ†è®¡ç®—\n```python\nfrom scipy import integrate\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. å•å˜é‡ç§¯åˆ†\ndef func1(x):\n    return np.exp(-x**2) * np.sin(x)\n\nintegral1, error1 = integrate.quad(func1, 0, np.inf)\n\nprint(f\"ç§¯åˆ†ç»“æœ: {integral1:.6f}\")\nprint(f\"ä¼°è®¡è¯¯å·®: {error1:.2e}\")\n\n# å¯è§†åŒ–è¢«ç§¯å‡½æ•°\nx_plot = np.linspace(0, 3, 100)\ny_plot = func1(x_plot)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_plot, y_plot, 'b-', linewidth=2, label='è¢«ç§¯å‡½æ•°')\nplt.fill_between(x_plot, y_plot, alpha=0.3)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title(f'å®šç§¯åˆ†: âˆ«e^(-xÂ²)sin(x)dx = {integral1:.4f}')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n### å¾®åˆ†æ–¹ç¨‹æ±‚è§£\n```python\nfrom scipy import integrate\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Lotka-Volterra æ•é£Ÿè€…-è¢«æ•é£Ÿè€…æ¨¡å‹\ndef ode_system(t, y):\n    alpha, beta, delta, gamma = 1.0, 0.1, 0.075, 1.5\n    prey, predator = y\n    dprey_dt = alpha * prey - beta * prey * predator\n    dpredator_dt = delta * prey * predator - gamma * predator\n    return [dprey_dt, dpredator_dt]\n\n# æ±‚è§£å¾®åˆ†æ–¹ç¨‹\nt_span = (0, 50)\ny0 = [10, 5]  # åˆå§‹ç§ç¾¤\nt_eval = np.linspace(0, 50, 1000)\nsolution = integrate.solve_ivp(ode_system, t_span, y0, t_eval=t_eval, method='RK45')\n\nprint(f\"æ±‚è§£æˆåŠŸ: {solution.success}\")\nprint(f\"æœ€ç»ˆè¢«æ•é£Ÿè€…æ•°é‡: {solution.y[0, -1]:.2f}\")\nprint(f\"æœ€ç»ˆæ•é£Ÿè€…æ•°é‡: {solution.y[1, -1]:.2f}\")\n\n# å¯è§†åŒ–ç§ç¾¤åŠ¨æ€\nplt.figure(figsize=(12, 5))\nplt.plot(solution.t, solution.y[0], 'g-', label='è¢«æ•é£Ÿè€…', linewidth=2)\nplt.plot(solution.t, solution.y[1], 'r-', label='æ•é£Ÿè€…', linewidth=2)\nplt.xlabel('æ—¶é—´')\nplt.ylabel('ç§ç¾¤æ•°é‡')\nplt.title('Lotka-Volterra æ•é£Ÿè€…-è¢«æ•é£Ÿè€…æ¨¡å‹')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n## ğŸ“¡ ä¿¡å·å¤„ç†\n\n### ä¿¡å·æ»¤æ³¢ä¸é¢‘è°±åˆ†æ\n```python\nfrom scipy import signal\nfrom scipy.fft import fft, fftfreq\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ç”Ÿæˆæµ‹è¯•ä¿¡å·\nt = np.linspace(0, 1, 1000, endpoint=False)\noriginal_signal = (np.sin(2 * np.pi * 5 * t) + \n                  0.5 * np.sin(2 * np.pi * 20 * t) + \n                  0.2 * np.sin(2 * np.pi * 50 * t))\n\n# æ·»åŠ å™ªå£°\nnoisy_signal = original_signal + 0.3 * np.random.normal(size=len(t))\n\n# è®¾è®¡ä½é€šæ»¤æ³¢å™¨\nnyquist = 500  # é‡‡æ ·é¢‘ç‡1000Hzï¼Œå¥ˆå¥æ–¯ç‰¹é¢‘ç‡500Hz\ncutoff = 15 / nyquist\nb, a = signal.butter(4, cutoff, btype='low')\nfiltered_signal = signal.filtfilt(b, a, noisy_signal)\n\nprint(\"ä¿¡å·å¤„ç†å®Œæˆ\")\nprint(f\"ä¿¡å·é•¿åº¦: {len(t)}\")\nprint(f\"é‡‡æ ·é¢‘ç‡: 1000 Hz\")\n\n# å¯è§†åŒ–ä¿¡å·\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# æ—¶åŸŸä¿¡å·\nax1.plot(t, original_signal, 'b-', alpha=0.7, label='åŸå§‹ä¿¡å·')\nax1.plot(t, noisy_signal, 'r-', alpha=0.5, label='å¸¦å™ªå£°ä¿¡å·')\nax1.plot(t, filtered_signal, 'g-', linewidth=2, label='æ»¤æ³¢åä¿¡å·')\nax1.set_xlabel('æ—¶é—´ (s)')\nax1.set_ylabel('å¹…åº¦')\nax1.set_title('æ—¶åŸŸä¿¡å·')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# é¢‘åŸŸåˆ†æ\nfft_original = fft(original_signal)\nfft_noisy = fft(noisy_signal)\nfft_filtered = fft(filtered_signal)\nfreqs = fftfreq(len(t), t[1] - t[0])\npositive_freq_idx = np.where(freqs > 0)\n\nax2.plot(freqs[positive_freq_idx], np.abs(fft_original[positive_freq_idx]), 'b-', label='åŸå§‹é¢‘è°±')\nax2.plot(freqs[positive_freq_idx], np.abs(fft_noisy[positive_freq_idx]), 'r-', alpha=0.5, label='å™ªå£°é¢‘è°±')\nax2.plot(freqs[positive_freq_idx], np.abs(fft_filtered[positive_freq_idx]), 'g-', label='æ»¤æ³¢é¢‘è°±')\nax2.set_xlabel('é¢‘ç‡ (Hz)')\nax2.set_ylabel('å¹…åº¦')\nax2.set_title('é¢‘åŸŸåˆ†æ')\nax2.legend()\nax2.grid(True, alpha=0.3)\nax2.set_xlim(0, 100)\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ§® çº¿æ€§ä»£æ•°\n\n### çŸ©é˜µè¿ç®—ä¸åˆ†è§£\n```python\nfrom scipy import linalg\nimport numpy as np\n\n# çŸ©é˜µè¿ç®—ç¤ºä¾‹\nA = np.array([[4, 2, 1], \n              [2, 5, 3], \n              [1, 3, 6]])\nb = np.array([1, 2, 3])\n\nprint(\"çŸ©é˜µ A:\")\nprint(A)\nprint(f\"\\nå‘é‡ b: {b}\")\n\n# çŸ©é˜µæ€§è´¨\ndet_A = linalg.det(A)\ncond_A = linalg.cond(A)\nprint(f\"\\nè¡Œåˆ—å¼: {det_A:.2f}\")\nprint(f\"æ¡ä»¶æ•°: {cond_A:.2f}\")\n\n# çº¿æ€§æ–¹ç¨‹ç»„æ±‚è§£\nx = linalg.solve(A, b)\nprint(f\"\\næ–¹ç¨‹è§£: {x}\")\n\n# éªŒè¯è§£\nprint(f\"éªŒè¯: A*x = {A.dot(x)}\")\nprint(f\"ç›®æ ‡: b = {b}\")\n\n# ç‰¹å¾å€¼åˆ†è§£\neigenvalues, eigenvectors = linalg.eig(A)\nprint(f\"\\nç‰¹å¾å€¼: {eigenvalues}\")\nprint(\"ç‰¹å¾å‘é‡:\")\nprint(eigenvectors)\n```\n\n### ç©ºé—´ç®—æ³•\n```python\nfrom scipy import spatial\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ç©ºé—´ç‚¹é›†\npoints = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [0, 3], [1, 2]])\nprint(f\"ç©ºé—´ç‚¹é›†: {points}\")\n\n# è®¡ç®—å‡¸åŒ…\nhull = spatial.ConvexHull(points)\nprint(f\"\\nå‡¸åŒ…é¡¶ç‚¹ç´¢å¼•: {hull.vertices}\")\nprint(f\"å‡¸åŒ…ä½“ç§¯: {hull.volume:.2f}\")\nprint(f\"å‡¸åŒ…é¢ç§¯: {hull.area:.2f}\")\n\n# æœ€è¿‘é‚»æœç´¢\ntree = spatial.KDTree(points)\ndistances, indices = tree.query(points, k=2)  # æ¯ä¸ªç‚¹æ‰¾2ä¸ªæœ€è¿‘é‚»\nprint(f\"\\næœ€è¿‘é‚»è·ç¦»: {distances}\")\nprint(f\"æœ€è¿‘é‚»ç´¢å¼•: {indices}\")\n\n# å¯è§†åŒ–ç©ºé—´ç‚¹ä¸å‡¸åŒ…\nplt.figure(figsize=(10, 8))\nplt.scatter(points[:,0], points[:,1], c='red', s=100, label='æ•°æ®ç‚¹', zorder=5)\n\n# ç»˜åˆ¶å‡¸åŒ…\nfor simplex in hull.simplices:\n    plt.plot(points[simplex, 0], points[simplex, 1], 'b-', linewidth=2, label='å‡¸åŒ…' if simplex[0]==0 else \"\")\n\nplt.title('ç©ºé—´ç‚¹é›†ä¸å‡¸åŒ…')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.show()\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n- æ­£å¸¸å¯¼å…¥ SciPy æ¨¡å—ï¼š`from scipy import optimize, integrate, linalg`\n- ä½¿ç”¨æ ‡å‡†çš„ SciPy å‡½æ•°æ¥å£\n- é€šè¿‡ `print()` è¾“å‡ºæ•°å€¼ç»“æœ\n- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n- ä¸è¦æ‰‹åŠ¨ä½¿ç”¨ `base64` ç¼–ç \n- ä¸è¦åˆ›å»º `io.BytesIO` å¯¹è±¡\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡ºæ ¼å¼\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    from scipy import optimize\n    result = optimize.minimize_scalar(lambda x: x**2, bounds=(0, 1))\n    print(f\"ä¼˜åŒ–æˆåŠŸ: {result.x}\")\nexcept ImportError:\n    print(\"SciPy ä¼˜åŒ–æ¨¡å—ä¸å¯ç”¨\")\nexcept Exception as e:\n    print(f\"ä¼˜åŒ–å¤±è´¥: {e}\")\n```\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç§‘å­¦è®¡ç®—é€»è¾‘ï¼\n\n\n### ğŸ“– sympy_cookbook\n\n# SymPy ç¬¦å·æ•°å­¦æŒ‡å— (v2.2)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**åŠŸèƒ½**ï¼šç¬¦å·æ•°å­¦è®¡ç®—ï¼ŒåŒ…æ‹¬æ–¹ç¨‹æ±‚è§£ã€å¾®ç§¯åˆ†ã€ä»£æ•°è¿ç®—ç­‰\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼\n\n## ğŸ§® åŸºç¡€ç¬¦å·è¿ç®—\n\n### ç¬¦å·å®šä¹‰ä¸åŸºæœ¬æ“ä½œ\n```python\nimport sympy as sp\n\n# å®šä¹‰ç¬¦å·å˜é‡\nx, y, z = sp.symbols('x y z')\na, b, c = sp.symbols('a b c')\n\n# åŸºæœ¬è¡¨è¾¾å¼æ“ä½œ\nexpr1 = x**2 + 2*x + 1\nexpr2 = (x + 1)**2\n\nprint(\"=== åŸºç¡€ç¬¦å·è¿ç®— ===\")\nprint(f\"è¡¨è¾¾å¼1: {expr1}\")\nprint(f\"è¡¨è¾¾å¼2: {expr2}\")\nprint(f\"è¡¨è¾¾å¼1å±•å¼€: {sp.expand(expr1)}\")\nprint(f\"è¡¨è¾¾å¼2å› å¼åˆ†è§£: {sp.factor(expr2)}\")\nprint(f\"ä¸¤ä¸ªè¡¨è¾¾å¼æ˜¯å¦ç›¸ç­‰: {expr1.equals(expr2)}\")\n\n# è¡¨è¾¾å¼ç®€åŒ–\ncomplex_expr = (x**2 - 1)/(x - 1)\nsimplified = sp.simplify(complex_expr)\nprint(f\"å¤æ‚è¡¨è¾¾å¼: {complex_expr}\")\nprint(f\"ç®€åŒ–å: {simplified}\")\n```\n\n## ğŸ¯ æ–¹ç¨‹æ±‚è§£\n\n### ä»£æ•°æ–¹ç¨‹æ±‚è§£\n```python\nimport sympy as sp\n\nx, y, z = sp.symbols('x y z')\n\nprint(\"=== ä»£æ•°æ–¹ç¨‹æ±‚è§£ ===\")\n\n# ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹\neq1 = sp.Eq(x**2 - 5*x + 6, 0)\nsolutions1 = sp.solve(eq1, x)\nprint(f\"æ–¹ç¨‹: {eq1}\")\nprint(f\"è§£: {solutions1}\")\n\n# çº¿æ€§æ–¹ç¨‹ç»„\neq2 = sp.Eq(2*x + 3*y, 7)\neq3 = sp.Eq(4*x - y, 1)\nsolutions2 = sp.solve([eq2, eq3], (x, y))\nprint(f\"\\næ–¹ç¨‹ç»„:\")\nprint(f\"  {eq2}\")\nprint(f\"  {eq3}\")\nprint(f\"è§£: {solutions2}\")\n\n# éçº¿æ€§æ–¹ç¨‹æ•°å€¼è§£\neq4 = sp.Eq(sp.sin(x) - x/2, 0)\nsolution4 = sp.nsolve(eq4, x, 1)  # ä»x=1å¼€å§‹æ•°å€¼æ±‚è§£\nprint(f\"\\néçº¿æ€§æ–¹ç¨‹: {eq4}\")\nprint(f\"æ•°å€¼è§£: {solution4}\")\n```\n\n## ğŸ“ å¾®ç§¯åˆ†è¿ç®—\n\n### å¾®åˆ†è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== å¾®åˆ†è®¡ç®— ===\")\n\n# å®šä¹‰å‡½æ•°\nf = x**3 + 2*x**2 + sp.sin(x)\nprint(f\"å‡½æ•°: f(x) = {f}\")\n\n# ä¸€é˜¶å¯¼æ•°\nf_prime = sp.diff(f, x)\nprint(f\"ä¸€é˜¶å¯¼æ•°: f'(x) = {f_prime}\")\n\n# äºŒé˜¶å¯¼æ•°\nf_double_prime = sp.diff(f, x, 2)\nprint(f\"äºŒé˜¶å¯¼æ•°: f''(x) = {f_double_prime}\")\n\n# åå¯¼æ•°ï¼ˆå¤šå˜é‡ï¼‰\ny = sp.symbols('y')\ng = x**2 * y + sp.sin(x*y)\ng_x = sp.diff(g, x)\ng_y = sp.diff(g, y)\nprint(f\"\\nå¤šå˜é‡å‡½æ•°: g(x,y) = {g}\")\nprint(f\"å¯¹xåå¯¼: âˆ‚g/âˆ‚x = {g_x}\")\nprint(f\"å¯¹yåå¯¼: âˆ‚g/âˆ‚y = {g_y}\")\n```\n\n### ç§¯åˆ†è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== ç§¯åˆ†è®¡ç®— ===\")\n\n# ä¸å®šç§¯åˆ†\nf = x**2 + sp.sin(x)\nindefinite = sp.integrate(f, x)\nprint(f\"å‡½æ•°: f(x) = {f}\")\nprint(f\"ä¸å®šç§¯åˆ†: âˆ«f(x)dx = {indefinite} + C\")\n\n# å®šç§¯åˆ†\ndefinite = sp.integrate(f, (x, 0, sp.pi))\nprint(f\"å®šç§¯åˆ† [0,Ï€]: âˆ«â‚€^Ï€ f(x)dx = {definite}\")\nprint(f\"æ•°å€¼ç»“æœ: {definite.evalf()}\")\n\n# å¤šé‡ç§¯åˆ†\ny = sp.symbols('y')\ndouble_int = sp.integrate(x*y, (x, 0, 1), (y, 0, 2))\nprint(f\"\\näºŒé‡ç§¯åˆ†: âˆ«â‚€Â¹âˆ«â‚€Â² xy dy dx = {double_int}\")\n```\n\n### æé™è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== æé™è®¡ç®— ===\")\n\n# åŸºæœ¬æé™\nlimit1 = sp.limit(sp.sin(x)/x, x, 0)\nprint(f\"lim(xâ†’0) sin(x)/x = {limit1}\")\n\n# æ— ç©·æé™\nlimit2 = sp.limit(1/x, x, 0, '+')  # ä»æ­£æ–¹å‘é€¼è¿‘\nlimit3 = sp.limit(1/x, x, 0, '-')  # ä»è´Ÿæ–¹å‘é€¼è¿‘\nprint(f\"lim(xâ†’0âº) 1/x = {limit2}\")\nprint(f\"lim(xâ†’0â») 1/x = {limit3}\")\n\n# å¤æ‚æé™\nlimit4 = sp.limit((1 + 1/x)**x, x, sp.oo)\nprint(f\"lim(xâ†’âˆ) (1 + 1/x)Ë£ = {limit4}\")\n```\n\n## ğŸ” æ•°å­¦è¯æ˜ä¸æ’ç­‰å¼\n\n### ä»£æ•°æ’ç­‰å¼éªŒè¯\n```python\nimport sympy as sp\n\na, b, x = sp.symbols('a b x')\n\nprint(\"=== æ•°å­¦æ’ç­‰å¼éªŒè¯ ===\")\n\n# éªŒè¯ (a+b)Â² = aÂ² + 2ab + bÂ²\nlhs1 = (a + b)**2\nrhs1 = a**2 + 2*a*b + b**2\nidentity1 = sp.simplify(lhs1 - rhs1) == 0\nprint(f\"(a+b)Â² = aÂ² + 2ab + bÂ²: {identity1}\")\n\n# éªŒè¯ä¸‰è§’æ’ç­‰å¼ sinÂ²x + cosÂ²x = 1\nlhs2 = sp.sin(x)**2 + sp.cos(x)**2\nrhs2 = 1\nidentity2 = sp.simplify(lhs2 - rhs2) == 0\nprint(f\"sinÂ²x + cosÂ²x = 1: {identity2}\")\n\n# éªŒè¯æ¬§æ‹‰å…¬å¼\ntheta = sp.symbols('theta')\neuler_lhs = sp.exp(sp.I * theta)\neuler_rhs = sp.cos(theta) + sp.I * sp.sin(theta)\neuler_identity = sp.simplify(euler_lhs - euler_rhs) == 0\nprint(f\"e^(iÎ¸) = cosÎ¸ + i sinÎ¸: {euler_identity}\")\n```\n\n## ğŸ§© çº¿æ€§ä»£æ•°\n\n### çŸ©é˜µè¿ç®—\n```python\nimport sympy as sp\n\nprint(\"=== çŸ©é˜µè¿ç®— ===\")\n\n# å®šä¹‰ç¬¦å·çŸ©é˜µ\nA = sp.Matrix([[1, 2], [3, 4]])\nB = sp.Matrix([[2, 0], [1, 2]])\n\nprint(f\"çŸ©é˜µ A:\\n{A}\")\nprint(f\"çŸ©é˜µ B:\\n{B}\")\n\n# åŸºæœ¬è¿ç®—\nprint(f\"\\nçŸ©é˜µåŠ æ³• A+B:\\n{A + B}\")\nprint(f\"çŸ©é˜µä¹˜æ³• AÃ—B:\\n{A * B}\")\nprint(f\"Açš„è¡Œåˆ—å¼: {A.det()}\")\nprint(f\"Açš„é€†çŸ©é˜µ:\\n{A.inv()}\")\n\n# ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡\neigenvals = A.eigenvals()\neigenvects = A.eigenvects()\nprint(f\"\\nAçš„ç‰¹å¾å€¼: {eigenvals}\")\nprint(f\"Açš„ç‰¹å¾å‘é‡: {eigenvects}\")\n\n# è§£çº¿æ€§æ–¹ç¨‹ç»„\nx1, x2 = sp.symbols('x1 x2')\neq1 = sp.Eq(2*x1 + 3*x2, 7)\neq2 = sp.Eq(4*x1 + 5*x2, 13)\nsolution = sp.solve([eq1, eq2], (x1, x2))\nprint(f\"\\næ–¹ç¨‹ç»„:\")\nprint(f\"  {eq1}\")\nprint(f\"  {eq2}\")\nprint(f\"è§£: {solution}\")\n```\n\n## ğŸ“ˆ çº§æ•°å±•å¼€ä¸æ•°å€¼è®¡ç®—\n\n### æ³°å‹’çº§æ•°å±•å¼€\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== çº§æ•°å±•å¼€ ===\")\n\n# å¸¸ç”¨å‡½æ•°çš„æ³°å‹’å±•å¼€\nsin_series = sp.sin(x).series(x, 0, 6)  # åœ¨0å¤„å±•å¼€åˆ°6é˜¶\ncos_series = sp.cos(x).series(x, 0, 6)\nexp_series = sp.exp(x).series(x, 0, 5)\n\nprint(f\"sin(x)çš„æ³°å‹’å±•å¼€: {sin_series}\")\nprint(f\"cos(x)çš„æ³°å‹’å±•å¼€: {cos_series}\")\nprint(f\"e^xçš„æ³°å‹’å±•å¼€: {exp_series}\")\n\n# æ•°å€¼è¿‘ä¼¼\nprint(f\"\\næ•°å€¼è¿‘ä¼¼:\")\nprint(f\"Ï€ â‰ˆ {sp.N(sp.pi, 10)}\")  # 10ä½ç²¾åº¦\nprint(f\"e â‰ˆ {sp.N(sp.E, 8)}\")    # 8ä½ç²¾åº¦\nprint(f\"âˆš2 â‰ˆ {sp.N(sp.sqrt(2), 6)}\")\n\n# ç¬¦å·è¡¨è¾¾å¼çš„æ•°å€¼è®¡ç®—\nexpr = sp.integrate(sp.sin(x), (x, 0, sp.pi/2))\nnumerical_result = sp.N(expr)\nprint(f\"\\nç¬¦å·ç§¯åˆ†: âˆ«â‚€^(Ï€/2) sin(x) dx = {expr}\")\nprint(f\"æ•°å€¼ç»“æœ: {numerical_result}\")\n```\n\n## ğŸ“ å¤æ‚æ•°å­¦é—®é¢˜\n\n### å‡½æ•°åˆ†æä¸æå€¼\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== å‡½æ•°åˆ†æä¸æå€¼ ===\")\n\n# å®šä¹‰å‡½æ•°\nf = x**3 - 6*x**2 + 9*x + 1\nprint(f\"å‡½æ•°: f(x) = {f}\")\n\n# æ±‚å¯¼æ‰¾ä¸´ç•Œç‚¹\nf_prime = sp.diff(f, x)\ncritical_points = sp.solve(f_prime, x)\nprint(f\"ä¸€é˜¶å¯¼æ•°: f'(x) = {f_prime}\")\nprint(f\"ä¸´ç•Œç‚¹: {critical_points}\")\n\n# äºŒé˜¶å¯¼æ•°æµ‹è¯•\nf_double_prime = sp.diff(f, x, 2)\nfor point in critical_points:\n    second_deriv_val = f_double_prime.subs(x, point)\n    if second_deriv_val > 0:\n        extremum_type = \"å±€éƒ¨æå°å€¼\"\n    elif second_deriv_val < 0:\n        extremum_type = \"å±€éƒ¨æå¤§å€¼\"\n    else:\n        extremum_type = \"éœ€è¦è¿›ä¸€æ­¥åˆ†æ\"\n    print(f\"ç‚¹ x = {point}: {extremum_type}\")\n\n# å‡½æ•°å€¼\nfor point in critical_points:\n    func_val = f.subs(x, point)\n    print(f\"f({point}) = {func_val}\")\n```\n\n### æ›²çº¿æ€§è´¨åˆ†æ\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== æ›²çº¿æ€§è´¨åˆ†æ ===\")\n\nf = x**2 * sp.sin(x)\n\n# æ›²çº¿é•¿åº¦ï¼ˆå¼§é•¿ï¼‰\ncurve_length = sp.integrate(sp.sqrt(1 + sp.diff(f, x)**2), (x, 0, sp.pi))\nprint(f\"å‡½æ•°: f(x) = {f}\")\nprint(f\"æ›²çº¿åœ¨ [0,Ï€] ä¸Šçš„é•¿åº¦: {sp.N(curve_length)}\")\n\n# æ—‹è½¬ä½“ä½“ç§¯\nvolume = sp.pi * sp.integrate(f**2, (x, 0, sp.pi))\nprint(f\"æ›²çº¿ç»•xè½´æ—‹è½¬çš„ä½“ç§¯: {sp.N(volume)}\")\n\n# æ›²ç‡\nf_prime = sp.diff(f, x)\nf_double_prime = sp.diff(f, x, 2)\ncurvature = f_double_prime / (1 + f_prime**2)**(3/2)\nprint(f\"æ›²ç‡å…¬å¼: Îº(x) = {curvature}\")\n```\n\n## ğŸ’¡ å®ç”¨å·¥å…·å‡½æ•°\n\n### è‡ªåŠ¨éªŒè¯ç­‰å¼\n```python\nimport sympy as sp\n\ndef verify_identity(expr1, expr2, method=\"simplify\"):\n    \"\"\"\n    éªŒè¯ä¸¤ä¸ªè¡¨è¾¾å¼æ˜¯å¦æ’ç­‰\n    method: \"simplify\", \"expand\", \"factor\", \"trigsimp\"\n    \"\"\"\n    if method == \"simplify\":\n        difference = sp.simplify(expr1 - expr2)\n    elif method == \"expand\":\n        difference = sp.expand(expr1 - expr2)\n    elif method == \"factor\":\n        difference = sp.factor(expr1 - expr2)\n    elif method == \"trigsimp\":\n        difference = sp.trigsimp(expr1 - expr2)\n    else:\n        difference = expr1 - expr2\n    \n    is_identity = (difference == 0)\n    \n    print(f\"è¡¨è¾¾å¼1: {expr1}\")\n    print(f\"è¡¨è¾¾å¼2: {expr2}\")\n    print(f\"éªŒè¯æ–¹æ³•: {method}\")\n    print(f\"æ˜¯å¦æ’ç­‰: {is_identity}\")\n    \n    return is_identity\n\n# ä½¿ç”¨ç¤ºä¾‹\nx, y = sp.symbols('x y')\nverify_identity((x + y)**2, x**2 + 2*x*y + y**2, \"expand\")\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n- æ­£å¸¸å¯¼å…¥ï¼š`import sympy as sp`\n- ä½¿ç”¨æ ‡å‡†çš„ SymPy å‡½æ•°å’Œè¯­æ³•\n- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœ\n- å¯¹äºå¤æ‚è¡¨è¾¾å¼ï¼Œä½¿ç”¨ `sp.N()` è·å–æ•°å€¼ç»“æœ\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\n- ä¸è¦ä½¿ç”¨å¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\n- ä¸è¦æ··åˆä½¿ç”¨ SymPy å’Œæ•°å€¼è®¡ç®—åº“ï¼ˆé™¤éå¿…è¦ï¼‰\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    import sympy as sp\n    x = sp.symbols('x')\n    result = sp.solve(x**2 - 1, x)\n    print(f\"æ–¹ç¨‹è§£: {result}\")\nexcept ImportError:\n    print(\"SymPy ä¸å¯ç”¨\")\nexcept Exception as e:\n    print(f\"è®¡ç®—é”™è¯¯: {e}\")\n```\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç¬¦å·æ•°å­¦è®¡ç®—ï¼\n\n",
    "resources": {
      "references": {
        "matplotlib_cookbook.md": "# Matplotlib å›¾è¡¨ç”ŸæˆæŒ‡å— (v2.2)\n\n## ğŸš€ æ ¸å¿ƒä½¿ç”¨æ–¹æ³•\n\n**é‡è¦æç¤º**ï¼šæ‚¨åªéœ€è¦ä¸“æ³¨äºç»˜å›¾é€»è¾‘ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†å›¾åƒè¾“å‡ºã€‚\n\n### å¿…é¡»éµå¾ªçš„åŸåˆ™ï¼š\n1. **æ­£å¸¸å¯¼å…¥**ï¼š`import matplotlib.pyplot as plt`\n2. **æ­£å¸¸ç»˜å›¾**ï¼šä½¿ç”¨æ ‡å‡†çš„matplotlibå‡½æ•°\n3. **æ— éœ€ç¼–ç **ï¼šç¦æ­¢ä½¿ç”¨`io.BytesIO`ã€`base64`ç­‰æ‰‹åŠ¨ç¼–ç \n4. **æ¨èä½¿ç”¨**ï¼šåœ¨ä»£ç æœ«å°¾è°ƒç”¨`plt.show()`\n\n## ğŸ“Š å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\n\n### æ¨¡æ¿1ï¼šåŸºç¡€æ¡å½¢å›¾\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# å‡†å¤‡æ•°æ®\ndata = {'Category': ['A', 'B', 'C', 'D'], 'Values': [23, 45, 56, 33]}\ndf = pd.DataFrame(data)\n\n# ç»˜å›¾\nplt.figure(figsize=(10, 6))\nplt.bar(df['Category'], df['Values'], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\nplt.title('äº§å“é”€å”®é¢å¯¹æ¯”')\nplt.xlabel('äº§å“ç±»åˆ«')\nplt.ylabel('é”€å”®é¢ (ä¸‡å…ƒ)')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\n\nplt.show()\n```\n\n### æ¨¡æ¿2ï¼šæŠ˜çº¿å›¾\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# æ—¶é—´åºåˆ—æ•°æ®\ndata = {'Time': [1, 2, 3, 4, 5], 'Value': [10, 20, 15, 25, 30]}\ndf = pd.DataFrame(data)\n\nplt.figure(figsize=(10, 6))\nplt.plot(df['Time'], df['Value'], marker='o', linestyle='-', linewidth=2)\nplt.title('æ•°æ®è¶‹åŠ¿åˆ†æ')\nplt.xlabel('æ—¶é—´')\nplt.ylabel('æ•°å€¼')\nplt.grid(True)\nplt.tight_layout()\n\nplt.show()\n```\n\n### æ¨¡æ¿3ï¼šæ•£ç‚¹å›¾\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# ç”Ÿæˆç¤ºä¾‹æ•°æ®\nx = np.random.randn(100)\ny = np.random.randn(100)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.6)\nplt.title('æ•£ç‚¹å›¾ç¤ºä¾‹')\nplt.xlabel('Xè½´')\nplt.ylabel('Yè½´')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\n\nplt.show()\n```\n\n### æ¨¡æ¿4ï¼šå¤šå­å›¾å¸ƒå±€\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.plot(x, y1, 'b-', linewidth=2)\nax1.set_title('æ­£å¼¦å‡½æ•°')\nax1.grid(True)\n\nax2.plot(x, y2, 'r-', linewidth=2)\nax2.set_title('ä½™å¼¦å‡½æ•°')\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ¨ å›¾è¡¨ç±»å‹é€‰æ‹©æŒ‡å—\n\n### æ•°æ®æ¯”è¾ƒï¼š\n- **æ¡å½¢å›¾**ï¼šæ¯”è¾ƒä¸åŒç±»åˆ«çš„æ•°å€¼\n- **æ°´å¹³æ¡å½¢å›¾**ï¼šç±»åˆ«åç§°è¾ƒé•¿æ—¶ä½¿ç”¨\n\n### è¶‹åŠ¿åˆ†æï¼š\n- **æŠ˜çº¿å›¾**ï¼šæ˜¾ç¤ºæ•°æ®éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿\n- **é¢ç§¯å›¾**ï¼šæ˜¾ç¤ºç´¯ç§¯æ•ˆæœ\n\n### åˆ†å¸ƒåˆ†æï¼š\n- **ç›´æ–¹å›¾**ï¼šæ˜¾ç¤ºæ•°æ®åˆ†å¸ƒ\n- **ç®±çº¿å›¾**ï¼šæ˜¾ç¤ºæ•°æ®åˆ†å¸ƒå’Œå¼‚å¸¸å€¼\n- **æ•£ç‚¹å›¾**ï¼šè§‚å¯Ÿä¸¤ä¸ªå˜é‡çš„å…³ç³»\n\n### æ¯”ä¾‹åˆ†æï¼š\n- **é¥¼å›¾**ï¼šæ˜¾ç¤ºå„éƒ¨åˆ†å æ¯”\n- **ç¯å½¢å›¾**ï¼šé¥¼å›¾çš„å˜ä½“\n\n## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹\n\n### å¿…é¡»åŒ…å«ï¼š\n- `import matplotlib.pyplot as plt`\n- æœ‰æ„ä¹‰çš„`plt.title()`ï¼ˆæ ‡é¢˜ä¼šè¢«è‡ªåŠ¨æ•è·ï¼‰\n- `plt.show()`ï¼ˆæ¨èä½†éå¿…é¡»ï¼‰\n\n### ç¦æ­¢æ“ä½œï¼š\n- âŒ ä¸è¦ä½¿ç”¨`base64.b64encode()`\n- âŒ ä¸è¦åˆ›å»º`io.BytesIO()`å¯¹è±¡\n- âŒ ä¸è¦æ‰‹åŠ¨æ„å»ºJSONè¾“å‡º\n\n### æœ€ä½³å®è·µï¼š\n- ä½¿ç”¨`plt.tight_layout()`è‡ªåŠ¨è°ƒæ•´å¸ƒå±€\n- ä½¿ç”¨`plt.grid()`æ·»åŠ ç½‘æ ¼æé«˜å¯è¯»æ€§\n- è®¾ç½®åˆé€‚çš„`figsize`ç¡®ä¿å›¾è¡¨æ¸…æ™°\n\n## ğŸ”§ æ ·å¼é…ç½®ä¸ä¸­æ–‡æ”¯æŒ (å…³é”®)\n\næœ¬ç¯å¢ƒå·²é¢„è£…å¼€æºä¸­æ–‡å­—ä½“ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ä»¥ä¸‹é…ç½®ä»¥é¿å…ä¸­æ–‡ä¹±ç ã€‚\n\n### âœ… æ¨èçš„ä¸­æ–‡å­—ä½“é…ç½®ï¼š\n```python\nimport matplotlib.pyplot as plt\n\n# å¿…é¡»æŒ‡å®šç¯å¢ƒå†…çœŸå®å­˜åœ¨çš„å­—ä½“å\n# ä¼˜å…ˆçº§ï¼šWenQuanYi Micro Hei > WenQuanYi Zen Hei\nplt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei']\nplt.rcParams['axes.unicode_minus'] = False # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n\n# è®¾ç½®å…¨å±€æ ·å¼ï¼ˆå¯é€‰ï¼‰\nplt.style.use('seaborn-v0_8')\nplt.rcParams['font.size'] = 12\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# æ‚¨çš„ç»˜å›¾ä»£ç ...\nplt.plot([1, 2, 3, 4], [1, 4, 2, 3])\nplt.title('å¸¦æ ·å¼é…ç½®çš„å›¾è¡¨')\nplt.show()\n```\n### âŒ ç¦æ­¢ä½¿ç”¨çš„å­—ä½“ (ç¯å¢ƒå†…ä¸å­˜åœ¨)ï¼š\nä¸è¦ä½¿ç”¨ SimHei\nä¸è¦ä½¿ç”¨ Microsoft YaHei\nä¸è¦ä½¿ç”¨ Songti\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨æ•è·æ‰€æœ‰å›¾è¡¨å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç»˜å›¾é€»è¾‘ï¼\n\n\n## ğŸ—ï¸ æµç¨‹å›¾ä¸æ¶æ„å›¾ç”ŸæˆæŒ‡å—\n\n### ä½¿ç”¨åœºæ™¯å¯¹æ¯”\n| éœ€æ±‚ç±»å‹ | æ¨èå·¥å…· | è¾“å‡ºç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |\n|----------|----------|----------|----------|\n| æ•°æ®å›¾è¡¨ | Matplotlib | æ•°æ®é©±åŠ¨ï¼Œæ ·å¼ä¸°å¯Œ | æ•°æ®åˆ†æã€ç»Ÿè®¡å›¾è¡¨ |\n| ä¸“ä¸šæµç¨‹å›¾ | Graphviz | è‡ªåŠ¨å¸ƒå±€ï¼Œæ ·å¼ç»Ÿä¸€ | ç³»ç»Ÿæ¶æ„ã€æµç¨‹å›¾ |\n| ç½‘ç»œå…³ç³»å›¾ | NetworkX | å¤æ‚å…³ç³»ï¼Œç®—æ³•æ”¯æŒ | ç¤¾äº¤ç½‘ç»œã€æ‹“æ‰‘å›¾ |\n\n### Graphviz ä¸“ä¸šæµç¨‹å›¾\n\n#### åŸºç¡€æµç¨‹å›¾æ¨¡æ¿\n```python\nfrom graphviz import Digraph\n\ndef create_basic_flowchart():\n    # åˆ›å»ºæœ‰å‘å›¾\n    dot = Digraph('BasicFlow', comment='åŸºç¡€æµç¨‹å›¾')\n    dot.attr(rankdir='TB', size='8,5')  # å¸ƒå±€æ–¹å‘ï¼šTB(ä»ä¸Šåˆ°ä¸‹), LR(ä»å·¦åˆ°å³)\n    \n    # æ·»åŠ èŠ‚ç‚¹ï¼ˆä¸åŒå½¢çŠ¶ä»£è¡¨ä¸åŒç±»å‹ï¼‰\n    dot.node('start', 'å¼€å§‹', shape='ellipse', color='green')\n    dot.node('process1', 'æ•°æ®å¤„ç†', shape='box')\n    dot.node('decision', 'åˆ¤æ–­æ¡ä»¶', shape='diamond', color='blue')\n    dot.node('process2', 'åç»­å¤„ç†', shape='box')\n    dot.node('end', 'ç»“æŸ', shape='ellipse', color='red')\n    \n    # æ·»åŠ è¿æ¥çº¿\n    dot.edge('start', 'process1', label='è¾“å…¥')\n    dot.edge('process1', 'decision', label='ç»“æœ')\n    dot.edge('decision', 'process2', label='æ˜¯', color='green')\n    dot.edge('decision', 'end', label='å¦', color='red')\n    dot.edge('process2', 'end', label='å®Œæˆ')\n    \n    # ä¿å­˜åˆ°å·¥ä½œåŒºï¼ˆé‡è¦ï¼šå¿…é¡»æŒ‡å®šç»å¯¹è·¯å¾„ï¼‰\n    dot.render('/data/basic_flowchart', format='png', cleanup=True)\n    print(\"æµç¨‹å›¾å·²ä¿å­˜åˆ°å·¥ä½œåŒºï¼š/data/basic_flowchart.png\")\n\ncreate_basic_flowchart()\n```\n\n#### ç³»ç»Ÿæ¶æ„å›¾æ¨¡æ¿\n```python\nfrom graphviz import Digraph\n\ndef create_system_architecture():\n    dot = Digraph('SystemArch', comment='ç³»ç»Ÿæ¶æ„å›¾')\n    dot.attr(rankdir='LR', size='12,8')  # ä»å·¦åˆ°å³å¸ƒå±€\n    \n    # å®šä¹‰èŠ‚ç‚¹ç»„\n    with dot.subgraph(name='cluster_frontend') as frontend:\n        frontend.attr(label='å‰ç«¯å±‚', style='filled', color='lightgrey')\n        frontend.node('web', 'Webåº”ç”¨', shape='box')\n        frontend.node('mobile', 'ç§»åŠ¨ç«¯', shape='box')\n    \n    with dot.subgraph(name='cluster_backend') as backend:\n        backend.attr(label='åç«¯æœåŠ¡', style='filled', color='lightblue')\n        backend.node('api', 'APIç½‘å…³', shape='box')\n        backend.node('auth', 'è®¤è¯æœåŠ¡', shape='box')\n        backend.node('business', 'ä¸šåŠ¡é€»è¾‘', shape='box')\n    \n    with dot.subgraph(name='cluster_data') as data:\n        data.attr(label='æ•°æ®å±‚', style='filled', color='lightgreen')\n        data.node('db', 'æ•°æ®åº“', shape='cylinder')\n        data.node('cache', 'ç¼“å­˜', shape='cylinder')\n    \n    # è¿æ¥å„å±‚\n    dot.edge('web', 'api', label='HTTP')\n    dot.edge('mobile', 'api', label='REST')\n    dot.edge('api', 'auth', label='éªŒè¯')\n    dot.edge('api', 'business', label='è¯·æ±‚')\n    dot.edge('business', 'db', label='æŸ¥è¯¢')\n    dot.edge('business', 'cache', label='è¯»å†™')\n    \n    dot.render('/data/system_architecture', format='png', cleanup=True)\n    print(\"ç³»ç»Ÿæ¶æ„å›¾å·²ä¿å­˜åˆ°å·¥ä½œåŒº\")\n\ncreate_system_architecture()\n```\n\n### NetworkX ç½‘ç»œå…³ç³»å›¾\n\n#### åŸºç¡€ç½‘ç»œå›¾æ¨¡æ¿\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\ndef create_network_diagram():\n    # åˆ›å»ºæœ‰å‘å›¾\n    G = nx.DiGraph()\n    \n    # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹\n    G.add_edge('æ•°æ®æº', 'ETLå¤„ç†')\n    G.add_edge('ETLå¤„ç†', 'æ•°æ®ä»“åº“')\n    G.add_edge('æ•°æ®ä»“åº“', 'æ•°æ®åˆ†æ')\n    G.add_edge('æ•°æ®åˆ†æ', 'å¯è§†åŒ–')\n    G.add_edge('å¯è§†åŒ–', 'ä¸šåŠ¡å†³ç­–')\n    \n    # è®¾ç½®ç»˜å›¾æ ·å¼\n    plt.figure(figsize=(12, 8))\n    \n    # é€‰æ‹©å¸ƒå±€ç®—æ³•\n    pos = nx.spring_layout(G, k=1, iterations=50)\n    \n    # ç»˜åˆ¶èŠ‚ç‚¹å’Œè¾¹\n    nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n                          node_size=2000, alpha=0.9)\n    nx.draw_networkx_edges(G, pos, edge_color='gray', \n                          arrows=True, arrowsize=20)\n    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n    \n    # æ·»åŠ æ ‡é¢˜å’Œè°ƒæ•´å¸ƒå±€\n    plt.title('æ•°æ®å¤„ç†æµæ°´çº¿ç½‘ç»œå›¾', size=16, pad=20)\n    plt.axis('off')\n    plt.tight_layout()\n    \n    # ä¿å­˜åˆ°å·¥ä½œåŒº\n    plt.savefig('/data/network_pipeline.png', dpi=150, bbox_inches='tight')\n    plt.close()\n    print(\"ç½‘ç»œå›¾å·²ä¿å­˜åˆ°å·¥ä½œåŒºï¼š/data/network_pipeline.png\")\n\ncreate_network_diagram()\n```\n\n#### å¤æ‚ç½‘ç»œåˆ†ææ¨¡æ¿\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_complex_network():\n    # åˆ›å»ºéšæœºç½‘ç»œ\n    G = nx.erdos_renyi_graph(30, 0.1)\n    \n    # è®¡ç®—ç½‘ç»œæŒ‡æ ‡\n    degrees = dict(G.degree())\n    betweenness = nx.betweenness_centrality(G)\n    \n    # è®¾ç½®èŠ‚ç‚¹å¤§å°å’Œé¢œè‰²åŸºäºä¸­å¿ƒæ€§\n    node_sizes = [v * 500 for v in degrees.values()]\n    node_colors = list(betweenness.values())\n    \n    # ç»˜åˆ¶å›¾å½¢\n    plt.figure(figsize=(14, 10))\n    pos = nx.spring_layout(G, seed=42)\n    \n    # ç»˜åˆ¶ç½‘ç»œ\n    nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes,\n                                 node_color=node_colors, \n                                 cmap=plt.cm.viridis, alpha=0.8)\n    nx.draw_networkx_edges(G, pos, alpha=0.5)\n    nx.draw_networkx_labels(G, pos, font_size=8)\n    \n    # æ·»åŠ é¢œè‰²æ¡\n    plt.colorbar(nodes, label='ä»‹æ•°ä¸­å¿ƒæ€§')\n    plt.title('å¤æ‚ç½‘ç»œåˆ†æå›¾ï¼ˆèŠ‚ç‚¹å¤§å°=åº¦ï¼Œé¢œè‰²=ä¸­å¿ƒæ€§ï¼‰', size=14)\n    plt.axis('off')\n    \n    # ä¿å­˜ç»“æœ\n    plt.savefig('/data/complex_network.png', dpi=150, bbox_inches='tight')\n    plt.close()\n    \n    # è¾“å‡ºç½‘ç»œç»Ÿè®¡ä¿¡æ¯\n    print(f\"ç½‘ç»œç»Ÿè®¡:\")\n    print(f\"- èŠ‚ç‚¹æ•°: {G.number_of_nodes()}\")\n    print(f\"- è¾¹æ•°: {G.number_of_edges()}\")\n    print(f\"- å¹³å‡åº¦: {np.mean(list(degrees.values())):.2f}\")\n    print(\"ç½‘ç»œå›¾å·²ä¿å­˜åˆ°å·¥ä½œåŒº\")\n\ncreate_complex_network()\n```\n\n### æœ€ä½³å®è·µä¸æ³¨æ„äº‹é¡¹\nâœ… æ¨èåšæ³•ï¼š\n- Graphviz ç”¨äºï¼šæµç¨‹å›¾ã€æ¶æ„å›¾ã€ç±»å›¾ç­‰éœ€è¦ä¸“ä¸šå¸ƒå±€çš„å›¾è¡¨\n- NetworkX + Matplotlib ç”¨äºï¼šæ•°æ®å…³ç³»ç½‘ç»œã€ç¤¾äº¤ç½‘ç»œã€æ‹“æ‰‘åˆ†æ\n- çº¯ Matplotlib ç”¨äºï¼šæ•°æ®å¯è§†åŒ–ã€ç»Ÿè®¡å›¾è¡¨\n\nâš ï¸ é‡è¦æé†’ï¼š\n- Graphviz å¿…é¡»æŒ‡å®šç»å¯¹è·¯å¾„ï¼š`/data/æ–‡ä»¶å`\n- æ¸…ç†ä¸­é—´æ–‡ä»¶ï¼šä½¿ç”¨ `cleanup=True` åˆ é™¤ä¸´æ—¶æ–‡ä»¶\n- å†…å­˜ç®¡ç†ï¼šå¤æ‚ç½‘ç»œåˆ†ææ—¶æ³¨æ„èŠ‚ç‚¹æ•°é‡\n- æ–‡ä»¶æ ¼å¼ï¼šæ”¯æŒ PNGã€PDFã€SVG ç­‰æ ¼å¼\n\nğŸ”§ æ•…éšœæ’é™¤ï¼š\n```python\n# éªŒè¯ Graphviz å®‰è£…\ndef check_graphviz_installation():\n    try:\n        from graphviz import Digraph\n        dot = Digraph()\n        dot.node('test', 'Test')\n        dot.render('/data/test_graphviz', format='png', cleanup=True)\n        print(\"âœ… Graphviz å·¥ä½œæ­£å¸¸\")\n        return True\n    except Exception as e:\n        print(f\"âŒ Graphviz é”™è¯¯: {e}\")\n        return False\n\ncheck_graphviz_installation()\n```\n**è®°ä½**ï¼šé€‰æ‹©åˆé€‚çš„å·¥å…·å¯ä»¥è®©å›¾è¡¨æ›´åŠ ä¸“ä¸šå’Œæ¸…æ™°ï¼\n",
        "ml_workflow.md": "# æœºå™¨å­¦ä¹ å·¥ä½œæµæŒ‡å— (v2.2)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**åŠŸèƒ½**ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼\n\n## ğŸ“Š åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡æ¿\n\n### æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef prepare_ml_data():\n    \"\"\"æœºå™¨å­¦ä¹ æ•°æ®å‡†å¤‡ç¤ºä¾‹\"\"\"\n    \n    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†\n    np.random.seed(42)\n    n_samples = 1000\n    \n    # å›å½’é—®é¢˜æ•°æ®\n    X_reg = np.random.normal(0, 1, (n_samples, 5))\n    y_reg = 2 * X_reg[:, 0] + 1.5 * X_reg[:, 1] - X_reg[:, 2] + np.random.normal(0, 0.5, n_samples)\n    \n    # åˆ†ç±»é—®é¢˜æ•°æ®\n    X_clf = np.random.normal(0, 1, (n_samples, 4))\n    y_clf = (X_clf[:, 0] + X_clf[:, 1] > 0).astype(int)\n    \n    print(\"=== æ•°æ®å‡†å¤‡å®Œæˆ ===\")\n    print(f\"æ ·æœ¬æ•°é‡: {n_samples}\")\n    print(f\"å›å½’ç‰¹å¾ç»´åº¦: {X_reg.shape[1]}\")\n    print(f\"åˆ†ç±»ç‰¹å¾ç»´åº¦: {X_clf.shape[1]}\")\n    print(f\"åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(y_clf, return_counts=True)}\")\n    \n    return X_reg, y_reg, X_clf, y_clf\n\n# ä½¿ç”¨ç¤ºä¾‹\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\n```\n\n### æ ‡å‡†æœºå™¨å­¦ä¹ å·¥ä½œæµ\n```python\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\nfrom sklearn.model_selection import cross_val_score\n\ndef standard_ml_pipeline(X, y, problem_type='regression'):\n    \"\"\"æ ‡å‡†æœºå™¨å­¦ä¹ æµç¨‹\"\"\"\n    \n    print(f\"=== å¼€å§‹ {problem_type} æ¨¡å‹è®­ç»ƒ ===\")\n    \n    # æ•°æ®åˆ†å‰²\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42,\n        stratify=y if problem_type == 'classification' else None\n    )\n    \n    print(f\"è®­ç»ƒé›†å¤§å°: {X_train.shape}\")\n    print(f\"æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\n    \n    # ç‰¹å¾æ ‡å‡†åŒ–\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # é€‰æ‹©æ¨¡å‹\n    if problem_type == 'regression':\n        model = RandomForestRegressor(n_estimators=100, random_state=42)\n    else:\n        model = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    # è®­ç»ƒæ¨¡å‹\n    model.fit(X_train_scaled, y_train)\n    \n    # é¢„æµ‹\n    y_pred = model.predict(X_test_scaled)\n    \n    # æ¨¡å‹è¯„ä¼°\n    if problem_type == 'regression':\n        mse = mean_squared_error(y_test, y_pred)\n        rmse = np.sqrt(mse)\n        r2 = r2_score(y_test, y_pred)\n        \n        print(f\"å›å½’æ¨¡å‹æ€§èƒ½:\")\n        print(f\"  MSE: {mse:.4f}\")\n        print(f\"  RMSE: {rmse:.4f}\")\n        print(f\"  RÂ²: {r2:.4f}\")\n        \n        metrics = {'mse': mse, 'rmse': rmse, 'r2': r2}\n    else:\n        accuracy = accuracy_score(y_test, y_pred)\n        print(f\"åˆ†ç±»æ¨¡å‹æ€§èƒ½:\")\n        print(f\"  å‡†ç¡®ç‡: {accuracy:.4f}\")\n        print(\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n        print(classification_report(y_test, y_pred))\n        \n        metrics = {'accuracy': accuracy}\n    \n    # äº¤å‰éªŒè¯\n    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, \n                               scoring='r2' if problem_type == 'regression' else 'accuracy')\n    print(f\"äº¤å‰éªŒè¯å¹³å‡å¾—åˆ†: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})\")\n    \n    return {\n        'model': model,\n        'metrics': metrics,\n        'X_test': X_test,\n        'y_test': y_test,\n        'y_pred': y_pred,\n        'cv_scores': cv_scores\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\n# regression_results = standard_ml_pipeline(X_reg, y_reg, 'regression')\n# classification_results = standard_ml_pipeline(X_clf, y_clf, 'classification')\n```\n\n## ğŸ“ˆ å›å½’åˆ†æå®Œæ•´å·¥ä½œæµ\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef complete_regression_analysis():\n    \"\"\"å®Œæ•´çš„å›å½’åˆ†æå·¥ä½œæµ\"\"\"\n    \n    print(\"=== å¼€å§‹å›å½’åˆ†æ ===\")\n    \n    # 1. æ•°æ®ç”Ÿæˆ\n    np.random.seed(42)\n    n_samples = 500\n    \n    # åˆ›å»ºæœ‰æ„ä¹‰çš„ç‰¹å¾\n    feature1 = np.random.normal(50, 15, n_samples)  # å¹´é¾„\n    feature2 = np.random.normal(100, 25, n_samples) # æ”¶å…¥\n    feature3 = np.random.normal(10, 3, n_samples)   # æ•™è‚²å¹´é™\n    feature4 = np.random.normal(0, 1, n_samples)    # å™ªå£°ç‰¹å¾\n    \n    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆæ¨¡æ‹Ÿæˆ¿ä»·ï¼‰\n    target = (50 * feature1 + 80 * feature2 + 5000 * feature3 + \n              10 * feature1 * feature3 + np.random.normal(0, 10000, n_samples))\n    \n    df = pd.DataFrame({\n        'å¹´é¾„': feature1,\n        'æ”¶å…¥': feature2,\n        'æ•™è‚²å¹´é™': feature3,\n        'å™ªå£°ç‰¹å¾': feature4,\n        'æˆ¿ä»·': target\n    })\n    \n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\n    print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n    print(f\"ç‰¹å¾åˆ—è¡¨: {list(df.columns[:-1])}\")\n    print(f\"ç›®æ ‡å˜é‡: {df.columns[-1]}\")\n    \n    # 2. æ•°æ®æ¢ç´¢\n    print(\"\\n=== æ•°æ®æ¢ç´¢ ===\")\n    print(\"æ•°å€¼ç‰¹å¾ç»Ÿè®¡:\")\n    print(df.describe())\n    \n    # ç›¸å…³æ€§åˆ†æ\n    correlation = df.corr()['æˆ¿ä»·'].sort_values(ascending=False)\n    print(\"\\nç‰¹å¾ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§:\")\n    for feature, corr in correlation.items():\n        if feature != 'æˆ¿ä»·':\n            print(f\"  {feature}: {corr:.3f}\")\n    \n    # 3. æ¨¡å‹è®­ç»ƒ\n    X = df.drop('æˆ¿ä»·', axis=1)\n    y = df['æˆ¿ä»·']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    model = RandomForestRegressor(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    # 4. æ¨¡å‹è¯„ä¼°\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½ ===\")\n    print(f\"å‡æ–¹è¯¯å·® (MSE): {mse:,.2f}\")\n    print(f\"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:,.2f}\")\n    print(f\"å†³å®šç³»æ•° (RÂ²): {r2:.4f}\")\n    \n    # 5. ç‰¹å¾é‡è¦æ€§\n    feature_importance = pd.DataFrame({\n        'ç‰¹å¾': X.columns,\n        'é‡è¦æ€§': model.feature_importances_\n    }).sort_values('é‡è¦æ€§', ascending=False)\n    \n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ ===\")\n    for _, row in feature_importance.iterrows():\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\n    \n    # 6. å¯è§†åŒ–åˆ†æ\n    plt.figure(figsize=(15, 10))\n    \n    # å®é™…å€¼ vs é¢„æµ‹å€¼\n    plt.subplot(2, 3, 1)\n    plt.scatter(y_test, y_pred, alpha=0.6)\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n    plt.xlabel('å®é™…å€¼')\n    plt.ylabel('é¢„æµ‹å€¼')\n    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')\n    plt.grid(True, alpha=0.3)\n    \n    # æ®‹å·®åˆ†æ\n    plt.subplot(2, 3, 2)\n    residuals = y_test - y_pred\n    plt.scatter(y_pred, residuals, alpha=0.6)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('é¢„æµ‹å€¼')\n    plt.ylabel('æ®‹å·®')\n    plt.title('æ®‹å·®åˆ†æ')\n    plt.grid(True, alpha=0.3)\n    \n    # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\n    plt.subplot(2, 3, 3)\n    top_features = feature_importance.head(5)\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\n    plt.xlabel('é‡è¦æ€§')\n    plt.title('Top 5 ç‰¹å¾é‡è¦æ€§')\n    plt.gca().invert_yaxis()\n    \n    # è¯¯å·®åˆ†å¸ƒ\n    plt.subplot(2, 3, 4)\n    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n    plt.xlabel('æ®‹å·®')\n    plt.ylabel('é¢‘æ•°')\n    plt.title('è¯¯å·®åˆ†å¸ƒ')\n    plt.grid(True, alpha=0.3)\n    \n    # ç›¸å¯¹è¯¯å·®\n    plt.subplot(2, 3, 5)\n    relative_error = np.abs(residuals / y_test) * 100\n    plt.hist(relative_error, bins=30, alpha=0.7, edgecolor='black')\n    plt.xlabel('ç›¸å¯¹è¯¯å·® (%)')\n    plt.ylabel('é¢‘æ•°')\n    plt.title('ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ')\n    plt.grid(True, alpha=0.3)\n    \n    # é¢„æµ‹è¯¯å·®ç®±çº¿å›¾\n    plt.subplot(2, 3, 6)\n    plt.boxplot(relative_error)\n    plt.ylabel('ç›¸å¯¹è¯¯å·® (%)')\n    plt.title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 7. æ¨¡å‹è§£é‡Š\n    print(f\"\\n=== æ¨¡å‹è§£é‡Š ===\")\n    print(f\"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if r2 > 0.8 else 'è‰¯å¥½' if r2 > 0.6 else 'ä¸€èˆ¬'}\")\n    print(f\"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}\")\n    print(f\"å»ºè®®: å…³æ³¨{feature_importance.iloc[0]['ç‰¹å¾']}å’Œ{feature_importance.iloc[1]['ç‰¹å¾']}çš„ä¼˜åŒ–\")\n    \n    return {\n        'model': model,\n        'metrics': {'mse': mse, 'rmse': rmse, 'r2': r2},\n        'feature_importance': feature_importance,\n        'predictions': y_pred\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# regression_results = complete_regression_analysis()\n```\n\n## ğŸ” åˆ†ç±»åˆ†æå®Œæ•´å·¥ä½œæµ\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import make_classification\n\ndef complete_classification_analysis():\n    \"\"\"å®Œæ•´çš„åˆ†ç±»åˆ†æå·¥ä½œæµ\"\"\"\n    \n    print(\"=== å¼€å§‹åˆ†ç±»åˆ†æ ===\")\n    \n    # 1. æ•°æ®ç”Ÿæˆ\n    X, y = make_classification(\n        n_samples=1000,\n        n_features=8,\n        n_informative=5,\n        n_redundant=2,\n        n_classes=3,\n        random_state=42\n    )\n    \n    feature_names = [f'ç‰¹å¾_{i+1}' for i in range(X.shape[1])]\n    df = pd.DataFrame(X, columns=feature_names)\n    df['ç±»åˆ«'] = y\n    \n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\n    print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n    print(f\"ç‰¹å¾æ•°é‡: {X.shape[1]}\")\n    print(f\"ç±»åˆ«æ•°é‡: {len(np.unique(y))}\")\n    print(f\"ç±»åˆ«åˆ†å¸ƒ: {np.unique(y, return_counts=True)}\")\n    \n    # 2. æ•°æ®æ¢ç´¢\n    print(\"\\n=== æ•°æ®æ¢ç´¢ ===\")\n    print(\"æ•°å€¼ç‰¹å¾ç»Ÿè®¡:\")\n    print(df.describe())\n    \n    # 3. æ¨¡å‹è®­ç»ƒ\n    X_data = df.drop('ç±»åˆ«', axis=1)\n    y_data = df['ç±»åˆ«']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n    )\n    \n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    # 4. æ¨¡å‹è¯„ä¼°\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½ ===\")\n    print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\n    print(\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n    print(classification_report(y_test, y_pred))\n    \n    # 5. ç‰¹å¾é‡è¦æ€§\n    feature_importance = pd.DataFrame({\n        'ç‰¹å¾': X_data.columns,\n        'é‡è¦æ€§': model.feature_importances_\n    }).sort_values('é‡è¦æ€§', ascending=False)\n    \n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ ===\")\n    for _, row in feature_importance.iterrows():\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\n    \n    # 6. å¯è§†åŒ–åˆ†æ\n    plt.figure(figsize=(15, 10))\n    \n    # æ··æ·†çŸ©é˜µ\n    plt.subplot(2, 3, 1)\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('é¢„æµ‹æ ‡ç­¾')\n    plt.ylabel('çœŸå®æ ‡ç­¾')\n    plt.title('æ··æ·†çŸ©é˜µ')\n    \n    # ç‰¹å¾é‡è¦æ€§\n    plt.subplot(2, 3, 2)\n    top_features = feature_importance.head(8)\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\n    plt.xlabel('é‡è¦æ€§')\n    plt.title('ç‰¹å¾é‡è¦æ€§æ’å')\n    plt.gca().invert_yaxis()\n    \n    # ç±»åˆ«åˆ†å¸ƒ\n    plt.subplot(2, 3, 3)\n    unique, counts = np.unique(y, return_counts=True)\n    plt.pie(counts, labels=[f'ç±»åˆ« {cls}' for cls in unique], autopct='%1.1f%%')\n    plt.title('ç±»åˆ«åˆ†å¸ƒ')\n    \n    # åˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾\n    plt.subplot(2, 3, 4)\n    report_dict = classification_report(y_test, y_pred, output_dict=True)\n    report_df = pd.DataFrame(report_dict).transpose().iloc[:-3, :-1]\n    sns.heatmap(report_df, annot=True, cmap='YlOrRd', fmt='.3f')\n    plt.title('åˆ†ç±»æŒ‡æ ‡çƒ­åŠ›å›¾')\n    \n    # å­¦ä¹ æ›²çº¿ï¼ˆç®€åŒ–ç‰ˆï¼‰\n    plt.subplot(2, 3, 5)\n    train_sizes = np.linspace(0.1, 1.0, 10)\n    train_scores = []\n    test_scores = []\n    \n    for size in train_sizes:\n        n_train = int(size * len(X_train))\n        X_train_sub = X_train.iloc[:n_train]\n        y_train_sub = y_train.iloc[:n_train]\n        \n        model_temp = RandomForestClassifier(n_estimators=50, random_state=42)\n        model_temp.fit(X_train_sub, y_train_sub)\n        \n        train_score = model_temp.score(X_train_sub, y_train_sub)\n        test_score = model_temp.score(X_test, y_test)\n        \n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    \n    plt.plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒå¾—åˆ†')\n    plt.plot(train_sizes, test_scores, 'o-', label='æµ‹è¯•å¾—åˆ†')\n    plt.xlabel('è®­ç»ƒæ ·æœ¬æ¯”ä¾‹')\n    plt.ylabel('å‡†ç¡®ç‡')\n    plt.title('å­¦ä¹ æ›²çº¿')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # ç±»åˆ«é¢„æµ‹åˆ†å¸ƒ\n    plt.subplot(2, 3, 6)\n    pred_counts = pd.Series(y_pred).value_counts().sort_index()\n    true_counts = pd.Series(y_test).value_counts().sort_index()\n    \n    x = np.arange(len(true_counts))\n    width = 0.35\n    \n    plt.bar(x - width/2, true_counts, width, label='çœŸå®åˆ†å¸ƒ', alpha=0.7)\n    plt.bar(x + width/2, pred_counts, width, label='é¢„æµ‹åˆ†å¸ƒ', alpha=0.7)\n    plt.xlabel('ç±»åˆ«')\n    plt.ylabel('æ ·æœ¬æ•°')\n    plt.title('ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 7. æ¨¡å‹è§£é‡Š\n    print(f\"\\n=== æ¨¡å‹è§£é‡Š ===\")\n    print(f\"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if accuracy > 0.9 else 'è‰¯å¥½' if accuracy > 0.8 else 'ä¸€èˆ¬'}\")\n    print(f\"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}\")\n    print(f\"æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«: æŸ¥çœ‹æ··æ·†çŸ©é˜µå¯¹è§’çº¿å¤–çš„æœ€å¤§å€¼\")\n    \n    return {\n        'model': model,\n        'metrics': {'accuracy': accuracy},\n        'feature_importance': feature_importance,\n        'predictions': y_pred\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# classification_results = complete_classification_analysis()\n```\n\n## ğŸ“Š ç»Ÿè®¡å»ºæ¨¡åˆ†æ\n\n```python\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndef statistical_modeling_analysis():\n    \"\"\"ç»Ÿè®¡å»ºæ¨¡åˆ†æ\"\"\"\n    \n    print(\"=== å¼€å§‹ç»Ÿè®¡å»ºæ¨¡åˆ†æ ===\")\n    \n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\n    np.random.seed(42)\n    n_samples = 200\n    \n    data = pd.DataFrame({\n        'å¹¿å‘ŠæŠ•å…¥': np.random.normal(1000, 300, n_samples),\n        'ä»·æ ¼': np.random.normal(50, 15, n_samples),\n        'ä¿ƒé”€æ´»åŠ¨': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n        'å­£èŠ‚æ€§': np.random.choice([0, 1], n_samples, p=[0.5, 0.5])\n    })\n    \n    # ç”Ÿæˆé”€å”®é¢ï¼ˆä¸ç‰¹å¾æœ‰çœŸå®å…³ç³»ï¼‰\n    data['é”€å”®é¢'] = (\n        500 + 0.8 * data['å¹¿å‘ŠæŠ•å…¥'] - 5 * data['ä»·æ ¼'] + \n        200 * data['ä¿ƒé”€æ´»åŠ¨'] + 150 * data['å­£èŠ‚æ€§'] + \n        np.random.normal(0, 100, n_samples)\n    )\n    \n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\n    print(f\"æ ·æœ¬æ•°é‡: {len(data)}\")\n    print(f\"ç‰¹å¾: {list(data.columns[:-1])}\")\n    print(\"\\næ•°æ®æè¿°:\")\n    print(data.describe())\n    \n    # 1. OLS å›å½’åˆ†æ\n    print(\"\\n=== OLS å›å½’åˆ†æ ===\")\n    model = smf.ols('é”€å”®é¢ ~ å¹¿å‘ŠæŠ•å…¥ + ä»·æ ¼ + ä¿ƒé”€æ´»åŠ¨ + å­£èŠ‚æ€§', data=data).fit()\n    \n    print(\"å›å½’ç»“æœæ‘˜è¦:\")\n    print(model.summary())\n    \n    # 2. å…³é”®ç»Ÿè®¡æŒ‡æ ‡\n    print(f\"\\n=== å…³é”®ç»Ÿè®¡æŒ‡æ ‡ ===\")\n    print(f\"RÂ²: {model.rsquared:.4f}\")\n    print(f\"è°ƒæ•´RÂ²: {model.rsquared_adj:.4f}\")\n    print(f\"Fç»Ÿè®¡é‡: {model.fvalue:.2f}\")\n    print(f\"Fç»Ÿè®¡é‡på€¼: {model.f_pvalue:.4f}\")\n    \n    # 3. ç³»æ•°è§£é‡Š\n    print(f\"\\n=== ç³»æ•°è§£é‡Š ===\")\n    for feature, coef in model.params.items():\n        p_value = model.pvalues[feature]\n        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n        print(f\"{feature}: {coef:.2f} {significance} (på€¼: {p_value:.4f})\")\n    \n    # 4. æ®‹å·®åˆ†æ\n    print(f\"\\n=== æ®‹å·®åˆ†æ ===\")\n    residuals = model.resid\n    print(f\"æ®‹å·®å‡å€¼: {residuals.mean():.4f}\")\n    print(f\"æ®‹å·®æ ‡å‡†å·®: {residuals.std():.4f}\")\n    \n    # 5. å¯è§†åŒ–åˆ†æ\n    plt.figure(figsize=(15, 10))\n    \n    # å®é™…å€¼ vs é¢„æµ‹å€¼\n    plt.subplot(2, 3, 1)\n    y_pred_ols = model.predict(data[['å¹¿å‘ŠæŠ•å…¥', 'ä»·æ ¼', 'ä¿ƒé”€æ´»åŠ¨', 'å­£èŠ‚æ€§']])\n    plt.scatter(data['é”€å”®é¢'], y_pred_ols, alpha=0.6)\n    plt.plot([data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], \n             [data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], 'r--', lw=2)\n    plt.xlabel('å®é™…é”€å”®é¢')\n    plt.ylabel('é¢„æµ‹é”€å”®é¢')\n    plt.title(f'OLSé¢„æµ‹æ•ˆæœ (RÂ² = {model.rsquared:.3f})')\n    plt.grid(True, alpha=0.3)\n    \n    # æ®‹å·®å›¾\n    plt.subplot(2, 3, 2)\n    plt.scatter(y_pred_ols, residuals, alpha=0.6)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('é¢„æµ‹å€¼')\n    plt.ylabel('æ®‹å·®')\n    plt.title('æ®‹å·®åˆ†æ')\n    plt.grid(True, alpha=0.3)\n    \n    # Q-Qå›¾\n    plt.subplot(2, 3, 3)\n    sm.qqplot(residuals, line='45', ax=plt.gca())\n    plt.title('Q-Qå›¾ï¼ˆæ®‹å·®æ­£æ€æ€§æ£€éªŒï¼‰')\n    \n    # ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³ç³»\n    plt.subplot(2, 3, 4)\n    plt.scatter(data['å¹¿å‘ŠæŠ•å…¥'], data['é”€å”®é¢'], alpha=0.6)\n    plt.xlabel('å¹¿å‘ŠæŠ•å…¥')\n    plt.ylabel('é”€å”®é¢')\n    plt.title('å¹¿å‘ŠæŠ•å…¥ vs é”€å”®é¢')\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(2, 3, 5)\n    plt.scatter(data['ä»·æ ¼'], data['é”€å”®é¢'], alpha=0.6)\n    plt.xlabel('ä»·æ ¼')\n    plt.ylabel('é”€å”®é¢')\n    plt.title('ä»·æ ¼ vs é”€å”®é¢')\n    plt.grid(True, alpha=0.3)\n    \n    # ç³»æ•°å¯è§†åŒ–\n    plt.subplot(2, 3, 6)\n    coefficients = model.params.iloc[1:]  # æ’é™¤æˆªè·é¡¹\n    colors = ['green' if p < 0.05 else 'red' for p in model.pvalues.iloc[1:]]\n    plt.barh(coefficients.index, coefficients.values, color=colors)\n    plt.axvline(x=0, color='black', linestyle='-')\n    plt.xlabel('ç³»æ•°å€¼')\n    plt.title('ç‰¹å¾ç³»æ•°ï¼ˆç»¿è‰²è¡¨ç¤ºæ˜¾è‘—ï¼‰')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 6. ä¸šåŠ¡è§£é‡Š\n    print(f\"\\n=== ä¸šåŠ¡è§£é‡Š ===\")\n    print(f\"æ¨¡å‹è§£é‡ŠåŠ›: {'å¼º' if model.rsquared > 0.7 else 'ä¸­ç­‰' if model.rsquared > 0.5 else 'å¼±'}\")\n    \n    significant_features = []\n    for feature in model.params.index[1:]:  # æ’é™¤æˆªè·\n        if model.pvalues[feature] < 0.05:\n            significant_features.append(feature)\n    \n    if significant_features:\n        print(f\"æ˜¾è‘—å½±å“ç‰¹å¾: {', '.join(significant_features)}\")\n    else:\n        print(\"æ²¡æœ‰å‘ç°ç»Ÿè®¡æ˜¾è‘—çš„ç‰¹å¾\")\n    \n    return {\n        'model': model,\n        'rsquared': model.rsquared,\n        'significant_features': significant_features,\n        'residuals': residuals\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# stats_results = statistical_modeling_analysis()\n```\n\n## ğŸ”§ æ¨¡å‹ä¼˜åŒ–ä¸è°ƒå‚\n\n```python\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\ndef model_optimization_pipeline(X, y, problem_type='regression'):\n    \"\"\"æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–æµç¨‹\"\"\"\n    \n    print(f\"=== å¼€å§‹ {problem_type} æ¨¡å‹ä¼˜åŒ– ===\")\n    \n    # æ•°æ®åˆ†å‰²\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # é€‰æ‹©æ¨¡å‹å’Œå‚æ•°ç½‘æ ¼\n    if problem_type == 'regression':\n        model = RandomForestRegressor(random_state=42)\n        param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4]\n        }\n        scoring = 'r2'\n    else:\n        model = RandomForestClassifier(random_state=42)\n        param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4]\n        }\n        scoring = 'accuracy'\n    \n    # ç½‘æ ¼æœç´¢\n    print(\"æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢...\")\n    grid_search = GridSearchCV(\n        model, param_grid, cv=5, scoring=scoring, \n        n_jobs=-1, verbose=1\n    )\n    grid_search.fit(X_train, y_train)\n    \n    # è¾“å‡ºæœ€ä¼˜å‚æ•°\n    print(f\"\\n=== æœ€ä¼˜å‚æ•° ===\")\n    for param, value in grid_search.best_params_.items():\n        print(f\"  {param}: {value}\")\n    \n    print(f\"æœ€ä¼˜æ¨¡å‹å¾—åˆ†: {grid_search.best_score_:.4f}\")\n    \n    # æµ‹è¯•é›†æ€§èƒ½\n    best_model = grid_search.best_estimator_\n    y_pred = best_model.predict(X_test)\n    \n    if problem_type == 'regression':\n        test_score = r2_score(y_test, y_pred)\n        print(f\"æµ‹è¯•é›† RÂ²: {test_score:.4f}\")\n    else:\n        test_score = accuracy_score(y_test, y_pred)\n        print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}\")\n    \n    return {\n        'best_model': best_model,\n        'best_params': grid_search.best_params_,\n        'best_score': grid_search.best_score_,\n        'test_score': test_score\n    }\n\n# ä½¿ç”¨ç¤ºä¾‹\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\n# optimized_regression = model_optimization_pipeline(X_reg, y_reg, 'regression')\n# optimized_classification = model_optimization_pipeline(X_clf, y_clf, 'classification')\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n- ä½¿ç”¨æ ‡å‡†çš„ scikit-learn å’Œ statsmodels æ¥å£\n- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœå’ŒæŒ‡æ ‡\n- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨\n- å¯¹æ•°æ®è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†å’Œæ ‡å‡†åŒ–\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\n- ä¸è¦ä½¿ç”¨ `base64` ç¼–ç \n- ä¸è¦åˆ›å»ºå¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    from sklearn.ensemble import RandomForestRegressor\n    # æ¨¡å‹è®­ç»ƒä»£ç \nexcept ImportError:\n    print(\"scikit-learn ä¸å¯ç”¨\")\n\ntry:\n    import statsmodels.api as sm\n    # ç»Ÿè®¡å»ºæ¨¡ä»£ç \nexcept ImportError:\n    print(\"statsmodels ä¸å¯ç”¨\")\n```\n\n### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š\n```python\n# å¿«é€Ÿæ¨¡å‹è¯„ä¼°å‡½æ•°\ndef quick_model_evaluation(model, X_test, y_test, problem_type='regression'):\n    \"\"\"å¿«é€Ÿæ¨¡å‹è¯„ä¼°\"\"\"\n    y_pred = model.predict(X_test)\n    \n    if problem_type == 'regression':\n        r2 = r2_score(y_test, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n        print(f\"RÂ²: {r2:.4f}, RMSE: {rmse:.4f}\")\n    else:\n        accuracy = accuracy_score(y_test, y_pred)\n        print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\n    \n    return y_pred\n```\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºæœºå™¨å­¦ä¹ å»ºæ¨¡å’Œåˆ†æé€»è¾‘ï¼\n",
        "pandas_cheatsheet.md": "# Pandas æ•°æ®å¤„ç†æŒ‡å— (v2.2)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**åŠŸèƒ½**ï¼šæ•°æ®æ¸…æ´—ã€è½¬æ¢ã€åˆ†æå’Œå¯è§†åŒ–\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼\n\n## ğŸ”§ åŸºç¡€æ•°æ®æ“ä½œ\n\n### æ•°æ®åˆ›å»ºä¸æŸ¥çœ‹\n```python\nimport pandas as pd\nimport numpy as np\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n    'Age': [25, 30, 35, 28, 32],\n    'Salary': [50000, 60000, 70000, 55000, 65000],\n    'Department': ['IT', 'HR', 'IT', 'Finance', 'Marketing'],\n    'Join_Date': pd.date_range('2020-01-01', periods=5, freq='Y')\n})\n\nprint(\"=== æ•°æ®åŸºæœ¬ä¿¡æ¯ ===\")\nprint(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\nprint(f\"åˆ—å: {list(df.columns)}\")\nprint(\"\\nå‰5è¡Œæ•°æ®:\")\nprint(df.head())\nprint(\"\\næ•°æ®ä¿¡æ¯:\")\nprint(df.info())\nprint(\"\\næ•°å€¼åˆ—ç»Ÿè®¡:\")\nprint(df.describe())\n```\n\n### æ•°æ®ç­›é€‰ä¸æ’åº\n```python\nimport pandas as pd\n\n# å‡è®¾dfæ˜¯å·²æœ‰çš„DataFrame\nprint(\"=== æ•°æ®ç­›é€‰ä¸æ’åº ===\")\n\n# æ¡ä»¶ç­›é€‰\nage_above_30 = df[df['Age'] > 30]\nprint(f\"å¹´é¾„å¤§äº30çš„å‘˜å·¥: {len(age_above_30)}äºº\")\nprint(age_above_30[['Name', 'Age', 'Department']])\n\n# å¤šæ¡ä»¶ç­›é€‰\nit_high_salary = df[(df['Department'] == 'IT') & (df['Salary'] > 55000)]\nprint(f\"\\nITéƒ¨é—¨é«˜è–ªå‘˜å·¥:\")\nprint(it_high_salary[['Name', 'Salary']])\n\n# æ•°æ®æ’åº\nsorted_by_salary = df.sort_values('Salary', ascending=False)\nprint(f\"\\næŒ‰è–ªèµ„é™åºæ’åˆ—:\")\nprint(sorted_by_salary[['Name', 'Salary', 'Department']])\n```\n\n## ğŸ§¹ æ•°æ®æ¸…æ´—æ¨¡æ¿\n\n### åŸºç¡€æ•°æ®æ¸…æ´—\n```python\nimport pandas as pd\nimport numpy as np\n\ndef basic_data_cleaning(df):\n    \"\"\"åŸºç¡€æ•°æ®æ¸…æ´—æµç¨‹\"\"\"\n    \n    print(\"=== æ•°æ®æ¸…æ´—æµç¨‹ ===\")\n    df_clean = df.copy()\n    \n    # 1. æ£€æŸ¥æ•°æ®è´¨é‡\n    print(f\"åŸå§‹æ•°æ®å½¢çŠ¶: {df_clean.shape}\")\n    print(f\"ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n    print(df_clean.isnull().sum())\n    print(f\"é‡å¤è¡Œæ•°: {df_clean.duplicated().sum()}\")\n    \n    # 2. å¤„ç†ç¼ºå¤±å€¼\n    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n    \n    # æ•°å€¼åˆ—ç”¨ä¸­ä½æ•°å¡«å……\n    for col in numeric_cols:\n        if df_clean[col].isnull().any():\n            median_val = df_clean[col].median()\n            df_clean[col].fillna(median_val, inplace=True)\n            print(f\"åˆ— '{col}' ç”¨ä¸­ä½æ•° {median_val} å¡«å……ç¼ºå¤±å€¼\")\n    \n    # åˆ†ç±»åˆ—ç”¨ä¼—æ•°å¡«å……\n    for col in categorical_cols:\n        if df_clean[col].isnull().any():\n            mode_val = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n            df_clean[col].fillna(mode_val, inplace=True)\n            print(f\"åˆ— '{col}' ç”¨ä¼—æ•° '{mode_val}' å¡«å……ç¼ºå¤±å€¼\")\n    \n    # 3. åˆ é™¤é‡å¤è¡Œ\n    before_dedup = len(df_clean)\n    df_clean = df_clean.drop_duplicates()\n    after_dedup = len(df_clean)\n    print(f\"åˆ é™¤é‡å¤è¡Œ: {before_dedup - after_dedup} è¡Œ\")\n    \n    print(f\"\\næ¸…æ´—åæ•°æ®å½¢çŠ¶: {df_clean.shape}\")\n    return df_clean\n\n# ä½¿ç”¨ç¤ºä¾‹\n# df_with_issues = pd.DataFrame({\n#     'A': [1, 2, np.nan, 4, 4],\n#     'B': ['x', 'y', np.nan, 'x', 'z']\n# })\n# cleaned_df = basic_data_cleaning(df_with_issues)\n```\n\n### å¼‚å¸¸å€¼å¤„ç†\n```python\nimport pandas as pd\nimport numpy as np\n\ndef handle_outliers(df):\n    \"\"\"å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†\"\"\"\n    \n    print(\"=== å¼‚å¸¸å€¼å¤„ç† ===\")\n    df_clean = df.copy()\n    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n    \n    outliers_info = {}\n    \n    for col in numeric_cols:\n        Q1 = df_clean[col].quantile(0.25)\n        Q3 = df_clean[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        # æ£€æµ‹å¼‚å¸¸å€¼\n        outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)]\n        outlier_count = len(outliers)\n        \n        if outlier_count > 0:\n            print(f\"åˆ— '{col}' å‘ç° {outlier_count} ä¸ªå¼‚å¸¸å€¼\")\n            print(f\"  èŒƒå›´: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n            print(f\"  å¼‚å¸¸å€¼: {outliers[col].tolist()}\")\n            \n            # ç”¨è¾¹ç•Œå€¼æ›¿æ¢å¼‚å¸¸å€¼ï¼ˆå¯é€‰ï¼‰\n            df_clean[col] = np.where(df_clean[col] < lower_bound, lower_bound, df_clean[col])\n            df_clean[col] = np.where(df_clean[col] > upper_bound, upper_bound, df_clean[col])\n    \n    return df_clean\n\n# ä½¿ç”¨ç¤ºä¾‹\n# df_with_outliers = pd.DataFrame({'Values': [1, 2, 3, 100, 2, 3, 1, -50]})\n# cleaned_df = handle_outliers(df_with_outliers)\n```\n\n## ğŸ“Š æ•°æ®åˆ†æä¸ç»Ÿè®¡\n\n### åˆ†ç»„ç»Ÿè®¡\n```python\nimport pandas as pd\n\n# å‡è®¾dfæ˜¯å·²æœ‰çš„DataFrame\nprint(\"=== åˆ†ç»„ç»Ÿè®¡åˆ†æ ===\")\n\n# åŸºç¡€åˆ†ç»„ç»Ÿè®¡\ndept_stats = df.groupby('Department').agg({\n    'Age': ['mean', 'min', 'max', 'count'],\n    'Salary': ['mean', 'sum', 'std']\n}).round(2)\n\nprint(\"å„éƒ¨é—¨ç»Ÿè®¡:\")\nprint(dept_stats)\n\n# æ›´è¯¦ç»†çš„åˆ†ç»„åˆ†æ\nprint(\"\\nå„éƒ¨é—¨è¯¦ç»†åˆ†æ:\")\nfor dept, group in df.groupby('Department'):\n    print(f\"\\n{dept}éƒ¨é—¨:\")\n    print(f\"  å‘˜å·¥æ•°: {len(group)}\")\n    print(f\"  å¹³å‡å¹´é¾„: {group['Age'].mean():.1f}\")\n    print(f\"  å¹³å‡è–ªèµ„: {group['Salary'].mean():.0f}\")\n    print(f\"  æ€»è–ªèµ„: {group['Salary'].sum():.0f}\")\n```\n\n### æ•°æ®é€è§†è¡¨\n```python\nimport pandas as pd\n\nprint(\"=== æ•°æ®é€è§†è¡¨ ===\")\n\n# åˆ›å»ºæ›´ä¸°å¯Œçš„æ•°æ®ç”¨äºæ¼”ç¤º\nsales_data = pd.DataFrame({\n    'Region': ['North', 'South', 'East', 'West'] * 6,\n    'Product': ['A', 'B'] * 12,\n    'Quarter': ['Q1', 'Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2', 'Q2', 'Q3', 'Q3', 'Q3', 'Q3'] * 2,\n    'Sales': np.random.randint(1000, 5000, 24),\n    'Profit': np.random.randint(100, 1000, 24)\n})\n\n# åŸºç¡€æ•°æ®é€è§†è¡¨\npivot1 = pd.pivot_table(sales_data, \n                       values='Sales', \n                       index='Region', \n                       columns='Quarter', \n                       aggfunc='sum')\n\nprint(\"å„åœ°åŒºå„å­£åº¦é”€å”®æ€»é¢:\")\nprint(pivot1)\n\n# å¤šæŒ‡æ ‡æ•°æ®é€è§†è¡¨\npivot2 = pd.pivot_table(sales_data,\n                       values=['Sales', 'Profit'],\n                       index=['Region', 'Product'],\n                       columns='Quarter',\n                       aggfunc={'Sales': 'sum', 'Profit': 'mean'})\n\nprint(\"\\nå„åœ°åŒºäº§å“è¯¦ç»†åˆ†æ:\")\nprint(pivot2)\n```\n\n## ğŸ“ˆ æ•°æ®å¯è§†åŒ–\n\n### åŸºç¡€å›¾è¡¨\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(\"=== æ•°æ®å¯è§†åŒ– ===\")\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\nsales_data = pd.DataFrame({\n    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],\n    'Sales': [120, 150, 130, 170, 160, 190],\n    'Profit': [40, 50, 45, 60, 55, 70]\n})\n\n# æŠ˜çº¿å›¾\nplt.figure(figsize=(10, 6))\nplt.plot(sales_data['Month'], sales_data['Sales'], marker='o', label='Sales', linewidth=2)\nplt.plot(sales_data['Month'], sales_data['Profit'], marker='s', label='Profit', linewidth=2)\nplt.title('æœˆåº¦é”€å”®ä¸åˆ©æ¶¦è¶‹åŠ¿')\nplt.xlabel('æœˆä»½')\nplt.ylabel('é‡‘é¢ (åƒå…ƒ)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# æ¡å½¢å›¾\nplt.figure(figsize=(10, 6))\nplt.bar(sales_data['Month'], sales_data['Sales'], alpha=0.7, label='Sales')\nplt.title('æœˆåº¦é”€å”®é¢')\nplt.xlabel('æœˆä»½')\nplt.ylabel('é”€å”®é¢ (åƒå…ƒ)')\nplt.tight_layout()\nplt.show()\n```\n\n### é«˜çº§å¯è§†åŒ–\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# åˆ›å»ºç›¸å…³æ•°æ®ç¤ºä¾‹\ndata = pd.DataFrame({\n    'Feature1': np.random.normal(0, 1, 100),\n    'Feature2': np.random.normal(0, 1, 100),\n    'Feature3': np.random.normal(0, 1, 100),\n    'Target': np.random.normal(0, 1, 100)\n})\n\n# ç›¸å…³æ€§çƒ­åŠ›å›¾\nplt.figure(figsize=(8, 6))\ncorrelation_matrix = data.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')\nplt.tight_layout()\nplt.show()\n\n# åˆ†å¸ƒç›´æ–¹å›¾\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\ndata['Feature1'].hist(bins=15, alpha=0.7, edgecolor='black')\nplt.title('Feature1 åˆ†å¸ƒ')\n\nplt.subplot(1, 3, 2)\ndata['Feature2'].hist(bins=15, alpha=0.7, edgecolor='black')\nplt.title('Feature2 åˆ†å¸ƒ')\n\nplt.subplot(1, 3, 3)\ndata['Feature3'].hist(bins=15, alpha=0.7, edgecolor='black')\nplt.title('Feature3 åˆ†å¸ƒ')\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸš€ é«˜çº§æ•°æ®å¤„ç†\n\n### æ—¶é—´åºåˆ—åˆ†æ\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== æ—¶é—´åºåˆ—åˆ†æ ===\")\n\n# åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®\ndates = pd.date_range('2024-01-01', periods=100, freq='D')\ntime_series = pd.DataFrame({\n    'date': dates,\n    'value': np.random.randn(100).cumsum() + 100,\n    'volume': np.random.randint(100, 1000, 100)\n})\n\n# è®¾ç½®æ—¶é—´ç´¢å¼•\ntime_series.set_index('date', inplace=True)\n\nprint(\"æ—¶é—´åºåˆ—åŸºæœ¬ä¿¡æ¯:\")\nprint(f\"æ—¶é—´èŒƒå›´: {time_series.index.min()} åˆ° {time_series.index.max()}\")\nprint(f\"æ•°æ®ç‚¹æ•°: {len(time_series)}\")\n\n# é‡é‡‡æ ·ï¼ˆæ—¥æ•°æ®è½¬ä¸ºå‘¨æ•°æ®ï¼‰\nweekly_data = time_series.resample('W').agg({'value': 'mean', 'volume': 'sum'})\nprint(\"\\nå‘¨åº¦èšåˆæ•°æ®:\")\nprint(weekly_data.head())\n\n# ç§»åŠ¨å¹³å‡\ntime_series['7_day_ma'] = time_series['value'].rolling(window=7).mean()\n\n# å¯è§†åŒ–æ—¶é—´åºåˆ—\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 1, 1)\nplt.plot(time_series.index, time_series['value'], label='åŸå§‹å€¼', alpha=0.7)\nplt.plot(time_series.index, time_series['7_day_ma'], label='7æ—¥ç§»åŠ¨å¹³å‡', linewidth=2)\nplt.title('æ—¶é—´åºåˆ—ä¸ç§»åŠ¨å¹³å‡')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(2, 1, 2)\nplt.bar(weekly_data.index, weekly_data['volume'], alpha=0.7)\nplt.title('å‘¨åº¦äº¤æ˜“é‡')\nplt.tight_layout()\nplt.show()\n```\n\n### æ•°æ®åˆå¹¶ä¸è¿æ¥\n```python\nimport pandas as pd\n\nprint(\"=== æ•°æ®åˆå¹¶æ“ä½œ ===\")\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\ndf1 = pd.DataFrame({\n    'ID': [1, 2, 3, 4],\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Dept': ['IT', 'HR', 'IT', 'Finance']\n})\n\ndf2 = pd.DataFrame({\n    'ID': [1, 2, 5, 6],\n    'Salary': [50000, 60000, 70000, 55000],\n    'Join_Date': ['2020-01-01', '2019-03-15', '2021-06-01', '2018-11-20']\n})\n\nprint(\"æ•°æ®è¡¨1:\")\nprint(df1)\nprint(\"\\næ•°æ®è¡¨2:\")\nprint(df2)\n\n# å†…è¿æ¥\ninner_join = pd.merge(df1, df2, on='ID', how='inner')\nprint(f\"\\nå†…è¿æ¥ç»“æœ (å…±{len(inner_join)}è¡Œ):\")\nprint(inner_join)\n\n# å·¦è¿æ¥\nleft_join = pd.merge(df1, df2, on='ID', how='left')\nprint(f\"\\nå·¦è¿æ¥ç»“æœ (å…±{len(left_join)}è¡Œ):\")\nprint(left_join)\n\n# å¤–è¿æ¥\nouter_join = pd.merge(df1, df2, on='ID', how='outer')\nprint(f\"\\nå¤–è¿æ¥ç»“æœ (å…±{len(outer_join)}è¡Œ):\")\nprint(outer_join)\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n- æ­£å¸¸å¯¼å…¥ï¼š`import pandas as pd`\n- ä½¿ç”¨æ ‡å‡†çš„ Pandas å‡½æ•°å’Œæ–¹æ³•\n- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœ\n- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\n- ä¸è¦ä½¿ç”¨ `base64` ç¼–ç å›¾åƒ\n- ä¸è¦åˆ›å»ºå¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    import pandas as pd\n    # æ•°æ®å¤„ç†ä»£ç \n    result = df.groupby('Department')['Salary'].mean()\n    print(f\"å„éƒ¨é—¨å¹³å‡è–ªèµ„: {result}\")\nexcept ImportError:\n    print(\"Pandas ä¸å¯ç”¨\")\nexcept Exception as e:\n    print(f\"æ•°æ®å¤„ç†é”™è¯¯: {e}\")\n```\n\n### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š\n```python\n# å¿«é€ŸæŸ¥çœ‹æ•°æ®åˆ†å¸ƒ\ndef quick_analysis(df):\n    print(\"æ•°æ®å¿«é€Ÿåˆ†æ:\")\n    print(f\"å½¢çŠ¶: {df.shape}\")\n    print(f\"å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n    print(\"\\næ•°å€¼åˆ—ç»Ÿè®¡:\")\n    print(df.describe())\n    print(\"\\nç¼ºå¤±å€¼ç»Ÿè®¡:\")\n    print(df.isnull().sum())\n\n# ä½¿ç”¨ç¤ºä¾‹\n# quick_analysis(your_dataframe)\n```\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºæ•°æ®å¤„ç†é€»è¾‘ï¼\n",
        "report_generator_workflow.md": "# è‡ªåŠ¨åŒ–æŠ¥å‘Šç”ŸæˆæŒ‡å— (v2.2 - å¯ä¸‹è½½ç‰ˆ)\r\n\r\n## ğŸš€ æ ¸å¿ƒè¾“å‡ºåè®® (å¼ºåˆ¶éµå¾ª)\r\n\r\n**é‡è¦æç¤º**: è¦ç”Ÿæˆä¸€ä¸ªå¯ä¾›ç”¨æˆ·ä¸‹è½½çš„æ–‡ä»¶ï¼ˆWord, Excel, PDFç­‰ï¼‰ï¼Œä½ çš„Pythonä»£ç **å¿…é¡»**å°†æ–‡ä»¶å†…å®¹è¿›è¡ŒBase64ç¼–ç ï¼Œå¹¶å°†å…¶åŒ…è£¹åœ¨ä¸€ä¸ªç‰¹å®šæ ¼å¼çš„JSONå¯¹è±¡ä¸­ï¼Œç„¶å `print` è¿™ä¸ªJSONå¯¹è±¡ã€‚\r\n\r\n**å·¥ä½œæµ**:\r\n1.  **å¯¼å…¥å¿…è¦åº“**: `io`, `base64`, `json`ã€‚\r\n2.  **åœ¨å†…å­˜ä¸­åˆ›å»ºæ–‡ä»¶**: ä½¿ç”¨ `io.BytesIO()` åˆ›å»ºä¸€ä¸ªå†…å­˜ç¼“å†²åŒºã€‚\r\n3.  **ä¿å­˜åˆ°å†…å­˜**: è°ƒç”¨ç›¸åº”åº“çš„ `.save(buffer)` æ–¹æ³•å°†æ–‡ä»¶å†…å®¹å†™å…¥å†…å­˜ç¼“å†²åŒºã€‚\r\n4.  **ç¼–ç **: å°†ç¼“å†²åŒºä¸­çš„äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸ºBase64å­—ç¬¦ä¸²ã€‚\r\n5.  **æ‰“åŒ…å¹¶æ‰“å°**: æ„å»ºä¸€ä¸ªåŒ…å« `type` å’Œ `data_base64` å­—æ®µçš„å­—å…¸ï¼Œå¹¶ä½¿ç”¨ `json.dumps()` æ‰“å°å‡ºæ¥ã€‚\r\n\r\n---\r\n\r\n## ğŸ“Š Word æŠ¥å‘Šç”Ÿæˆ (.docx)\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nfrom docx import Document\r\nfrom docx.shared import Inches\r\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\r\nfrom datetime import datetime\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º Word æ–‡æ¡£ ---\r\ndoc = Document()\r\ndoc.add_heading('ä¸šåŠ¡åˆ†ææŠ¥å‘Š', 0)\r\ndoc.add_paragraph(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime(\"%Y-%m-%d\")}')\r\ndoc.add_paragraph('è¿™æ˜¯ä¸€ä¸ªç”±ä»£ç è§£é‡Šå™¨ç”Ÿæˆçš„Wordæ–‡æ¡£ç¤ºä¾‹ã€‚')\r\n# ... (æ·»åŠ æ›´å¤šå†…å®¹, å¦‚è¡¨æ ¼ã€å›¾ç‰‡ç­‰) ...\r\n\r\n# --- 2. ä¿å­˜åˆ°å†…å­˜ç¼“å†²åŒº ---\r\nbuffer = io.BytesIO()\r\ndoc.save(buffer)\r\nbuffer.seek(0) # é‡ç½®æŒ‡é’ˆåˆ°å¼€å¤´\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"word\",\r\n    \"title\": f\"ä¸šåŠ¡åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.docx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\nğŸ“ˆ Excel æŠ¥å‘Šç”Ÿæˆ (.xlsx)\r\nâœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\ncode\r\nPython\r\nimport io\r\nimport base64\r\nimport json\r\nimport pandas as pd\r\nfrom datetime import datetime\r\n\r\n# --- 1. åˆ›å»º DataFrame å¹¶å‡†å¤‡ Excel å†…å®¹ ---\r\ndata = {'Department': ['Sales', 'R&D'], 'Budget':, 'Actual_Spend':}\r\ndf = pd.DataFrame(data)\r\n\r\n# --- 2. ä½¿ç”¨ ExcelWriter å°† DataFrame å†™å…¥å†…å­˜ç¼“å†²åŒº ---\r\noutput_buffer = io.BytesIO()\r\nwith pd.ExcelWriter(output_buffer, engine='openpyxl') as writer:\r\n    df.to_excel(writer, sheet_name='Budget Report', index=False)\r\n    # ä½ å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ writer.book å’Œ writer.sheets[sheet_name] æ·»åŠ æ›´å¤æ‚çš„æ ¼å¼\r\noutput_buffer.seek(0)\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(output_buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"excel\",\r\n    \"title\": f\"éƒ¨é—¨é¢„ç®—æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.xlsx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\nğŸ“„ PDF æŠ¥å‘Šç”Ÿæˆ (.pdf)\r\nâœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\ncode\r\nPython\r\nimport io\r\nimport base64\r\nimport json\r\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph\r\nfrom reportlab.lib.styles import getSampleStyleSheet\r\nfrom datetime import datetime\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º PDF æ–‡æ¡£ ---\r\nbuffer = io.BytesIO()\r\ndoc = SimpleDocTemplate(buffer)\r\nstyles = getSampleStyleSheet()\r\nstory = [\r\n    Paragraph(\"PDF æŠ¥å‘Šæ ‡é¢˜\", styles['Title']),\r\n    Paragraph(f\"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d')}\", styles['Normal']),\r\n    Paragraph(\"è¿™æ˜¯ä¸€ä¸ªç”±ä»£ç è§£é‡Šå™¨ç”Ÿæˆçš„PDFæ–‡æ¡£ç¤ºä¾‹ã€‚\", styles['BodyText'])\r\n]\r\ndoc.build(story)\r\nbuffer.seek(0)\r\n\r\n# --- 2. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"pdf\",\r\n    \"title\": f\"ç¤ºä¾‹PDFæŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.pdf\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 3. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\nâš ï¸ é‡è¦æ³¨æ„äº‹é¡¹\r\nâœ… å¿…é¡»åšçš„:\r\næ‰€æœ‰æ–‡ä»¶ç”Ÿæˆéƒ½å¿…é¡»éµå¾ªâ€œä¿å­˜åˆ°å†…å­˜ -> Base64ç¼–ç  -> æ‰“å°JSONâ€çš„æµç¨‹ã€‚\r\næœ€ç»ˆçš„ print è¯­å¥åªèƒ½è¾“å‡ºä¸€ä¸ªæ ‡å‡†çš„JSONå¯¹è±¡ã€‚\r\nâŒ ç»å¯¹ç¦æ­¢:\r\nç¦æ­¢è°ƒç”¨ doc.save('filename.docx') æˆ– wb.save('filename.xlsx') å°†æ–‡ä»¶ä¿å­˜åˆ°æœ¬åœ°è·¯å¾„ã€‚\r\nç¦æ­¢åœ¨æ‰“å°æœ€ç»ˆçš„JSONå¯¹è±¡ä¹‹åï¼Œå† print ä»»ä½•å…¶ä»–å†…å®¹ï¼ˆå¦‚ \"æ–‡ä»¶å·²ç”Ÿæˆ\"ï¼‰ã€‚",
        "scipy_cookbook.md": "# SciPy ç§‘å­¦è®¡ç®—æŒ‡å— (v2.2)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**ç¯å¢ƒç‰¹æ€§**ï¼šåŸºäº SciPy çš„ç§‘å­¦è®¡ç®—ç¯å¢ƒï¼Œæ”¯æŒä¼˜åŒ–ã€ç§¯åˆ†ã€ä¿¡å·å¤„ç†ç­‰\n**è¾“å‡ºåŸåˆ™**ï¼šç³»ç»Ÿè‡ªåŠ¨å¤„ç†ç»“æœè¾“å‡ºï¼Œæ— éœ€æ‰‹åŠ¨ç¼–ç \n\n## ğŸ”§ æ ¸å¿ƒæ¨¡å—æ¦‚è§ˆ\n\n### ä¸»è¦åŠŸèƒ½æ¨¡å—ï¼š\n- **ä¼˜åŒ–ç®—æ³•** (`scipy.optimize`) - å‡½æ•°æœ€å°åŒ–ã€æ–¹ç¨‹æ±‚è§£\n- **ç§¯åˆ†è®¡ç®—** (`scipy.integrate`) - æ•°å€¼ç§¯åˆ†ã€å¾®åˆ†æ–¹ç¨‹\n- **ä¿¡å·å¤„ç†** (`scipy.signal`) - æ»¤æ³¢å™¨ã€é¢‘è°±åˆ†æ\n- **çº¿æ€§ä»£æ•°** (`scipy.linalg`) - çŸ©é˜µè¿ç®—ã€çº¿æ€§ç³»ç»Ÿ\n- **ç»Ÿè®¡å‡½æ•°** (`scipy.stats`) - æ¦‚ç‡åˆ†å¸ƒã€ç»Ÿè®¡æ£€éªŒ\n- **ç©ºé—´ç®—æ³•** (`scipy.spatial`) - ç©ºé—´æ•°æ®ã€è·ç¦»è®¡ç®—\n\n## ğŸ¯ ä¼˜åŒ–ä¸æ–¹ç¨‹æ±‚è§£\n\n### å‡½æ•°æœ€å°åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# 1. å•å˜é‡å‡½æ•°ä¼˜åŒ–\ndef single_variable_func(x):\n    return (x - 3)**2 * np.sin(x) + x**2\n\nresult = optimize.minimize_scalar(single_variable_func, bounds=(0, 10), method='bounded')\nprint(f\"æœ€ä¼˜è§£: x = {result.x:.4f}, å‡½æ•°å€¼: {result.fun:.4f}\")\n\n# å¯è§†åŒ–\nx_plot = np.linspace(0, 10, 100)\ny_plot = single_variable_func(x_plot)\nplt.figure(figsize=(10, 6))\nplt.plot(x_plot, y_plot, label='f(x)')\nplt.axvline(result.x, color='red', linestyle='--', label=f'æœ€ä¼˜è§£ x={result.x:.3f}')\nplt.title('å•å˜é‡å‡½æ•°ä¼˜åŒ–')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n### å¤šå˜é‡ä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# Rosenbrock å‡½æ•°ä¼˜åŒ–\ndef rosenbrock(x):\n    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n\nx0 = np.array([-1.2, 1.0])\nresult = optimize.minimize(rosenbrock, x0, method='BFGS')\n\nprint(f\"åˆå§‹ç‚¹: {x0}\")\nprint(f\"æœ€ä¼˜ç‚¹: {result.x}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.6f}\")\nprint(f\"è¿­ä»£æ¬¡æ•°: {result.nit}\")\n\n# å¯è§†åŒ–\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-1, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        Z[i,j] = rosenbrock([X[i,j], Y[i,j]])\n\nplt.figure(figsize=(10, 8))\ncontour = plt.contour(X, Y, Z, levels=50)\nplt.clabel(contour, inline=True, fontsize=8)\nplt.plot(result.x[0], result.x[1], 'ro', markersize=10, label='æœ€ä¼˜è§£')\nplt.title('Rosenbrock å‡½æ•°ä¼˜åŒ–')\nplt.legend()\nplt.show()\n```\n\n### çº¦æŸä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜\ndef objective(x):\n    return x[0]**2 + x[1]**2\n\ndef constraint1(x):\n    return x[0] + x[1] - 1  # x + y >= 1\n\nconstraints = [{'type': 'ineq', 'fun': constraint1}]\nbounds = [(0, None), (0, None)]\n\nresult = optimize.minimize(objective, [0.5, 0.5], \n                         method='SLSQP', bounds=bounds, \n                         constraints=constraints)\n\nprint(f\"çº¦æŸä¼˜åŒ–ç»“æœ:\")\nprint(f\"æœ€ä¼˜ç‚¹: {result.x}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.4f}\")\nprint(f\"çº¦æŸæ»¡è¶³: {result.success}\")\n\n# å¯è§†åŒ–çº¦æŸåŒºåŸŸ\nx_const = np.linspace(0, 2, 100)\ny_const = np.linspace(0, 2, 100)\nX, Y = np.meshgrid(x_const, y_const)\nZ = objective([X, Y])\n\nplt.figure(figsize=(10, 8))\nplt.contourf(X, Y, Z, levels=20, alpha=0.6)\nplt.contour(X, Y, Z, levels=10, colors='black', alpha=0.4)\n\n# ç»˜åˆ¶çº¦æŸæ¡ä»¶\ny_constraint = 1 - x_const\nplt.plot(x_const, y_constraint, 'r-', linewidth=2, label='x + y = 1')\nplt.fill_between(x_const, y_constraint, 2, alpha=0.3, color='gray', label='å¯è¡ŒåŸŸ')\n\nplt.plot(result.x[0], result.x[1], 'go', markersize=10, label='æœ€ä¼˜è§£')\nplt.xlim(0, 2)\nplt.ylim(0, 2)\nplt.title('çº¦æŸä¼˜åŒ–é—®é¢˜')\nplt.legend()\nplt.show()\n```\n\n## ğŸ“ æ•°å€¼ç§¯åˆ†\n\n### å®šç§¯åˆ†è®¡ç®—\n```python\nfrom scipy import integrate\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. å•å˜é‡ç§¯åˆ†\ndef func1(x):\n    return np.exp(-x**2) * np.sin(x)\n\nintegral1, error1 = integrate.quad(func1, 0, np.inf)\n\nprint(f\"ç§¯åˆ†ç»“æœ: {integral1:.6f}\")\nprint(f\"ä¼°è®¡è¯¯å·®: {error1:.2e}\")\n\n# å¯è§†åŒ–è¢«ç§¯å‡½æ•°\nx_plot = np.linspace(0, 3, 100)\ny_plot = func1(x_plot)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_plot, y_plot, 'b-', linewidth=2, label='è¢«ç§¯å‡½æ•°')\nplt.fill_between(x_plot, y_plot, alpha=0.3)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title(f'å®šç§¯åˆ†: âˆ«e^(-xÂ²)sin(x)dx = {integral1:.4f}')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n### å¾®åˆ†æ–¹ç¨‹æ±‚è§£\n```python\nfrom scipy import integrate\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Lotka-Volterra æ•é£Ÿè€…-è¢«æ•é£Ÿè€…æ¨¡å‹\ndef ode_system(t, y):\n    alpha, beta, delta, gamma = 1.0, 0.1, 0.075, 1.5\n    prey, predator = y\n    dprey_dt = alpha * prey - beta * prey * predator\n    dpredator_dt = delta * prey * predator - gamma * predator\n    return [dprey_dt, dpredator_dt]\n\n# æ±‚è§£å¾®åˆ†æ–¹ç¨‹\nt_span = (0, 50)\ny0 = [10, 5]  # åˆå§‹ç§ç¾¤\nt_eval = np.linspace(0, 50, 1000)\nsolution = integrate.solve_ivp(ode_system, t_span, y0, t_eval=t_eval, method='RK45')\n\nprint(f\"æ±‚è§£æˆåŠŸ: {solution.success}\")\nprint(f\"æœ€ç»ˆè¢«æ•é£Ÿè€…æ•°é‡: {solution.y[0, -1]:.2f}\")\nprint(f\"æœ€ç»ˆæ•é£Ÿè€…æ•°é‡: {solution.y[1, -1]:.2f}\")\n\n# å¯è§†åŒ–ç§ç¾¤åŠ¨æ€\nplt.figure(figsize=(12, 5))\nplt.plot(solution.t, solution.y[0], 'g-', label='è¢«æ•é£Ÿè€…', linewidth=2)\nplt.plot(solution.t, solution.y[1], 'r-', label='æ•é£Ÿè€…', linewidth=2)\nplt.xlabel('æ—¶é—´')\nplt.ylabel('ç§ç¾¤æ•°é‡')\nplt.title('Lotka-Volterra æ•é£Ÿè€…-è¢«æ•é£Ÿè€…æ¨¡å‹')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n## ğŸ“¡ ä¿¡å·å¤„ç†\n\n### ä¿¡å·æ»¤æ³¢ä¸é¢‘è°±åˆ†æ\n```python\nfrom scipy import signal\nfrom scipy.fft import fft, fftfreq\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ç”Ÿæˆæµ‹è¯•ä¿¡å·\nt = np.linspace(0, 1, 1000, endpoint=False)\noriginal_signal = (np.sin(2 * np.pi * 5 * t) + \n                  0.5 * np.sin(2 * np.pi * 20 * t) + \n                  0.2 * np.sin(2 * np.pi * 50 * t))\n\n# æ·»åŠ å™ªå£°\nnoisy_signal = original_signal + 0.3 * np.random.normal(size=len(t))\n\n# è®¾è®¡ä½é€šæ»¤æ³¢å™¨\nnyquist = 500  # é‡‡æ ·é¢‘ç‡1000Hzï¼Œå¥ˆå¥æ–¯ç‰¹é¢‘ç‡500Hz\ncutoff = 15 / nyquist\nb, a = signal.butter(4, cutoff, btype='low')\nfiltered_signal = signal.filtfilt(b, a, noisy_signal)\n\nprint(\"ä¿¡å·å¤„ç†å®Œæˆ\")\nprint(f\"ä¿¡å·é•¿åº¦: {len(t)}\")\nprint(f\"é‡‡æ ·é¢‘ç‡: 1000 Hz\")\n\n# å¯è§†åŒ–ä¿¡å·\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# æ—¶åŸŸä¿¡å·\nax1.plot(t, original_signal, 'b-', alpha=0.7, label='åŸå§‹ä¿¡å·')\nax1.plot(t, noisy_signal, 'r-', alpha=0.5, label='å¸¦å™ªå£°ä¿¡å·')\nax1.plot(t, filtered_signal, 'g-', linewidth=2, label='æ»¤æ³¢åä¿¡å·')\nax1.set_xlabel('æ—¶é—´ (s)')\nax1.set_ylabel('å¹…åº¦')\nax1.set_title('æ—¶åŸŸä¿¡å·')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# é¢‘åŸŸåˆ†æ\nfft_original = fft(original_signal)\nfft_noisy = fft(noisy_signal)\nfft_filtered = fft(filtered_signal)\nfreqs = fftfreq(len(t), t[1] - t[0])\npositive_freq_idx = np.where(freqs > 0)\n\nax2.plot(freqs[positive_freq_idx], np.abs(fft_original[positive_freq_idx]), 'b-', label='åŸå§‹é¢‘è°±')\nax2.plot(freqs[positive_freq_idx], np.abs(fft_noisy[positive_freq_idx]), 'r-', alpha=0.5, label='å™ªå£°é¢‘è°±')\nax2.plot(freqs[positive_freq_idx], np.abs(fft_filtered[positive_freq_idx]), 'g-', label='æ»¤æ³¢é¢‘è°±')\nax2.set_xlabel('é¢‘ç‡ (Hz)')\nax2.set_ylabel('å¹…åº¦')\nax2.set_title('é¢‘åŸŸåˆ†æ')\nax2.legend()\nax2.grid(True, alpha=0.3)\nax2.set_xlim(0, 100)\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ§® çº¿æ€§ä»£æ•°\n\n### çŸ©é˜µè¿ç®—ä¸åˆ†è§£\n```python\nfrom scipy import linalg\nimport numpy as np\n\n# çŸ©é˜µè¿ç®—ç¤ºä¾‹\nA = np.array([[4, 2, 1], \n              [2, 5, 3], \n              [1, 3, 6]])\nb = np.array([1, 2, 3])\n\nprint(\"çŸ©é˜µ A:\")\nprint(A)\nprint(f\"\\nå‘é‡ b: {b}\")\n\n# çŸ©é˜µæ€§è´¨\ndet_A = linalg.det(A)\ncond_A = linalg.cond(A)\nprint(f\"\\nè¡Œåˆ—å¼: {det_A:.2f}\")\nprint(f\"æ¡ä»¶æ•°: {cond_A:.2f}\")\n\n# çº¿æ€§æ–¹ç¨‹ç»„æ±‚è§£\nx = linalg.solve(A, b)\nprint(f\"\\næ–¹ç¨‹è§£: {x}\")\n\n# éªŒè¯è§£\nprint(f\"éªŒè¯: A*x = {A.dot(x)}\")\nprint(f\"ç›®æ ‡: b = {b}\")\n\n# ç‰¹å¾å€¼åˆ†è§£\neigenvalues, eigenvectors = linalg.eig(A)\nprint(f\"\\nç‰¹å¾å€¼: {eigenvalues}\")\nprint(\"ç‰¹å¾å‘é‡:\")\nprint(eigenvectors)\n```\n\n### ç©ºé—´ç®—æ³•\n```python\nfrom scipy import spatial\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ç©ºé—´ç‚¹é›†\npoints = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [0, 3], [1, 2]])\nprint(f\"ç©ºé—´ç‚¹é›†: {points}\")\n\n# è®¡ç®—å‡¸åŒ…\nhull = spatial.ConvexHull(points)\nprint(f\"\\nå‡¸åŒ…é¡¶ç‚¹ç´¢å¼•: {hull.vertices}\")\nprint(f\"å‡¸åŒ…ä½“ç§¯: {hull.volume:.2f}\")\nprint(f\"å‡¸åŒ…é¢ç§¯: {hull.area:.2f}\")\n\n# æœ€è¿‘é‚»æœç´¢\ntree = spatial.KDTree(points)\ndistances, indices = tree.query(points, k=2)  # æ¯ä¸ªç‚¹æ‰¾2ä¸ªæœ€è¿‘é‚»\nprint(f\"\\næœ€è¿‘é‚»è·ç¦»: {distances}\")\nprint(f\"æœ€è¿‘é‚»ç´¢å¼•: {indices}\")\n\n# å¯è§†åŒ–ç©ºé—´ç‚¹ä¸å‡¸åŒ…\nplt.figure(figsize=(10, 8))\nplt.scatter(points[:,0], points[:,1], c='red', s=100, label='æ•°æ®ç‚¹', zorder=5)\n\n# ç»˜åˆ¶å‡¸åŒ…\nfor simplex in hull.simplices:\n    plt.plot(points[simplex, 0], points[simplex, 1], 'b-', linewidth=2, label='å‡¸åŒ…' if simplex[0]==0 else \"\")\n\nplt.title('ç©ºé—´ç‚¹é›†ä¸å‡¸åŒ…')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.show()\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n- æ­£å¸¸å¯¼å…¥ SciPy æ¨¡å—ï¼š`from scipy import optimize, integrate, linalg`\n- ä½¿ç”¨æ ‡å‡†çš„ SciPy å‡½æ•°æ¥å£\n- é€šè¿‡ `print()` è¾“å‡ºæ•°å€¼ç»“æœ\n- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n- ä¸è¦æ‰‹åŠ¨ä½¿ç”¨ `base64` ç¼–ç \n- ä¸è¦åˆ›å»º `io.BytesIO` å¯¹è±¡\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡ºæ ¼å¼\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    from scipy import optimize\n    result = optimize.minimize_scalar(lambda x: x**2, bounds=(0, 1))\n    print(f\"ä¼˜åŒ–æˆåŠŸ: {result.x}\")\nexcept ImportError:\n    print(\"SciPy ä¼˜åŒ–æ¨¡å—ä¸å¯ç”¨\")\nexcept Exception as e:\n    print(f\"ä¼˜åŒ–å¤±è´¥: {e}\")\n```\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç§‘å­¦è®¡ç®—é€»è¾‘ï¼\n",
        "sympy_cookbook.md": "# SymPy ç¬¦å·æ•°å­¦æŒ‡å— (v2.2)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**åŠŸèƒ½**ï¼šç¬¦å·æ•°å­¦è®¡ç®—ï¼ŒåŒ…æ‹¬æ–¹ç¨‹æ±‚è§£ã€å¾®ç§¯åˆ†ã€ä»£æ•°è¿ç®—ç­‰\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼\n\n## ğŸ§® åŸºç¡€ç¬¦å·è¿ç®—\n\n### ç¬¦å·å®šä¹‰ä¸åŸºæœ¬æ“ä½œ\n```python\nimport sympy as sp\n\n# å®šä¹‰ç¬¦å·å˜é‡\nx, y, z = sp.symbols('x y z')\na, b, c = sp.symbols('a b c')\n\n# åŸºæœ¬è¡¨è¾¾å¼æ“ä½œ\nexpr1 = x**2 + 2*x + 1\nexpr2 = (x + 1)**2\n\nprint(\"=== åŸºç¡€ç¬¦å·è¿ç®— ===\")\nprint(f\"è¡¨è¾¾å¼1: {expr1}\")\nprint(f\"è¡¨è¾¾å¼2: {expr2}\")\nprint(f\"è¡¨è¾¾å¼1å±•å¼€: {sp.expand(expr1)}\")\nprint(f\"è¡¨è¾¾å¼2å› å¼åˆ†è§£: {sp.factor(expr2)}\")\nprint(f\"ä¸¤ä¸ªè¡¨è¾¾å¼æ˜¯å¦ç›¸ç­‰: {expr1.equals(expr2)}\")\n\n# è¡¨è¾¾å¼ç®€åŒ–\ncomplex_expr = (x**2 - 1)/(x - 1)\nsimplified = sp.simplify(complex_expr)\nprint(f\"å¤æ‚è¡¨è¾¾å¼: {complex_expr}\")\nprint(f\"ç®€åŒ–å: {simplified}\")\n```\n\n## ğŸ¯ æ–¹ç¨‹æ±‚è§£\n\n### ä»£æ•°æ–¹ç¨‹æ±‚è§£\n```python\nimport sympy as sp\n\nx, y, z = sp.symbols('x y z')\n\nprint(\"=== ä»£æ•°æ–¹ç¨‹æ±‚è§£ ===\")\n\n# ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹\neq1 = sp.Eq(x**2 - 5*x + 6, 0)\nsolutions1 = sp.solve(eq1, x)\nprint(f\"æ–¹ç¨‹: {eq1}\")\nprint(f\"è§£: {solutions1}\")\n\n# çº¿æ€§æ–¹ç¨‹ç»„\neq2 = sp.Eq(2*x + 3*y, 7)\neq3 = sp.Eq(4*x - y, 1)\nsolutions2 = sp.solve([eq2, eq3], (x, y))\nprint(f\"\\næ–¹ç¨‹ç»„:\")\nprint(f\"  {eq2}\")\nprint(f\"  {eq3}\")\nprint(f\"è§£: {solutions2}\")\n\n# éçº¿æ€§æ–¹ç¨‹æ•°å€¼è§£\neq4 = sp.Eq(sp.sin(x) - x/2, 0)\nsolution4 = sp.nsolve(eq4, x, 1)  # ä»x=1å¼€å§‹æ•°å€¼æ±‚è§£\nprint(f\"\\néçº¿æ€§æ–¹ç¨‹: {eq4}\")\nprint(f\"æ•°å€¼è§£: {solution4}\")\n```\n\n## ğŸ“ å¾®ç§¯åˆ†è¿ç®—\n\n### å¾®åˆ†è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== å¾®åˆ†è®¡ç®— ===\")\n\n# å®šä¹‰å‡½æ•°\nf = x**3 + 2*x**2 + sp.sin(x)\nprint(f\"å‡½æ•°: f(x) = {f}\")\n\n# ä¸€é˜¶å¯¼æ•°\nf_prime = sp.diff(f, x)\nprint(f\"ä¸€é˜¶å¯¼æ•°: f'(x) = {f_prime}\")\n\n# äºŒé˜¶å¯¼æ•°\nf_double_prime = sp.diff(f, x, 2)\nprint(f\"äºŒé˜¶å¯¼æ•°: f''(x) = {f_double_prime}\")\n\n# åå¯¼æ•°ï¼ˆå¤šå˜é‡ï¼‰\ny = sp.symbols('y')\ng = x**2 * y + sp.sin(x*y)\ng_x = sp.diff(g, x)\ng_y = sp.diff(g, y)\nprint(f\"\\nå¤šå˜é‡å‡½æ•°: g(x,y) = {g}\")\nprint(f\"å¯¹xåå¯¼: âˆ‚g/âˆ‚x = {g_x}\")\nprint(f\"å¯¹yåå¯¼: âˆ‚g/âˆ‚y = {g_y}\")\n```\n\n### ç§¯åˆ†è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== ç§¯åˆ†è®¡ç®— ===\")\n\n# ä¸å®šç§¯åˆ†\nf = x**2 + sp.sin(x)\nindefinite = sp.integrate(f, x)\nprint(f\"å‡½æ•°: f(x) = {f}\")\nprint(f\"ä¸å®šç§¯åˆ†: âˆ«f(x)dx = {indefinite} + C\")\n\n# å®šç§¯åˆ†\ndefinite = sp.integrate(f, (x, 0, sp.pi))\nprint(f\"å®šç§¯åˆ† [0,Ï€]: âˆ«â‚€^Ï€ f(x)dx = {definite}\")\nprint(f\"æ•°å€¼ç»“æœ: {definite.evalf()}\")\n\n# å¤šé‡ç§¯åˆ†\ny = sp.symbols('y')\ndouble_int = sp.integrate(x*y, (x, 0, 1), (y, 0, 2))\nprint(f\"\\näºŒé‡ç§¯åˆ†: âˆ«â‚€Â¹âˆ«â‚€Â² xy dy dx = {double_int}\")\n```\n\n### æé™è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== æé™è®¡ç®— ===\")\n\n# åŸºæœ¬æé™\nlimit1 = sp.limit(sp.sin(x)/x, x, 0)\nprint(f\"lim(xâ†’0) sin(x)/x = {limit1}\")\n\n# æ— ç©·æé™\nlimit2 = sp.limit(1/x, x, 0, '+')  # ä»æ­£æ–¹å‘é€¼è¿‘\nlimit3 = sp.limit(1/x, x, 0, '-')  # ä»è´Ÿæ–¹å‘é€¼è¿‘\nprint(f\"lim(xâ†’0âº) 1/x = {limit2}\")\nprint(f\"lim(xâ†’0â») 1/x = {limit3}\")\n\n# å¤æ‚æé™\nlimit4 = sp.limit((1 + 1/x)**x, x, sp.oo)\nprint(f\"lim(xâ†’âˆ) (1 + 1/x)Ë£ = {limit4}\")\n```\n\n## ğŸ” æ•°å­¦è¯æ˜ä¸æ’ç­‰å¼\n\n### ä»£æ•°æ’ç­‰å¼éªŒè¯\n```python\nimport sympy as sp\n\na, b, x = sp.symbols('a b x')\n\nprint(\"=== æ•°å­¦æ’ç­‰å¼éªŒè¯ ===\")\n\n# éªŒè¯ (a+b)Â² = aÂ² + 2ab + bÂ²\nlhs1 = (a + b)**2\nrhs1 = a**2 + 2*a*b + b**2\nidentity1 = sp.simplify(lhs1 - rhs1) == 0\nprint(f\"(a+b)Â² = aÂ² + 2ab + bÂ²: {identity1}\")\n\n# éªŒè¯ä¸‰è§’æ’ç­‰å¼ sinÂ²x + cosÂ²x = 1\nlhs2 = sp.sin(x)**2 + sp.cos(x)**2\nrhs2 = 1\nidentity2 = sp.simplify(lhs2 - rhs2) == 0\nprint(f\"sinÂ²x + cosÂ²x = 1: {identity2}\")\n\n# éªŒè¯æ¬§æ‹‰å…¬å¼\ntheta = sp.symbols('theta')\neuler_lhs = sp.exp(sp.I * theta)\neuler_rhs = sp.cos(theta) + sp.I * sp.sin(theta)\neuler_identity = sp.simplify(euler_lhs - euler_rhs) == 0\nprint(f\"e^(iÎ¸) = cosÎ¸ + i sinÎ¸: {euler_identity}\")\n```\n\n## ğŸ§© çº¿æ€§ä»£æ•°\n\n### çŸ©é˜µè¿ç®—\n```python\nimport sympy as sp\n\nprint(\"=== çŸ©é˜µè¿ç®— ===\")\n\n# å®šä¹‰ç¬¦å·çŸ©é˜µ\nA = sp.Matrix([[1, 2], [3, 4]])\nB = sp.Matrix([[2, 0], [1, 2]])\n\nprint(f\"çŸ©é˜µ A:\\n{A}\")\nprint(f\"çŸ©é˜µ B:\\n{B}\")\n\n# åŸºæœ¬è¿ç®—\nprint(f\"\\nçŸ©é˜µåŠ æ³• A+B:\\n{A + B}\")\nprint(f\"çŸ©é˜µä¹˜æ³• AÃ—B:\\n{A * B}\")\nprint(f\"Açš„è¡Œåˆ—å¼: {A.det()}\")\nprint(f\"Açš„é€†çŸ©é˜µ:\\n{A.inv()}\")\n\n# ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡\neigenvals = A.eigenvals()\neigenvects = A.eigenvects()\nprint(f\"\\nAçš„ç‰¹å¾å€¼: {eigenvals}\")\nprint(f\"Açš„ç‰¹å¾å‘é‡: {eigenvects}\")\n\n# è§£çº¿æ€§æ–¹ç¨‹ç»„\nx1, x2 = sp.symbols('x1 x2')\neq1 = sp.Eq(2*x1 + 3*x2, 7)\neq2 = sp.Eq(4*x1 + 5*x2, 13)\nsolution = sp.solve([eq1, eq2], (x1, x2))\nprint(f\"\\næ–¹ç¨‹ç»„:\")\nprint(f\"  {eq1}\")\nprint(f\"  {eq2}\")\nprint(f\"è§£: {solution}\")\n```\n\n## ğŸ“ˆ çº§æ•°å±•å¼€ä¸æ•°å€¼è®¡ç®—\n\n### æ³°å‹’çº§æ•°å±•å¼€\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== çº§æ•°å±•å¼€ ===\")\n\n# å¸¸ç”¨å‡½æ•°çš„æ³°å‹’å±•å¼€\nsin_series = sp.sin(x).series(x, 0, 6)  # åœ¨0å¤„å±•å¼€åˆ°6é˜¶\ncos_series = sp.cos(x).series(x, 0, 6)\nexp_series = sp.exp(x).series(x, 0, 5)\n\nprint(f\"sin(x)çš„æ³°å‹’å±•å¼€: {sin_series}\")\nprint(f\"cos(x)çš„æ³°å‹’å±•å¼€: {cos_series}\")\nprint(f\"e^xçš„æ³°å‹’å±•å¼€: {exp_series}\")\n\n# æ•°å€¼è¿‘ä¼¼\nprint(f\"\\næ•°å€¼è¿‘ä¼¼:\")\nprint(f\"Ï€ â‰ˆ {sp.N(sp.pi, 10)}\")  # 10ä½ç²¾åº¦\nprint(f\"e â‰ˆ {sp.N(sp.E, 8)}\")    # 8ä½ç²¾åº¦\nprint(f\"âˆš2 â‰ˆ {sp.N(sp.sqrt(2), 6)}\")\n\n# ç¬¦å·è¡¨è¾¾å¼çš„æ•°å€¼è®¡ç®—\nexpr = sp.integrate(sp.sin(x), (x, 0, sp.pi/2))\nnumerical_result = sp.N(expr)\nprint(f\"\\nç¬¦å·ç§¯åˆ†: âˆ«â‚€^(Ï€/2) sin(x) dx = {expr}\")\nprint(f\"æ•°å€¼ç»“æœ: {numerical_result}\")\n```\n\n## ğŸ“ å¤æ‚æ•°å­¦é—®é¢˜\n\n### å‡½æ•°åˆ†æä¸æå€¼\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== å‡½æ•°åˆ†æä¸æå€¼ ===\")\n\n# å®šä¹‰å‡½æ•°\nf = x**3 - 6*x**2 + 9*x + 1\nprint(f\"å‡½æ•°: f(x) = {f}\")\n\n# æ±‚å¯¼æ‰¾ä¸´ç•Œç‚¹\nf_prime = sp.diff(f, x)\ncritical_points = sp.solve(f_prime, x)\nprint(f\"ä¸€é˜¶å¯¼æ•°: f'(x) = {f_prime}\")\nprint(f\"ä¸´ç•Œç‚¹: {critical_points}\")\n\n# äºŒé˜¶å¯¼æ•°æµ‹è¯•\nf_double_prime = sp.diff(f, x, 2)\nfor point in critical_points:\n    second_deriv_val = f_double_prime.subs(x, point)\n    if second_deriv_val > 0:\n        extremum_type = \"å±€éƒ¨æå°å€¼\"\n    elif second_deriv_val < 0:\n        extremum_type = \"å±€éƒ¨æå¤§å€¼\"\n    else:\n        extremum_type = \"éœ€è¦è¿›ä¸€æ­¥åˆ†æ\"\n    print(f\"ç‚¹ x = {point}: {extremum_type}\")\n\n# å‡½æ•°å€¼\nfor point in critical_points:\n    func_val = f.subs(x, point)\n    print(f\"f({point}) = {func_val}\")\n```\n\n### æ›²çº¿æ€§è´¨åˆ†æ\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== æ›²çº¿æ€§è´¨åˆ†æ ===\")\n\nf = x**2 * sp.sin(x)\n\n# æ›²çº¿é•¿åº¦ï¼ˆå¼§é•¿ï¼‰\ncurve_length = sp.integrate(sp.sqrt(1 + sp.diff(f, x)**2), (x, 0, sp.pi))\nprint(f\"å‡½æ•°: f(x) = {f}\")\nprint(f\"æ›²çº¿åœ¨ [0,Ï€] ä¸Šçš„é•¿åº¦: {sp.N(curve_length)}\")\n\n# æ—‹è½¬ä½“ä½“ç§¯\nvolume = sp.pi * sp.integrate(f**2, (x, 0, sp.pi))\nprint(f\"æ›²çº¿ç»•xè½´æ—‹è½¬çš„ä½“ç§¯: {sp.N(volume)}\")\n\n# æ›²ç‡\nf_prime = sp.diff(f, x)\nf_double_prime = sp.diff(f, x, 2)\ncurvature = f_double_prime / (1 + f_prime**2)**(3/2)\nprint(f\"æ›²ç‡å…¬å¼: Îº(x) = {curvature}\")\n```\n\n## ğŸ’¡ å®ç”¨å·¥å…·å‡½æ•°\n\n### è‡ªåŠ¨éªŒè¯ç­‰å¼\n```python\nimport sympy as sp\n\ndef verify_identity(expr1, expr2, method=\"simplify\"):\n    \"\"\"\n    éªŒè¯ä¸¤ä¸ªè¡¨è¾¾å¼æ˜¯å¦æ’ç­‰\n    method: \"simplify\", \"expand\", \"factor\", \"trigsimp\"\n    \"\"\"\n    if method == \"simplify\":\n        difference = sp.simplify(expr1 - expr2)\n    elif method == \"expand\":\n        difference = sp.expand(expr1 - expr2)\n    elif method == \"factor\":\n        difference = sp.factor(expr1 - expr2)\n    elif method == \"trigsimp\":\n        difference = sp.trigsimp(expr1 - expr2)\n    else:\n        difference = expr1 - expr2\n    \n    is_identity = (difference == 0)\n    \n    print(f\"è¡¨è¾¾å¼1: {expr1}\")\n    print(f\"è¡¨è¾¾å¼2: {expr2}\")\n    print(f\"éªŒè¯æ–¹æ³•: {method}\")\n    print(f\"æ˜¯å¦æ’ç­‰: {is_identity}\")\n    \n    return is_identity\n\n# ä½¿ç”¨ç¤ºä¾‹\nx, y = sp.symbols('x y')\nverify_identity((x + y)**2, x**2 + 2*x*y + y**2, \"expand\")\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n- æ­£å¸¸å¯¼å…¥ï¼š`import sympy as sp`\n- ä½¿ç”¨æ ‡å‡†çš„ SymPy å‡½æ•°å’Œè¯­æ³•\n- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœ\n- å¯¹äºå¤æ‚è¡¨è¾¾å¼ï¼Œä½¿ç”¨ `sp.N()` è·å–æ•°å€¼ç»“æœ\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\n- ä¸è¦ä½¿ç”¨å¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\n- ä¸è¦æ··åˆä½¿ç”¨ SymPy å’Œæ•°å€¼è®¡ç®—åº“ï¼ˆé™¤éå¿…è¦ï¼‰\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    import sympy as sp\n    x = sp.symbols('x')\n    result = sp.solve(x**2 - 1, x)\n    print(f\"æ–¹ç¨‹è§£: {result}\")\nexcept ImportError:\n    print(\"SymPy ä¸å¯ç”¨\")\nexcept Exception as e:\n    print(f\"è®¡ç®—é”™è¯¯: {e}\")\n```\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç¬¦å·æ•°å­¦è®¡ç®—ï¼"
      }
    },
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\python_sandbox",
    "lastUpdated": "2025-11-29T10:58:06.602Z"
  },
  "stockfish_analyzer": {
    "metadata": {
      "name": "stockfish_analyzer",
      "description": "å›½é™…è±¡æ£‹å¼•æ“åˆ†æå·¥å…·ï¼Œæä¾›æœ€ä½³èµ°æ³•æ¨èã€å±€é¢è¯„ä¼°å’Œå¤šç§èµ°æ³•é€‰æ‹©åˆ†æã€‚æ”¯æŒFENå­—ç¬¦ä¸²ç›´æ¥è¾“å…¥åˆ†æã€‚",
      "tool_name": "stockfish_analyzer",
      "category": "chess",
      "priority": 6,
      "tags": [
        "chess",
        "analysis",
        "game",
        "strategy",
        "evaluation",
        "FEN",
        "SAN",
        "position",
        "move",
        "best-move",
        "top-moves",
        "chess-engine",
        "stockfish",
        "board",
        "æ£‹å±€",
        "èµ°æ³•",
        "è¯„ä¼°",
        "å±€é¢"
      ],
      "version": 1.1
    },
    "content": "# å›½é™…è±¡æ£‹AIåŠ©æ•™æŒ‡å—\n\nä½ æ˜¯ä¸€ä½é¡¶çº§çš„å›½é™…è±¡æ£‹AIåŠ©æ•™ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ä½œä¸ºç”¨æˆ·å’Œå¼ºå¤§çš„ \"stockfish_analyzer\" å·¥å…·ä¹‹é—´çš„æ™ºèƒ½æ¡¥æ¢ã€‚ä½  **ä¸è‡ªå·±ä¸‹æ£‹**ï¼Œè€Œæ˜¯ **è°ƒç”¨å·¥å…·** å¹¶ **è§£é‡Šç»“æœ**ã€‚\n\n## ğŸ¯ æ ¸å¿ƒå·¥ä½œæµç¨‹\n\n### 1. **è¯†åˆ«FENå­—ç¬¦ä¸²å’Œç”¨æˆ·æ„å›¾**\n- **FENå­—ç¬¦ä¸²ç‰¹å¾**: è¯†åˆ«å¦‚ `rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1` æ ¼å¼çš„å­—ç¬¦ä¸²\n- **è‡ªåŠ¨è§¦å‘**: å½“æ£€æµ‹åˆ°æœ‰æ•ˆçš„FENå­—ç¬¦ä¸²æ—¶ï¼Œè‡ªåŠ¨è°ƒç”¨åˆ†æå·¥å…·\n- **æ„å›¾åˆ†æ**: æ ¹æ®ç”¨æˆ·é—®é¢˜é€‰æ‹©åˆé€‚æ¨¡å¼ï¼š\n  - **æœ€ä½³èµ°æ³•**: \"æˆ‘è¯¥æ€ä¹ˆèµ°ï¼Ÿ\"ã€\"æœ€ä½³èµ°æ³•\"ã€\"ä¸‹ä¸€æ­¥\" â†’ `get_best_move`\n  - **å¤šç§é€‰æ‹©**: \"å‰ä¸‰æ­¥æ¨è\"ã€\"æœ‰å“ªäº›é€‰æ‹©\"ã€\"å‡ ä¸ªå¥½èµ°æ³•\" â†’ `get_top_moves`\n  - **å±€é¢è¯„ä¼°**: \"è°ä¼˜åŠ¿\"ã€\"å±€é¢å¦‚ä½•\"ã€\"è¯„ä¼°\" â†’ `evaluate_position`\n\n### 2. **è°ƒç”¨æ­£ç¡®å·¥å…·**\næ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©å¯¹åº”çš„åˆ†ææ¨¡å¼ã€‚\n\n### 3. **è§£é‡Šå·¥å…·ç»“æœ**\nå°†ä¸“ä¸šçš„å¼•æ“è¾“å‡ºè½¬åŒ–ä¸ºæ˜“æ‡‚çš„æ•™å­¦è¯­è¨€ã€‚\n\n## ğŸ“‹ å¿«é€Ÿä½¿ç”¨æŒ‡å—\n\n### åœºæ™¯1ï¼šç›´æ¥FENåˆ†æ\n**ç”¨æˆ·è¾“å…¥**: `rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1`\n**è‡ªåŠ¨å“åº”**: åˆ†æåˆå§‹å±€é¢ï¼Œæä¾›æœ€ä½³èµ°æ³•å’Œè¯„ä¼°\n\n### åœºæ™¯2ï¼šFEN + ç®€å•æŒ‡ä»¤  \n**ç”¨æˆ·è¾“å…¥**: `r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3` å‰ä¸‰æ­¥æ¨è\n**å·¥å…·è°ƒç”¨**: `get_top_moves` with `top_n: 3`\n\n### åœºæ™¯3ï¼šå±€é¢è¯„ä¼°è¯·æ±‚\n**ç”¨æˆ·è¾“å…¥**: `r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3` ç°åœ¨è°ä¼˜åŠ¿ï¼Ÿ\n**å·¥å…·è°ƒç”¨**: `evaluate_position`\n\n## ğŸ”§ å·¥å…·è°ƒç”¨è§„èŒƒ\n\n**é‡è¦æç¤º**: å½“ä½ å†³å®šè°ƒç”¨ `stockfish_analyzer` å·¥å…·æ—¶ï¼Œä½ çš„æ€è€ƒè¿‡ç¨‹åº”è¯¥ç”Ÿæˆä¸€ä¸ªåŒ…å« `tool_name` å’Œ `parameters` å­—æ®µçš„JSONå¯¹è±¡ã€‚`parameters` å­—æ®µçš„å€¼å¿…é¡»ä¸¥æ ¼éµå®ˆå·¥å…·çš„è¾“å…¥æ¨¡å¼ã€‚\n\n### âœ… æ­£ç¡®çš„è°ƒç”¨ç»“æ„\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\",\n  \"parameters\": {\n    \"fen\": \"<FENå­—ç¬¦ä¸²>\",\n    \"mode\": \"<åŠŸèƒ½æ¨¡å¼>\",\n    \"options\": {\n      \"<é€‰é¡¹å>\": \"<é€‰é¡¹å€¼>\"\n    }\n  }\n}\n```\n\n### åŠŸèƒ½æ¨¡å¼è¯¦è§£\n\n#### 1. è·å–æœ€ä½³èµ°æ³• (`get_best_move`)\n**é€‚ç”¨åœºæ™¯**: ç”¨æˆ·è¯¢é—®\"æœ€ä½³èµ°æ³•\"ã€\"ä¸‹ä¸€æ­¥æ€ä¹ˆèµ°\"\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\",\n  \"parameters\": {\n    \"fen\": \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\",\n    \"mode\": \"get_best_move\"\n  }\n}\n```\n\n#### 2. è·å–å¤šä¸ªèµ°æ³•é€‰é¡¹ (`get_top_moves`)\n**é€‚ç”¨åœºæ™¯**: ç”¨æˆ·è¯¢é—®\"å‰ä¸‰æ­¥\"ã€\"æœ‰å“ªäº›é€‰æ‹©\"ã€\"å‡ ä¸ªå¥½èµ°æ³•\"\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\", \n  \"parameters\": {\n    \"fen\": \"r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3\",\n    \"mode\": \"get_top_moves\",\n    \"options\": {\n      \"top_n\": 3\n    }\n  }\n}\n```\n\n#### 3. è¯„ä¼°å±€é¢ (`evaluate_position`)\n**é€‚ç”¨åœºæ™¯**: ç”¨æˆ·è¯¢é—®\"å±€é¢å¦‚ä½•\"ã€\"è°ä¼˜åŠ¿\"ã€\"è¯„ä¼°ä¸€ä¸‹\"\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\",\n  \"parameters\": {\n    \"fen\": \"r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3\", \n    \"mode\": \"evaluate_position\"\n  }\n}\n```\n\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\n\n- **ç¼ºå°‘ `fen` å‚æ•°**: `{\"tool_name\": \"stockfish_analyzer\", \"parameters\": {\"mode\": \"get_best_move\"}}`\n- **é”™è¯¯çš„ `mode` åç§°**: `{\"tool_name\": \"stockfish_analyzer\", \"parameters\": {\"fen\": \"...\", \"mode\": \"best_move\"}}` (åº”ä¸º \"get_best_move\")\n- **options æ ¼å¼é”™è¯¯**: `{\"tool_name\": \"stockfish_analyzer\", \"parameters\": {\"fen\": \"...\", \"mode\": \"get_top_moves\", \"options\": 3}}` (options å¿…é¡»æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚ `{\"top_n\": 3}`)\n\n## ğŸ’¡ ç»“æœè§£é‡ŠæŒ‡å—\n\n### è¯„ä¼°åˆ†æ•°è§£é‡Š\n- **å…µå€¼ä¼˜åŠ¿**: `\"evaluation\": {\"type\": \"cp\", \"value\": 250}` â†’ \"ç™½æ–¹æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œç›¸å½“äºå¤š2.5ä¸ªå…µ\"\n- **è½»å¾®ä¼˜åŠ¿**: `\"evaluation\": {\"type\": \"cp\", \"value\": -120}` â†’ \"é»‘æ–¹ç¨å ä¼˜ï¼Œä¼˜åŠ¿çº¦1.2ä¸ªå…µ\"  \n- **å°†æ­»å±€é¢**: `\"evaluation\": {\"type\": \"mate\", \"value\": 3}` â†’ \"ç™½æ–¹3æ­¥å†…å¯å°†æ­»å¯¹æ–¹\"\n\n### èµ°æ³•è§£é‡Š\n- **UCIè½¬SAN**: `\"best_move\": \"g1f3\"` â†’ \"æœ€ä½³èµ°æ³•æ˜¯ **Nf3**\"\n- **æˆ˜ç•¥æ„å›¾**: è§£é‡Šèµ°æ³•çš„ç›®çš„å’Œæˆ˜ç•¥æ„ä¹‰\n- **å¤šèµ°æ³•æ¯”è¾ƒ**: å½“æœ‰å¤šä¸ªé€‰é¡¹æ—¶ï¼Œåˆ†æå„è‡ªçš„ä¼˜ç¼ºç‚¹\n\n## ğŸš€ æ™ºèƒ½è¯†åˆ«å¢å¼º\n\n### FENå­—ç¬¦ä¸²ç‰¹å¾è¯†åˆ«\n- **æ ¼å¼ç‰¹å¾**: åŒ…å« `/` åˆ†éš”çš„è¡Œã€`w`/`b` èµ°å­æ–¹ã€æ˜“ä½æƒåˆ©ç­‰\n- **è‡ªåŠ¨æ£€æµ‹**: æ£€æµ‹åˆ°FENæ ¼å¼æ—¶è‡ªåŠ¨è§¦å‘åˆ†æ\n- **å®¹é”™å¤„ç†**: å¤„ç†å¸¸è§çš„FENæ ¼å¼å˜ä½“\n\n### ç”¨æˆ·æ„å›¾å…³é”®è¯\n- **æœ€ä½³èµ°æ³•ç±»**: \"æœ€ä½³\"ã€\"æœ€å¥½\"ã€\"æ€ä¹ˆèµ°\"ã€\"ä¸‹ä¸€æ­¥\"\n- **å¤šé€‰é¡¹ç±»**: \"å‡ ä¸ª\"ã€\"å“ªäº›\"ã€\"é€‰æ‹©\"ã€\"æ¨è\"ã€\"å‰ä¸‰\"  \n- **è¯„ä¼°ç±»**: \"è¯„ä¼°\"ã€\"ä¼˜åŠ¿\"ã€\"å±€é¢\"ã€\"è°å¥½\"\n- **ä¸­è‹±æ–‡æ··åˆ**: æ”¯æŒä¸­æ–‡æŒ‡ä»¤å¦‚\"æ£‹å±€\"ã€\"èµ°æ³•\"ã€\"è¯„ä¼°\"\n\n## âš ï¸ å¸¸è§é—®é¢˜å¤„ç†\n\n### FENè¯†åˆ«é—®é¢˜\n**ç”¨æˆ·è¾“å…¥ä¸åŒ…å«FEN**:\n```\n\"è¯·æä¾›å½“å‰å±€é¢çš„FENå­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n```\n\n**æ— æ•ˆFENæ ¼å¼**:\n```\n\"è¿™ä¸ªFENå­—ç¬¦ä¸²æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥å¹¶é‡æ–°æä¾›æœ‰æ•ˆçš„FENå­—ç¬¦ä¸²\"\n```\n\n### æ¨¡å¼é€‰æ‹©å»ºè®®\n**æ¨¡ç³ŠæŒ‡ä»¤**:\n```\n\"æ‚¨æ˜¯æƒ³çŸ¥é“æœ€ä½³èµ°æ³•ï¼Œè¿˜æ˜¯æƒ³çœ‹çœ‹å¤šä¸ªé€‰æ‹©ï¼Ÿ\"\n```\n\n## ğŸ“ æœ€ä½³å®è·µ\n\n### å“åº”æ¨¡æ¿\n1. **ç¡®è®¤å±€é¢**: \"åˆ†ææ‚¨æä¾›çš„å±€é¢...\"\n2. **è°ƒç”¨å·¥å…·**: [è‡ªåŠ¨è°ƒç”¨å¯¹åº”æ¨¡å¼]\n3. **è§£é‡Šç»“æœ**: ç”¨é€šä¿—è¯­è¨€è§£é‡Šå¼•æ“åˆ†æ\n4. **æ•™å­¦æŒ‡å¯¼**: æä¾›æˆ˜ç•¥å»ºè®®å’Œå­¦ä¹ è¦ç‚¹\n\n### é”™è¯¯å¤„ç†\n- **ç¼ºå°‘FEN**: å‹å¥½æç¤ºç”¨æˆ·æä¾›FEN\n- **æ— æ•ˆFEN**: è¯´æ˜æ­£ç¡®æ ¼å¼è¦æ±‚  \n- **ç½‘ç»œé—®é¢˜**: æç¤ºç¨åé‡è¯•\n\n---\n\n**é‡è¦æç¤º**: ä¸¥æ ¼éµå®ˆ\"ä¸åˆ›é€ èµ°æ³•ã€ä¸è‡ªè¡Œè¯„ä¼°\"çš„åŸåˆ™ï¼Œæ‰€æœ‰åˆ†æå¿…é¡»åŸºäºå·¥å…·è¾“å‡ºã€‚ä½ çš„ä»·å€¼åœ¨äºå°†ä¸“ä¸šçš„å¼•æ“åˆ†æè½¬åŒ–ä¸ºæ˜“æ‡‚çš„æ•™å­¦æŒ‡å¯¼ã€‚",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\stockfish_analyzer",
    "lastUpdated": "2025-11-29T10:58:06.603Z"
  },
  "tavily_search": {
    "metadata": {
      "name": "tavily_search",
      "description": "ä½¿ç”¨Tavily APIè¿›è¡Œç½‘ç»œæœç´¢ï¼Œè·å–å®æ—¶ä¿¡æ¯ã€å›ç­”é—®é¢˜æˆ–ç ”ç©¶ä¸»é¢˜",
      "tool_name": "tavily_search",
      "category": "search",
      "priority": 8,
      "tags": [
        "search",
        "research",
        "real-time",
        "information"
      ],
      "version": 1
    },
    "content": "# å·¥å…·è°ƒç”¨ç¤ºä¾‹ï¼ˆTavily Searchï¼‰\r\n\r\nå½“æ‚¨å†³å®šè°ƒç”¨ tavily_search å·¥å…·æ—¶ï¼Œæ‚¨çš„å“åº”åº”è¯¥æ˜¯ä¸€ä¸ªåŒ…å« tool_name å’Œ parameters å­—æ®µçš„ JSON å¯¹è±¡ã€‚parameters å­—æ®µçš„å€¼åº”æ˜¯å·¥å…·æ‰€éœ€çš„å‚æ•°å¯¹è±¡ã€‚\r\n\r\n## âœ… æ­£ç¡®ç¤ºä¾‹\r\n\r\n**parameters å­—æ®µå†…å®¹:**\r\n```json\r\n{\"query\": \"latest AI news\"}\r\n```\r\n\r\n**å®Œæ•´å·¥å…·è°ƒç”¨å“åº”ç¤ºä¾‹:**\r\n```json\r\n{\"tool_name\": \"tavily_search\", \"parameters\": {\"query\": \"latest AI news\"}}\r\n```\r\n\r\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\r\n\r\n- **åœ¨ JSON ä¸­åµŒå…¥ Markdown åˆ†éš”ç¬¦:** \r\n  ```json\r\n  \"```json\\n{\\\"query\\\": \\\"latest AI news\\\"}\\n```\"\r\n  ```\r\n  (Qwen æ¨¡å‹ä¼šå°†æ­¤ä½œä¸º JSON å­—ç¬¦ä¸²çš„ä¸€éƒ¨åˆ†ï¼Œå¯¼è‡´è§£æå¤±è´¥)\r\n\r\n- **å‚æ•°åé”™è¯¯:** \r\n  ```json\r\n  {\"q\": \"latest AI news\"}\r\n  ```\r\n  (åº”ä¸º \"query\" è€Œé \"q\")\r\n\r\n- **å‚æ•°å€¼é”™è¯¯:** \r\n  ```json\r\n  {\"query\": 123}\r\n  ```\r\n  (query å‚æ•°å€¼åº”ä¸ºå­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æ•°å­—)\r\n\r\n## å…³é”®æŒ‡ä»¤\r\n1. **æŸ¥è¯¢æ„å»º**: æŸ¥è¯¢åº”è¯¥å…·ä½“ä¸”ç›¸å…³\r\n2. **å®æ—¶æ€§**: é€‚ç”¨äºéœ€è¦æœ€æ–°ä¿¡æ¯çš„é—®é¢˜\r\n3. **éªŒè¯**: å¯ç”¨äºéªŒè¯å…¶ä»–ä¿¡æ¯æ¥æº\r\n\r\n## ä½¿ç”¨åœºæ™¯\r\n1. è·å–å®æ—¶æ–°é—»å’Œä¿¡æ¯\r\n2. å›ç­”éœ€è¦æœ€æ–°æ•°æ®çš„é—®é¢˜\r\n3. ç ”ç©¶ç‰¹å®šä¸»é¢˜çš„èƒŒæ™¯ä¿¡æ¯\r\n4. éªŒè¯äº‹å®å’Œæ•°æ®çš„å‡†ç¡®æ€§\r\n\r\n## æœ€ä½³å®è·µ\r\n- æŸ¥è¯¢åº”è¯¥å…·ä½“ä¸”ç›¸å…³\r\n- å¯¹äºå¤æ‚é—®é¢˜ï¼Œå¯ä»¥åˆ†è§£ä¸ºå¤šä¸ªæœç´¢æŸ¥è¯¢\r\n- ç»“åˆæœç´¢ç»“æœè¿›è¡Œç»¼åˆåˆ†æ\r\n- ä¼˜å…ˆä½¿ç”¨è‹±æ–‡å…³é”®è¯è·å–æ›´å‡†ç¡®çš„ç»“æœ\r\n\r\n## ç¤ºä¾‹æŸ¥è¯¢\r\n- \"2024å¹´äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•\"\r\n- \"OpenAIæœ€æ–°æ¨¡å‹å‘å¸ƒä¿¡æ¯\"\r\n- \"æ¯”ç‰¹å¸å½“å‰ä»·æ ¼å’Œè¶‹åŠ¿\"\r\n- \"Python 3.12æ–°ç‰¹æ€§è¯¦è§£\"",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\tavily_search",
    "lastUpdated": "2025-11-29T10:58:06.603Z"
  }
};

export function getSkillsRegistry() {
  const map = new Map();
  Object.entries(SKILLS_DATA).forEach(([key, value]) => {
    map.set(key, value);
  });
  return map;
}