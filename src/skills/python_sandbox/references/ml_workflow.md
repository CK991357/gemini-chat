# æœºå™¨å­¦ä¹ å·¥ä½œæµæŒ‡å— (v2.2)

## ğŸ¯ å·¥å…·æ¦‚è¿°
**åŠŸèƒ½**ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–
**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼

## ğŸ“Š åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡æ¿

### æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

def prepare_ml_data():
    """æœºå™¨å­¦ä¹ æ•°æ®å‡†å¤‡ç¤ºä¾‹"""
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
    np.random.seed(42)
    n_samples = 1000
    
    # å›å½’é—®é¢˜æ•°æ®
    X_reg = np.random.normal(0, 1, (n_samples, 5))
    y_reg = 2 * X_reg[:, 0] + 1.5 * X_reg[:, 1] - X_reg[:, 2] + np.random.normal(0, 0.5, n_samples)
    
    # åˆ†ç±»é—®é¢˜æ•°æ®
    X_clf = np.random.normal(0, 1, (n_samples, 4))
    y_clf = (X_clf[:, 0] + X_clf[:, 1] > 0).astype(int)
    
    print("=== æ•°æ®å‡†å¤‡å®Œæˆ ===")
    print(f"æ ·æœ¬æ•°é‡: {n_samples}")
    print(f"å›å½’ç‰¹å¾ç»´åº¦: {X_reg.shape[1]}")
    print(f"åˆ†ç±»ç‰¹å¾ç»´åº¦: {X_clf.shape[1]}")
    print(f"åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(y_clf, return_counts=True)}")
    
    return X_reg, y_reg, X_clf, y_clf

# ä½¿ç”¨ç¤ºä¾‹
# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()
```

### æ ‡å‡†æœºå™¨å­¦ä¹ å·¥ä½œæµ
```python
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report
from sklearn.model_selection import cross_val_score

def standard_ml_pipeline(X, y, problem_type='regression'):
    """æ ‡å‡†æœºå™¨å­¦ä¹ æµç¨‹"""
    
    print(f"=== å¼€å§‹ {problem_type} æ¨¡å‹è®­ç»ƒ ===")
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42,
        stratify=y if problem_type == 'classification' else None
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # é€‰æ‹©æ¨¡å‹
    if problem_type == 'regression':
        model = RandomForestRegressor(n_estimators=100, random_state=42)
    else:
        model = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train_scaled, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test_scaled)
    
    # æ¨¡å‹è¯„ä¼°
    if problem_type == 'regression':
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        
        print(f"å›å½’æ¨¡å‹æ€§èƒ½:")
        print(f"  MSE: {mse:.4f}")
        print(f"  RMSE: {rmse:.4f}")
        print(f"  RÂ²: {r2:.4f}")
        
        metrics = {'mse': mse, 'rmse': rmse, 'r2': r2}
    else:
        accuracy = accuracy_score(y_test, y_pred)
        print(f"åˆ†ç±»æ¨¡å‹æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred))
        
        metrics = {'accuracy': accuracy}
    
    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, 
                               scoring='r2' if problem_type == 'regression' else 'accuracy')
    print(f"äº¤å‰éªŒè¯å¹³å‡å¾—åˆ†: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
    
    return {
        'model': model,
        'metrics': metrics,
        'X_test': X_test,
        'y_test': y_test,
        'y_pred': y_pred,
        'cv_scores': cv_scores
    }

# ä½¿ç”¨ç¤ºä¾‹
# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()
# regression_results = standard_ml_pipeline(X_reg, y_reg, 'regression')
# classification_results = standard_ml_pipeline(X_clf, y_clf, 'classification')
```

## ğŸ“ˆ å›å½’åˆ†æå®Œæ•´å·¥ä½œæµ

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

def complete_regression_analysis():
    """å®Œæ•´çš„å›å½’åˆ†æå·¥ä½œæµ"""
    
    print("=== å¼€å§‹å›å½’åˆ†æ ===")
    
    # 1. æ•°æ®ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 500
    
    # åˆ›å»ºæœ‰æ„ä¹‰çš„ç‰¹å¾
    feature1 = np.random.normal(50, 15, n_samples)  # å¹´é¾„
    feature2 = np.random.normal(100, 25, n_samples) # æ”¶å…¥
    feature3 = np.random.normal(10, 3, n_samples)   # æ•™è‚²å¹´é™
    feature4 = np.random.normal(0, 1, n_samples)    # å™ªå£°ç‰¹å¾
    
    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆæ¨¡æ‹Ÿæˆ¿ä»·ï¼‰
    target = (50 * feature1 + 80 * feature2 + 5000 * feature3 + 
              10 * feature1 * feature3 + np.random.normal(0, 10000, n_samples))
    
    df = pd.DataFrame({
        'å¹´é¾„': feature1,
        'æ”¶å…¥': feature2,
        'æ•™è‚²å¹´é™': feature3,
        'å™ªå£°ç‰¹å¾': feature4,
        'æˆ¿ä»·': target
    })
    
    print("æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
    print(f"ç‰¹å¾åˆ—è¡¨: {list(df.columns[:-1])}")
    print(f"ç›®æ ‡å˜é‡: {df.columns[-1]}")
    
    # 2. æ•°æ®æ¢ç´¢
    print("\n=== æ•°æ®æ¢ç´¢ ===")
    print("æ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
    print(df.describe())
    
    # ç›¸å…³æ€§åˆ†æ
    correlation = df.corr()['æˆ¿ä»·'].sort_values(ascending=False)
    print("\nç‰¹å¾ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§:")
    for feature, corr in correlation.items():
        if feature != 'æˆ¿ä»·':
            print(f"  {feature}: {corr:.3f}")
    
    # 3. æ¨¡å‹è®­ç»ƒ
    X = df.drop('æˆ¿ä»·', axis=1)
    y = df['æˆ¿ä»·']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    # 4. æ¨¡å‹è¯„ä¼°
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    print(f"\n=== æ¨¡å‹æ€§èƒ½ ===")
    print(f"å‡æ–¹è¯¯å·® (MSE): {mse:,.2f}")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:,.2f}")
    print(f"å†³å®šç³»æ•° (RÂ²): {r2:.4f}")
    
    # 5. ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'ç‰¹å¾': X.columns,
        'é‡è¦æ€§': model.feature_importances_
    }).sort_values('é‡è¦æ€§', ascending=False)
    
    print(f"\n=== ç‰¹å¾é‡è¦æ€§ ===")
    for _, row in feature_importance.iterrows():
        print(f"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}")
    
    # 6. å¯è§†åŒ–åˆ†æ
    plt.figure(figsize=(15, 10))
    
    # å®é™…å€¼ vs é¢„æµ‹å€¼
    plt.subplot(2, 3, 1)
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('å®é™…å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')
    plt.grid(True, alpha=0.3)
    
    # æ®‹å·®åˆ†æ
    plt.subplot(2, 3, 2)
    residuals = y_test - y_pred
    plt.scatter(y_pred, residuals, alpha=0.6)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('é¢„æµ‹å€¼')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®åˆ†æ')
    plt.grid(True, alpha=0.3)
    
    # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
    plt.subplot(2, 3, 3)
    top_features = feature_importance.head(5)
    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])
    plt.xlabel('é‡è¦æ€§')
    plt.title('Top 5 ç‰¹å¾é‡è¦æ€§')
    plt.gca().invert_yaxis()
    
    # è¯¯å·®åˆ†å¸ƒ
    plt.subplot(2, 3, 4)
    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')
    plt.xlabel('æ®‹å·®')
    plt.ylabel('é¢‘æ•°')
    plt.title('è¯¯å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    # ç›¸å¯¹è¯¯å·®
    plt.subplot(2, 3, 5)
    relative_error = np.abs(residuals / y_test) * 100
    plt.hist(relative_error, bins=30, alpha=0.7, edgecolor='black')
    plt.xlabel('ç›¸å¯¹è¯¯å·® (%)')
    plt.ylabel('é¢‘æ•°')
    plt.title('ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    # é¢„æµ‹è¯¯å·®ç®±çº¿å›¾
    plt.subplot(2, 3, 6)
    plt.boxplot(relative_error)
    plt.ylabel('ç›¸å¯¹è¯¯å·® (%)')
    plt.title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 7. æ¨¡å‹è§£é‡Š
    print(f"\n=== æ¨¡å‹è§£é‡Š ===")
    print(f"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if r2 > 0.8 else 'è‰¯å¥½' if r2 > 0.6 else 'ä¸€èˆ¬'}")
    print(f"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}")
    print(f"å»ºè®®: å…³æ³¨{feature_importance.iloc[0]['ç‰¹å¾']}å’Œ{feature_importance.iloc[1]['ç‰¹å¾']}çš„ä¼˜åŒ–")
    
    return {
        'model': model,
        'metrics': {'mse': mse, 'rmse': rmse, 'r2': r2},
        'feature_importance': feature_importance,
        'predictions': y_pred
    }

# ä½¿ç”¨ç¤ºä¾‹
# regression_results = complete_regression_analysis()
```

## ğŸ” åˆ†ç±»åˆ†æå®Œæ•´å·¥ä½œæµ

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import make_classification

def complete_classification_analysis():
    """å®Œæ•´çš„åˆ†ç±»åˆ†æå·¥ä½œæµ"""
    
    print("=== å¼€å§‹åˆ†ç±»åˆ†æ ===")
    
    # 1. æ•°æ®ç”Ÿæˆ
    X, y = make_classification(
        n_samples=1000,
        n_features=8,
        n_informative=5,
        n_redundant=2,
        n_classes=3,
        random_state=42
    )
    
    feature_names = [f'ç‰¹å¾_{i+1}' for i in range(X.shape[1])]
    df = pd.DataFrame(X, columns=feature_names)
    df['ç±»åˆ«'] = y
    
    print("æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
    print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"ç±»åˆ«æ•°é‡: {len(np.unique(y))}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.unique(y, return_counts=True)}")
    
    # 2. æ•°æ®æ¢ç´¢
    print("\n=== æ•°æ®æ¢ç´¢ ===")
    print("æ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
    print(df.describe())
    
    # 3. æ¨¡å‹è®­ç»ƒ
    X_data = df.drop('ç±»åˆ«', axis=1)
    y_data = df['ç±»åˆ«']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_data, y_data, test_size=0.2, random_state=42, stratify=y_data
    )
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    # 4. æ¨¡å‹è¯„ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\n=== æ¨¡å‹æ€§èƒ½ ===")
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))
    
    # 5. ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'ç‰¹å¾': X_data.columns,
        'é‡è¦æ€§': model.feature_importances_
    }).sort_values('é‡è¦æ€§', ascending=False)
    
    print(f"\n=== ç‰¹å¾é‡è¦æ€§ ===")
    for _, row in feature_importance.iterrows():
        print(f"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}")
    
    # 6. å¯è§†åŒ–åˆ†æ
    plt.figure(figsize=(15, 10))
    
    # æ··æ·†çŸ©é˜µ
    plt.subplot(2, 3, 1)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title('æ··æ·†çŸ©é˜µ')
    
    # ç‰¹å¾é‡è¦æ€§
    plt.subplot(2, 3, 2)
    top_features = feature_importance.head(8)
    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])
    plt.xlabel('é‡è¦æ€§')
    plt.title('ç‰¹å¾é‡è¦æ€§æ’å')
    plt.gca().invert_yaxis()
    
    # ç±»åˆ«åˆ†å¸ƒ
    plt.subplot(2, 3, 3)
    unique, counts = np.unique(y, return_counts=True)
    plt.pie(counts, labels=[f'ç±»åˆ« {cls}' for cls in unique], autopct='%1.1f%%')
    plt.title('ç±»åˆ«åˆ†å¸ƒ')
    
    # åˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾
    plt.subplot(2, 3, 4)
    report_dict = classification_report(y_test, y_pred, output_dict=True)
    report_df = pd.DataFrame(report_dict).transpose().iloc[:-3, :-1]
    sns.heatmap(report_df, annot=True, cmap='YlOrRd', fmt='.3f')
    plt.title('åˆ†ç±»æŒ‡æ ‡çƒ­åŠ›å›¾')
    
    # å­¦ä¹ æ›²çº¿ï¼ˆç®€åŒ–ç‰ˆï¼‰
    plt.subplot(2, 3, 5)
    train_sizes = np.linspace(0.1, 1.0, 10)
    train_scores = []
    test_scores = []
    
    for size in train_sizes:
        n_train = int(size * len(X_train))
        X_train_sub = X_train.iloc[:n_train]
        y_train_sub = y_train.iloc[:n_train]
        
        model_temp = RandomForestClassifier(n_estimators=50, random_state=42)
        model_temp.fit(X_train_sub, y_train_sub)
        
        train_score = model_temp.score(X_train_sub, y_train_sub)
        test_score = model_temp.score(X_test, y_test)
        
        train_scores.append(train_score)
        test_scores.append(test_score)
    
    plt.plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒå¾—åˆ†')
    plt.plot(train_sizes, test_scores, 'o-', label='æµ‹è¯•å¾—åˆ†')
    plt.xlabel('è®­ç»ƒæ ·æœ¬æ¯”ä¾‹')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('å­¦ä¹ æ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ç±»åˆ«é¢„æµ‹åˆ†å¸ƒ
    plt.subplot(2, 3, 6)
    pred_counts = pd.Series(y_pred).value_counts().sort_index()
    true_counts = pd.Series(y_test).value_counts().sort_index()
    
    x = np.arange(len(true_counts))
    width = 0.35
    
    plt.bar(x - width/2, true_counts, width, label='çœŸå®åˆ†å¸ƒ', alpha=0.7)
    plt.bar(x + width/2, pred_counts, width, label='é¢„æµ‹åˆ†å¸ƒ', alpha=0.7)
    plt.xlabel('ç±»åˆ«')
    plt.ylabel('æ ·æœ¬æ•°')
    plt.title('ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 7. æ¨¡å‹è§£é‡Š
    print(f"\n=== æ¨¡å‹è§£é‡Š ===")
    print(f"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if accuracy > 0.9 else 'è‰¯å¥½' if accuracy > 0.8 else 'ä¸€èˆ¬'}")
    print(f"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}")
    print(f"æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«: æŸ¥çœ‹æ··æ·†çŸ©é˜µå¯¹è§’çº¿å¤–çš„æœ€å¤§å€¼")
    
    return {
        'model': model,
        'metrics': {'accuracy': accuracy},
        'feature_importance': feature_importance,
        'predictions': y_pred
    }

# ä½¿ç”¨ç¤ºä¾‹
# classification_results = complete_classification_analysis()
```

## ğŸ“Š ç»Ÿè®¡å»ºæ¨¡åˆ†æ

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf

def statistical_modeling_analysis():
    """ç»Ÿè®¡å»ºæ¨¡åˆ†æ"""
    
    print("=== å¼€å§‹ç»Ÿè®¡å»ºæ¨¡åˆ†æ ===")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 200
    
    data = pd.DataFrame({
        'å¹¿å‘ŠæŠ•å…¥': np.random.normal(1000, 300, n_samples),
        'ä»·æ ¼': np.random.normal(50, 15, n_samples),
        'ä¿ƒé”€æ´»åŠ¨': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),
        'å­£èŠ‚æ€§': np.random.choice([0, 1], n_samples, p=[0.5, 0.5])
    })
    
    # ç”Ÿæˆé”€å”®é¢ï¼ˆä¸ç‰¹å¾æœ‰çœŸå®å…³ç³»ï¼‰
    data['é”€å”®é¢'] = (
        500 + 0.8 * data['å¹¿å‘ŠæŠ•å…¥'] - 5 * data['ä»·æ ¼'] + 
        200 * data['ä¿ƒé”€æ´»åŠ¨'] + 150 * data['å­£èŠ‚æ€§'] + 
        np.random.normal(0, 100, n_samples)
    )
    
    print("æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"æ ·æœ¬æ•°é‡: {len(data)}")
    print(f"ç‰¹å¾: {list(data.columns[:-1])}")
    print("\næ•°æ®æè¿°:")
    print(data.describe())
    
    # 1. OLS å›å½’åˆ†æ
    print("\n=== OLS å›å½’åˆ†æ ===")
    model = smf.ols('é”€å”®é¢ ~ å¹¿å‘ŠæŠ•å…¥ + ä»·æ ¼ + ä¿ƒé”€æ´»åŠ¨ + å­£èŠ‚æ€§', data=data).fit()
    
    print("å›å½’ç»“æœæ‘˜è¦:")
    print(model.summary())
    
    # 2. å…³é”®ç»Ÿè®¡æŒ‡æ ‡
    print(f"\n=== å…³é”®ç»Ÿè®¡æŒ‡æ ‡ ===")
    print(f"RÂ²: {model.rsquared:.4f}")
    print(f"è°ƒæ•´RÂ²: {model.rsquared_adj:.4f}")
    print(f"Fç»Ÿè®¡é‡: {model.fvalue:.2f}")
    print(f"Fç»Ÿè®¡é‡på€¼: {model.f_pvalue:.4f}")
    
    # 3. ç³»æ•°è§£é‡Š
    print(f"\n=== ç³»æ•°è§£é‡Š ===")
    for feature, coef in model.params.items():
        p_value = model.pvalues[feature]
        significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else ""
        print(f"{feature}: {coef:.2f} {significance} (på€¼: {p_value:.4f})")
    
    # 4. æ®‹å·®åˆ†æ
    print(f"\n=== æ®‹å·®åˆ†æ ===")
    residuals = model.resid
    print(f"æ®‹å·®å‡å€¼: {residuals.mean():.4f}")
    print(f"æ®‹å·®æ ‡å‡†å·®: {residuals.std():.4f}")
    
    # 5. å¯è§†åŒ–åˆ†æ
    plt.figure(figsize=(15, 10))
    
    # å®é™…å€¼ vs é¢„æµ‹å€¼
    plt.subplot(2, 3, 1)
    y_pred_ols = model.predict(data[['å¹¿å‘ŠæŠ•å…¥', 'ä»·æ ¼', 'ä¿ƒé”€æ´»åŠ¨', 'å­£èŠ‚æ€§']])
    plt.scatter(data['é”€å”®é¢'], y_pred_ols, alpha=0.6)
    plt.plot([data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], 
             [data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], 'r--', lw=2)
    plt.xlabel('å®é™…é”€å”®é¢')
    plt.ylabel('é¢„æµ‹é”€å”®é¢')
    plt.title(f'OLSé¢„æµ‹æ•ˆæœ (RÂ² = {model.rsquared:.3f})')
    plt.grid(True, alpha=0.3)
    
    # æ®‹å·®å›¾
    plt.subplot(2, 3, 2)
    plt.scatter(y_pred_ols, residuals, alpha=0.6)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('é¢„æµ‹å€¼')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®åˆ†æ')
    plt.grid(True, alpha=0.3)
    
    # Q-Qå›¾
    plt.subplot(2, 3, 3)
    sm.qqplot(residuals, line='45', ax=plt.gca())
    plt.title('Q-Qå›¾ï¼ˆæ®‹å·®æ­£æ€æ€§æ£€éªŒï¼‰')
    
    # ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³ç³»
    plt.subplot(2, 3, 4)
    plt.scatter(data['å¹¿å‘ŠæŠ•å…¥'], data['é”€å”®é¢'], alpha=0.6)
    plt.xlabel('å¹¿å‘ŠæŠ•å…¥')
    plt.ylabel('é”€å”®é¢')
    plt.title('å¹¿å‘ŠæŠ•å…¥ vs é”€å”®é¢')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 3, 5)
    plt.scatter(data['ä»·æ ¼'], data['é”€å”®é¢'], alpha=0.6)
    plt.xlabel('ä»·æ ¼')
    plt.ylabel('é”€å”®é¢')
    plt.title('ä»·æ ¼ vs é”€å”®é¢')
    plt.grid(True, alpha=0.3)
    
    # ç³»æ•°å¯è§†åŒ–
    plt.subplot(2, 3, 6)
    coefficients = model.params.iloc[1:]  # æ’é™¤æˆªè·é¡¹
    colors = ['green' if p < 0.05 else 'red' for p in model.pvalues.iloc[1:]]
    plt.barh(coefficients.index, coefficients.values, color=colors)
    plt.axvline(x=0, color='black', linestyle='-')
    plt.xlabel('ç³»æ•°å€¼')
    plt.title('ç‰¹å¾ç³»æ•°ï¼ˆç»¿è‰²è¡¨ç¤ºæ˜¾è‘—ï¼‰')
    
    plt.tight_layout()
    plt.show()
    
    # 6. ä¸šåŠ¡è§£é‡Š
    print(f"\n=== ä¸šåŠ¡è§£é‡Š ===")
    print(f"æ¨¡å‹è§£é‡ŠåŠ›: {'å¼º' if model.rsquared > 0.7 else 'ä¸­ç­‰' if model.rsquared > 0.5 else 'å¼±'}")
    
    significant_features = []
    for feature in model.params.index[1:]:  # æ’é™¤æˆªè·
        if model.pvalues[feature] < 0.05:
            significant_features.append(feature)
    
    if significant_features:
        print(f"æ˜¾è‘—å½±å“ç‰¹å¾: {', '.join(significant_features)}")
    else:
        print("æ²¡æœ‰å‘ç°ç»Ÿè®¡æ˜¾è‘—çš„ç‰¹å¾")
    
    return {
        'model': model,
        'rsquared': model.rsquared,
        'significant_features': significant_features,
        'residuals': residuals
    }

# ä½¿ç”¨ç¤ºä¾‹
# stats_results = statistical_modeling_analysis()
```

## ğŸ”§ æ¨¡å‹ä¼˜åŒ–ä¸è°ƒå‚

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

def model_optimization_pipeline(X, y, problem_type='regression'):
    """æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–æµç¨‹"""
    
    print(f"=== å¼€å§‹ {problem_type} æ¨¡å‹ä¼˜åŒ– ===")
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # é€‰æ‹©æ¨¡å‹å’Œå‚æ•°ç½‘æ ¼
    if problem_type == 'regression':
        model = RandomForestRegressor(random_state=42)
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        scoring = 'r2'
    else:
        model = RandomForestClassifier(random_state=42)
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        scoring = 'accuracy'
    
    # ç½‘æ ¼æœç´¢
    print("æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢...")
    grid_search = GridSearchCV(
        model, param_grid, cv=5, scoring=scoring, 
        n_jobs=-1, verbose=1
    )
    grid_search.fit(X_train, y_train)
    
    # è¾“å‡ºæœ€ä¼˜å‚æ•°
    print(f"\n=== æœ€ä¼˜å‚æ•° ===")
    for param, value in grid_search.best_params_.items():
        print(f"  {param}: {value}")
    
    print(f"æœ€ä¼˜æ¨¡å‹å¾—åˆ†: {grid_search.best_score_:.4f}")
    
    # æµ‹è¯•é›†æ€§èƒ½
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    
    if problem_type == 'regression':
        test_score = r2_score(y_test, y_pred)
        print(f"æµ‹è¯•é›† RÂ²: {test_score:.4f}")
    else:
        test_score = accuracy_score(y_test, y_pred)
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}")
    
    return {
        'best_model': best_model,
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
        'test_score': test_score
    }

# ä½¿ç”¨ç¤ºä¾‹
# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()
# optimized_regression = model_optimization_pipeline(X_reg, y_reg, 'regression')
# optimized_classification = model_optimization_pipeline(X_clf, y_clf, 'classification')
```

## æœºå™¨å­¦ä¹ å¢å¼º(v2.5æ–°å¢)

### LightGBM - é«˜æ•ˆæ¢¯åº¦æå‡

**ç”¨é€”**: é«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘ç®—æ³•
**ä¼˜åŠ¿**: æ¯”XGBoostè®­ç»ƒæ›´å¿«ï¼Œå†…å­˜å ç”¨æ›´å°‘

```python
import lightgbm as lgb
from sklearn.model_selection import train_test_split
import pandas as pd

# å‡†å¤‡æ•°æ®
data = pd.read_csv('/data/train.csv')
X = data.drop('target', axis=1)
y = data['target']

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºæ•°æ®é›†
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# å‚æ•°è®¾ç½®ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'num_threads': 2  # é™åˆ¶çº¿ç¨‹æ•°
}

# è®­ç»ƒæ¨¡å‹
gbm = lgb.train(params, train_data, num_boost_round=100)
```

### Category Encoders - åˆ†ç±»ç‰¹å¾ç¼–ç 

**ç”¨é€”**: å„ç§åˆ†ç±»ç¼–ç æ–¹æ³•
**ä¼˜åŠ¿**: æå‡åˆ†ç±»æ¨¡å‹æ€§èƒ½ï¼Œæ”¯æŒå¤šç§ç¼–ç ç­–ç•¥

```python
import pandas as pd
import category_encoders as ce

# åˆ›å»ºç¤ºä¾‹æ•°æ®
df = pd.DataFrame({
    'category': ['A', 'B', 'A', 'C', 'B', 'A'],
    'value': [1, 2, 3, 4, 5, 6]
})

# ä½¿ç”¨Target Encoding
encoder = ce.TargetEncoder(cols=['category'])
df_encoded = encoder.fit_transform(df['category'], df['value'])

print(df_encoded)
```

### scikit-optimize - è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–

**ç”¨é€”**: è‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–
**ä¼˜åŠ¿**: æ¯”ç½‘æ ¼æœç´¢æ›´é«˜æ•ˆï¼Œæ‰¾åˆ°æ›´å¥½å‚æ•°ç»„åˆ

```python
from skopt import BayesSearchCV
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# å‡†å¤‡æ•°æ®
data = pd.read_csv('/data/train.csv')
X = data.drop('target', axis=1)
y = data['target']

# å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
param_space = {
    'n_estimators': (50, 200),
    'max_depth': (3, 10),
    'min_samples_split': (2, 10),
    'min_samples_leaf': (1, 4)
}

# è´å¶æ–¯ä¼˜åŒ–æœç´¢
opt = BayesSearchCV(
    RandomForestClassifier(),
    param_space,
    n_iter=50,
    cv=5,
    n_jobs=2  # é™åˆ¶å¹¶è¡Œçº¿ç¨‹
)

opt.fit(X, y)
print(f"æœ€ä½³å‚æ•°: {opt.best_params_}")
print(f"æœ€ä½³åˆ†æ•°: {opt.best_score_:.4f}")
```

## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹

### âœ… æ¨èåšæ³•ï¼š
- ä½¿ç”¨æ ‡å‡†çš„ scikit-learn å’Œ statsmodels æ¥å£
- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœå’ŒæŒ‡æ ‡
- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨
- å¯¹æ•°æ®è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†å’Œæ ‡å‡†åŒ–

### âŒ é¿å…çš„æ“ä½œï¼š
- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º
- ä¸è¦ä½¿ç”¨ `base64` ç¼–ç 
- ä¸è¦åˆ›å»ºå¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼

### ğŸ”§ é”™è¯¯å¤„ç†ï¼š
```python
try:
    from sklearn.ensemble import RandomForestRegressor
    # æ¨¡å‹è®­ç»ƒä»£ç 
except ImportError:
    print("scikit-learn ä¸å¯ç”¨")

try:
    import statsmodels.api as sm
    # ç»Ÿè®¡å»ºæ¨¡ä»£ç 
except ImportError:
    print("statsmodels ä¸å¯ç”¨")
```

### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š
```python
# å¿«é€Ÿæ¨¡å‹è¯„ä¼°å‡½æ•°
def quick_model_evaluation(model, X_test, y_test, problem_type='regression'):
    """å¿«é€Ÿæ¨¡å‹è¯„ä¼°"""
    y_pred = model.predict(X_test)
    
    if problem_type == 'regression':
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        print(f"RÂ²: {r2:.4f}, RMSE: {rmse:.4f}")
    else:
        accuracy = accuracy_score(y_test, y_pred)
        print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    
    return y_pred
```

**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºæœºå™¨å­¦ä¹ å»ºæ¨¡å’Œåˆ†æé€»è¾‘ï¼

## ğŸ“ æ²™ç›’ç¯å¢ƒæ–‡ä»¶æ“ä½œæŒ‡å—

### æ–‡ä»¶ä¸Šä¼ ï¼ˆå¿…é¡»æ­¥éª¤ï¼‰
åœ¨æ²™ç›’ä¸­è¿è¡Œä»£ç å‰ï¼Œ**å¿…é¡»å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶**ï¼š

```python
# ç¤ºä¾‹ï¼šå¦‚ä½•å¼•ç”¨å·²ä¸Šä¼ çš„æ–‡ä»¶
# å‡è®¾æ‚¨å·²ç»é€šè¿‡å‰ç«¯ç•Œé¢ä¸Šä¼ äº†ä»¥ä¸‹æ–‡ä»¶ï¼š
# - /data/train.csv      ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰
# - /data/dataset.xlsx   ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰
# - /data/sales.parquet  ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰

import pandas as pd
import os

def list_uploaded_files():
    """åˆ—å‡ºæ‰€æœ‰å·²ä¸Šä¼ çš„æ–‡ä»¶"""
    data_dir = '/data'
    if os.path.exists(data_dir):
        files = os.listdir(data_dir)
        print(f"å·²ä¸Šä¼ çš„æ–‡ä»¶: {files}")
        return files
    else:
        print("æ²¡æœ‰æ‰¾åˆ°/dataç›®å½•")
        return []

# åˆ—å‡ºæ–‡ä»¶
available_files = list_uploaded_files()

# è¯»å–ç‰¹å®šæ–‡ä»¶
if 'train.csv' in available_files:
    df = pd.read_csv('/data/train.csv')
    print(f"æˆåŠŸè¯»å– train.csvï¼Œå½¢çŠ¶: {df.shape}")
    
if 'dataset.xlsx' in available_files:
    df = pd.read_excel('/data/dataset.xlsx')
    print(f"æˆåŠŸè¯»å– dataset.xlsxï¼Œå½¢çŠ¶: {df.shape}")
```

### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
æ ¹æ®code_interpreter.pyï¼Œç³»ç»Ÿæ”¯æŒä»¥ä¸‹æ–‡ä»¶æ ¼å¼ï¼š
- ğŸ“Š æ•°æ®æ–‡ä»¶ï¼š`.csv`, `.xlsx`, `.xls`, `.parquet`, `.json`

### æ–‡ä»¶è¯»å–æœ€ä½³å®è·µ
```python
def safe_read_data(filename):
    """å®‰å…¨è¯»å–æ•°æ®æ–‡ä»¶ï¼Œå¸¦é”™è¯¯å¤„ç†"""
    try:
        filepath = f'/data/{filename}'
        
        # æ ¹æ®æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•
        if filename.endswith('.csv'):
            df = pd.read_csv(filepath)
        elif filename.endswith('.parquet'):
            df = pd.read_parquet(filepath)
        elif filename.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(filepath)
        elif filename.endswith('.json'):
            df = pd.read_json(filepath)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}")
        
        print(f"âœ… æˆåŠŸè¯»å– {filename}")
        print(f"   è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
        print(f"   åˆ—å: {list(df.columns)}")
        
        return df
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        print("è¯·å…ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ æ–‡ä»¶")
        return None
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ£€æŸ¥å¯ç”¨çš„æ–‡ä»¶
    files = list_uploaded_files()
    if files:
        for file in files:
            print(f"å‘ç°æ–‡ä»¶: {file}")
        
        # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶
        csv_files = [f for f in files if f.endswith('.csv')]
        if csv_files:
            df = safe_read_data(csv_files[0])
            if df is not None:
                # è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ
                pass
```

### å·¥ä½œæµæ•´åˆç¤ºä¾‹
```python
# å®Œæ•´çš„MLå·¥ä½œæµï¼ŒåŒ…å«æ–‡ä»¶æ£€æŸ¥
def complete_ml_workflow_with_file_check():
    """åŒ…å«æ–‡ä»¶æ£€æŸ¥çš„å®Œæ•´MLå·¥ä½œæµ"""
    
    print("=== æœºå™¨å­¦ä¹ å·¥ä½œæµå¼€å§‹ ===")
    
    # 1. æ£€æŸ¥æ•°æ®æ–‡ä»¶
    files = list_uploaded_files()
    if not files:
        print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        # ä½¿ç”¨generate_sample_data()å‡½æ•°åˆ›å»ºç¤ºä¾‹æ•°æ®
        from sklearn.datasets import make_regression
        X, y = make_regression(n_samples=1000, n_features=10, random_state=42)
    else:
        print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶: {files}")
        
        # è¯»å–ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶
        data_file = files[0]
        df = safe_read_data(data_file)
        
        if df is None:
            print("æ— æ³•è¯»å–æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
            from sklearn.datasets import make_regression
            X, y = make_regression(n_samples=1000, n_features=10, random_state=42)
        else:
            # å‡è®¾æœ€åä¸€åˆ—æ˜¯ç›®æ ‡å˜é‡
            X = df.iloc[:, :-1].values
            y = df.iloc[:, -1].values
    
    # 2. æ‰§è¡ŒMLåˆ†æï¼ˆä½¿ç”¨æ–‡æ¡£ä¸­çš„å‡½æ•°ï¼‰
    results = standard_ml_pipeline(X, y, problem_type='regression')
    
    return results
```

### âš¡ å¿«é€Ÿä½¿ç”¨æ¨¡æ¿
```python
# åœ¨æ²™ç›’ä¸­è¿è¡Œæœºå™¨å­¦ä¹ åˆ†æçš„å®Œæ•´ç¤ºä¾‹
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# æ­¥éª¤1ï¼šè¯»å–æ•°æ®ï¼ˆæ›¿æ¢ä¸ºæ‚¨çš„æ–‡ä»¶åï¼‰
try:
    # å¦‚æœæ‚¨ä¸Šä¼ äº†train.csv
    df = pd.read_csv('/data/train.csv')
    print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # æ­¥éª¤2ï¼šå‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
    X = df.drop('target_column', axis=1)  # æ›¿æ¢ä¸ºæ‚¨çš„ç›®æ ‡åˆ—å
    y = df['target_column']
    
    # æ­¥éª¤3ï¼šè®­ç»ƒæ¨¡å‹
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # æ­¥éª¤4ï¼šè¯„ä¼°
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    print(f"æ¨¡å‹æ€§èƒ½: RÂ²={r2:.4f}, RMSE={rmse:.4f}")
    
    # æ­¥éª¤5ï¼šå¯è§†åŒ–
    plt.figure(figsize=(10, 5))
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.xlabel('å®é™…å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')
    plt.grid(True, alpha=0.3)
    plt.show()
    
except FileNotFoundError:
    print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶ã€‚è¯·ç¡®ä¿ï¼š")
    print("   1. å·²é€šè¿‡æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ train.csv")
    print("   2. æ–‡ä»¶ä½äº/dataç›®å½•ä¸‹")
    print("   3. æ–‡ä»¶åæ‹¼å†™æ­£ç¡®")
    
    # æä¾›ç¤ºä¾‹æ•°æ®ä½œä¸ºå¤‡é€‰
    print("\nğŸ”§ æ­£åœ¨ç”Ÿæˆç¤ºä¾‹æ•°æ®è¿›è¡Œåˆ†æ...")
    from sklearn.datasets import make_regression
    X, y = make_regression(n_samples=1000, n_features=5, random_state=42)
    
    # ç»§ç»­æ‰§è¡Œåˆ†æ...
```
