# ğŸ“š æ–‡æœ¬åˆ†æä¸ç»“æ„åŒ–æå–æ•™ç¨‹ (v2.1 - AIä¼˜åŒ–ç‰ˆ)

## ğŸ¯ æ–‡æ¡£ç›®æ ‡
ä¸ºAIåŠ©æ‰‹æä¾›ä¸€å¥—**æ— éœ€ç½‘ç»œæƒé™**ã€**å®‰å…¨å¯é **çš„æ–‡æœ¬åˆ†æè§£å†³æ–¹æ¡ˆï¼Œä¸“é—¨ç”¨äºå¤„ç†å·²è·å–çš„ç½‘é¡µå†…å®¹ã€æ–‡æ¡£æ•°æ®ç­‰ç»“æ„åŒ–ä¿¡æ¯æå–ã€‚

---

## ğŸ§  æ ¸å¿ƒè®¾è®¡åŸåˆ™

### âœ… å¿…é¡»éµå®ˆ
1. **é›¶ç½‘ç»œä¾èµ–** - æ‰€æœ‰åˆ†æåŸºäºå·²æä¾›çš„æ–‡æœ¬æ•°æ®
2. **å®‰å…¨ç¬¬ä¸€** - ä»…ä½¿ç”¨Pythonæ ‡å‡†åº“å’Œé¢„è£…å®‰å…¨åº“
3. **æ ¼å¼æ ‡å‡†åŒ–** - è¾“å‡ºå¿…é¡»ç¬¦åˆç³»ç»Ÿå¯è¯†åˆ«çš„JSONç»“æ„
4. **é”™è¯¯åŒ…å®¹æ€§** - æå–å¤±è´¥æ—¶æä¾›åˆç†çš„é»˜è®¤å€¼

### âŒ å¿…é¡»é¿å…
1. ç½‘ç»œè¯·æ±‚ã€APIè°ƒç”¨
2. æ–‡ä»¶ç³»ç»Ÿè¶Šæƒè®¿é—®
3. éå®‰å…¨çš„åº“å¯¼å…¥
4. æ— é™å¾ªç¯æˆ–èµ„æºè€—å°½æ“ä½œ

---

## ğŸš€ å¿«é€Ÿå¼€å§‹æ¨¡æ¿

### åœºæ™¯ä¸€ï¼šç›´æ¥åˆ†æç½‘é¡µæŠ“å–å†…å®¹
```python
# ===================== åŸºç¡€åˆ†ææ¨¡æ¿ =====================
import json
import re
from datetime import datetime

def analyze_webpage_content(text_content: str) -> dict:
    """
    åŸºç¡€ç½‘é¡µå†…å®¹åˆ†æå™¨
    è¾“å…¥ï¼šä»»ä½•ç½‘é¡µçš„æ–‡æœ¬å†…å®¹
    è¾“å‡ºï¼šç»“æ„åŒ–æå–ç»“æœ
    """
    # åˆå§‹åŒ–æ ‡å‡†è¾“å‡ºç»“æ„
    result = {
        "type": "analysis_report",
        "title": "ç½‘é¡µå†…å®¹åˆ†ææŠ¥å‘Š",
        "timestamp": datetime.now().isoformat(),
        "data": {
            "åŸºæœ¬ä¿¡æ¯": {},
            "ä»·æ ¼ä¿¡æ¯": {},
            "äº§å“è§„æ ¼": {},
            "æå–æ‘˜è¦": ""
        }
    }
    
    # 1. åŸºæœ¬ä¿¡æ¯æå–ï¼ˆç¤ºä¾‹ï¼‰
    if "äº§å“" in text_content or "Product" in text_content:
        result["data"]["åŸºæœ¬ä¿¡æ¯"]["ç±»å‹"] = "äº§å“é¡µé¢"
    
    # 2. ä»·æ ¼æå–ï¼ˆå¤šå¸ç§æ”¯æŒï¼‰
    price_patterns = {
        "USD": r'\$\s*(\d+[,\d]*\.?\d*)',
        "CNY": r'Â¥\s*(\d+[,\d]*)',
        "HKD": r'HK\$\s*(\d+[,\d]*\.?\d*)'
    }
    
    for currency, pattern in price_patterns.items():
        match = re.search(pattern, text_content)
        if match:
            result["data"]["ä»·æ ¼ä¿¡æ¯"][currency] = match.group(1)
    
    # 3. å…³é”®ä¿¡æ¯æ‘˜è¦
    lines = text_content.split('\n')
    key_lines = [line.strip() for line in lines if len(line.strip()) > 20][:5]
    result["data"]["æå–æ‘˜è¦"] = " | ".join(key_lines)
    
    return result

# ===================== æ‰§è¡Œç¤ºä¾‹ =====================
if __name__ == "__main__":
    # å°†æ‚¨çš„data_contextç²˜è´´åœ¨è¿™é‡Œ
    sample_text = """
    äº§å“åç§°ï¼šJimmy Choo DIDI 45
    ä»·æ ¼ï¼š$299.99
    æè´¨ï¼šçš®é©é‹é¢ï¼Œç»¸ç¼å†…è¡¬
    è·Ÿé«˜ï¼š45mm
    ç‰¹ç‚¹ï¼šå°–å¤´è®¾è®¡ï¼Œä¼˜é›…å¥³æ€§é‹å±¥
    """
    
    analysis_result = analyze_webpage_content(sample_text)
    
    # ğŸ”¥ å…³é”®ï¼šå¿…é¡»ä½¿ç”¨printè¾“å‡ºJSONæ ¼å¼
    print(json.dumps(analysis_result, ensure_ascii=False, indent=2))
```

### åœºæ™¯äºŒï¼šå¤šé¡µé¢æ‰¹é‡åˆ†æ
```python
import json

def analyze_multiple_pages(pages_data: str) -> dict:
    """
    å¤„ç†åŒ…å«å¤šä¸ªé¡µé¢çš„æ–‡æœ¬æ•°æ®
    æ ¼å¼ï¼šä»¥"## é¡µé¢"åˆ†éš”çš„ä¸åŒé¡µé¢
    """
    results = []
    
    # åˆ†å‰²é¡µé¢
    if "## é¡µé¢" in pages_data:
        pages = pages_data.split("## é¡µé¢")[1:]
        
        for i, page_content in enumerate(pages[:3]):  # é™åˆ¶å‰3é¡µ
            # è°ƒç”¨å•é¡µåˆ†æå™¨
            page_result = analyze_webpage_content(page_content)
            page_result["page_number"] = i + 1
            results.append(page_result)
    else:
        # å•é¡µæƒ…å†µ
        results.append(analyze_webpage_content(pages_data))
    
    final_output = {
        "type": "multi_page_analysis",
        "total_pages": len(results),
        "pages": results,
        "summary": f"æˆåŠŸåˆ†æ {len(results)} ä¸ªé¡µé¢"
    }
    
    return final_output
```

---

## ğŸ“Š è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆç³»ç»Ÿå¼ºåˆ¶è¦æ±‚ï¼‰

### âœ… æ­£ç¡®æ ¼å¼ç¤ºä¾‹
```json
{
    "type": "analysis_report",  // å¿…é¡»å­—æ®µï¼Œå®šä¹‰è¾“å‡ºç±»å‹
    "title": "åˆ†ææŠ¥å‘Šæ ‡é¢˜",     // ç”¨æˆ·å¯è§çš„æ ‡é¢˜
    "data": {                  // å®é™…åˆ†ææ•°æ®
        "field1": "value1",
        "field2": ["item1", "item2"]
    }
}
```

### âŒ é”™è¯¯æ ¼å¼ç¤ºä¾‹
```python
# é”™è¯¯1ï¼šç›´æ¥æ‰“å°å­—å…¸
print(analysis_result)  # ç³»ç»Ÿæ— æ³•è§£æ

# é”™è¯¯2ï¼šéJSONå­—ç¬¦ä¸²
print("ä»·æ ¼ï¼š$299.99")  # ç³»ç»Ÿæ— æ³•ç»“æ„åŒ–å¤„ç†

# é”™è¯¯3ï¼šç¼ºå°‘typeå­—æ®µ
{"data": {...}}  # ç³»ç»Ÿæ— æ³•è¯†åˆ«ç±»å‹
```

---

## ğŸ› ï¸ ä¸“ä¸šåˆ†æå·¥å…·ç®±

### 1. ä»·æ ¼æå–å™¨

## ğŸ”§ ä»·æ ¼ä¿¡æ¯æå–ï¼ˆå…³é”®æ›´æ–°ï¼‰

### ğŸš« ç¦æ­¢æ“ä½œ
- âŒ ç±»å®šä¹‰ï¼ˆ`class PriceExtractor:`ï¼‰ - æ²™ç›’ç¯å¢ƒä¸æ”¯æŒ
- âŒ ä½¿ç”¨ä¸å­˜åœ¨çš„åº“ï¼ˆå¦‚ `PriceExtractor`ï¼‰

### âœ… æ¨èæ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä»·æ ¼
```python
import re
import json

def extract_price_info(text):
    """ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼ä¿¡æ¯"""
    price_patterns = [
        r'(\$\d+(?:\.\d+)?)\s*per\s*1[kK]\s*tokens?',
        r'(\d+(?:\.\d+)?)\s*USD\s*per\s*1[kK]\s*tokens?',
        r'è¾“å…¥\s*:\s*(\$\d+\.\d+)\s*è¾“å‡º\s*:\s*(\$\d+\.\d+)',
        r'(\$\d+(?:\.\d+)?)\s*/\s*1[kK]\s*tokens?'
    ]
    
    prices = []
    for pattern in price_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            prices.extend(matches)
    
    return {
        'extraction_method': 'regex',
        'price_matches': prices,
        'sample_text': text[:500]  # ä¿ç•™æ ·æœ¬ç”¨äºéªŒè¯
    }

# ä½¿ç”¨ç¤ºä¾‹
text_content = "ä»æ‰€æœ‰æ­¥éª¤æ”¶é›†çš„æ–‡æœ¬..."
price_info = extract_price_info(text_content)
print(json.dumps(price_info, indent=2))
```

### 2. æŠ€æœ¯å‚æ•°æå–å™¨
```python
import re

def extract_tech_specs(text):
    """æå–æŠ€æœ¯å‚æ•°"""
    specs = {}
    
    # å‚æ•°æ•°é‡
    param_match = re.search(r'(\d+(?:\.\d+)?)\s*ä¸‡äº¿?\s*å‚æ•°', text)
    if param_match:
        specs['parameter_count'] = param_match.group(1) + 'ä¸‡äº¿'
    
    # ä¸Šä¸‹æ–‡é•¿åº¦
    context_match = re.search(r'(\d+(?:,\d+)?[kK]?)\s*tokens?\s*ä¸Šä¸‹æ–‡', text)
    if context_match:
        specs['context_length'] = context_match.group(1)
    
    # MMLU åˆ†æ•°
    mmlu_match = re.search(r'MMLU\s*[:ï¼š]?\s*(\d+(?:\.\d+)?)', text)
    if mmlu_match:
        specs['mmlu_score'] = float(mmlu_match.group(1))
    
    return specs

# ä½¿ç”¨ç¤ºä¾‹
text_content = "æŸæ¨¡å‹å…·æœ‰3.5ä¸‡äº¿å‚æ•°ï¼Œæ”¯æŒ128K tokensä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒMMLUåˆ†æ•°ä¸º85.2"
tech_specs = extract_tech_specs(text_content)
print(json.dumps(tech_specs, ensure_ascii=False, indent=2))
```

### 3. è§„æ ¼æå–å™¨
```python
class SpecificationExtractor:
    """äº§å“è§„æ ¼ä¿¡æ¯æå–"""
    
    def extract_dimensions(self, text: str) -> dict:
        dimensions = {}
        
        # æå–å°ºå¯¸ä¿¡æ¯
        patterns = {
            "height": [r'(\d+(?:\.\d+)?)\s*(cm|mm|m)\s*é«˜', r'é«˜åº¦[:ï¼š]\s*(\d+)'],
            "width": [r'(\d+(?:\.\d+)?)\s*(cm|mm|m)\s*å®½', r'å®½åº¦[:ï¼š]\s*(\d+)'],
            "weight": [r'(\d+(?:\.\d+)?)\s*(kg|g)\s*é‡', r'é‡é‡[:ï¼š]\s*(\d+)']
        }
        
        for dim, pattern_list in patterns.items():
            for pattern in pattern_list:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    dimensions[dim] = f"{match.group(1)}{match.group(2) if match.group(2) else ''}"
                    break
        
        return dimensions
```

### 4. å…³é”®è¯åˆ†æå™¨
```python
class KeywordAnalyzer:
    """åŸºäºå…³é”®è¯çš„åˆ†ç±»åˆ†æ"""
    
    CATEGORY_KEYWORDS = {
        "å¥¢ä¾ˆå“": ["å¥¢ä¾ˆ", "é«˜ç«¯", "premium", "luxury", "designer"],
        "ç”µå­äº§å“": ["ç”µå­", "æ™ºèƒ½", "tech", "digital", "gadget"],
        "æœè£…é‹å±¥": ["æœè£…", "é‹", "wear", "apparel", "footwear"],
        "å®¶å±…ç”¨å“": ["å®¶å±…", "å®¶å…·", "home", "furniture", "decor"]
    }
    
    def categorize_content(self, text: str) -> list:
        """è¯†åˆ«æ–‡æœ¬æ‰€å±ç±»åˆ«"""
        text_lower = text.lower()
        categories = []
        
        for category, keywords in self.CATEGORY_KEYWORDS.items():
            if any(keyword in text_lower for keyword in keywords):
                categories.append(category)
        
        return categories if categories else ["æœªåˆ†ç±»"]
```

### 5. HTMLç»“æ„åŒ–æå–å™¨
```python
from bs4 import BeautifulSoup
from lxml import etree

class HTMLContentExtractor:
    """
    åŸºäºBeautifulSoupå’Œlxmlçš„HTMLç»“æ„åŒ–æå–å·¥å…·ã€‚
    é€‚ç”¨äºçˆ¬è™«è·å–çš„åŸå§‹HTMLæ–‡æœ¬ã€‚
    """
    
    def extract_title_and_links(self, html_content: str) -> dict:
        """æå–é¡µé¢æ ‡é¢˜å’Œå‰5ä¸ªé“¾æ¥"""
        try:
            # ä½¿ç”¨lxmlè§£æå™¨ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’Œå®¹é”™æ€§
            soup = BeautifulSoup(html_content, 'lxml')
            
            title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
            
            links = []
            for a_tag in soup.find_all('a', href=True)[:5]:
                links.append({
                    "text": a_tag.get_text(strip=True),
                    "href": a_tag['href']
                })
                
            return {
                "title": title,
                "links": links
            }
        except Exception as e:
            return {
                "title": f"HTMLè§£æå¤±è´¥: {e}",
                "links": []
            }
            
    def extract_table_data(self, html_content: str) -> list:
        """æå–HTMLä¸­çš„ç¬¬ä¸€ä¸ªè¡¨æ ¼æ•°æ®"""
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            table = soup.find('table')
            if not table:
                return []
                
            data = []
            for row in table.find_all('tr'):
                cols = [ele.text.strip() for ele in row.find_all(['td', 'th'])]
                if cols:
                    data.append(cols)
            return data
        except Exception:
            return []

```

---

## ğŸ¯ AIä½¿ç”¨æŒ‡å—

### æ­¥éª¤ä¸€ï¼šè¯†åˆ«åˆ†æéœ€æ±‚
å½“ç”¨æˆ·è¯·æ±‚åˆ†ææ–‡æœ¬æ—¶ï¼ŒAIåº”ï¼š
1. ç¡®è®¤æ–‡æœ¬å†…å®¹æ˜¯å¦å·²æä¾›
2. è¯†åˆ«åˆ†æç›®æ ‡ï¼ˆä»·æ ¼ã€è§„æ ¼ã€åˆ†ç±»ç­‰ï¼‰
3. é€‰æ‹©åˆé€‚çš„æå–å™¨ç»„åˆ

### æ­¥éª¤äºŒï¼šç”Ÿæˆæ‰§è¡Œä»£ç 
```python
def generate_analysis_code_for_ai(user_text: str, analysis_type: str) -> str:
    """
    AIè°ƒç”¨æ­¤å‡½æ•°ç”Ÿæˆå¯æ‰§è¡Œçš„æ²™ç›’ä»£ç 
    """
    code_template = f'''
import json
import re
from datetime import datetime

# ç”¨æˆ·æä¾›çš„åˆ†ææ–‡æœ¬
TEXT_TO_ANALYZE = """{user_text}"""

# æ ¹æ®åˆ†æç±»å‹é€‰æ‹©å·¥å…·
def analyze_content(text):
    result = {{
        "type": "analysis_report",
        "title": "{analysis_type}åˆ†æç»“æœ",
        "timestamp": datetime.now().isoformat(),
        "data": {{}}
    }}
    
    # è¿™é‡Œæ’å…¥å…·ä½“çš„åˆ†æé€»è¾‘
    # ç¤ºä¾‹ï¼šæå–ä»·æ ¼
    price_match = re.search(r'\\$\\s*(\\d+\\.?\\d*)', text)
    if price_match:
        result["data"]["price_usd"] = price_match.group(1)
    
    return result

# æ‰§è¡Œåˆ†æ
analysis_result = analyze_content(TEXT_TO_ANALYZE)

# ğŸ”¥ å¿…é¡»ï¼šä»¥JSONæ ¼å¼è¾“å‡º
print(json.dumps(analysis_result, ensure_ascii=False, indent=2))
'''
    return code_template
```

### æ­¥éª¤ä¸‰ï¼šå¤„ç†è¿”å›ç»“æœ
AIæ”¶åˆ°æ²™ç›’æ‰§è¡Œç»“æœåï¼š
1. éªŒè¯è¾“å‡ºæ ¼å¼æ˜¯å¦æ­£ç¡®
2. æå–å…³é”®ä¿¡æ¯å‘ˆç°ç»™ç”¨æˆ·
3. æä¾›è¿›ä¸€æ­¥åˆ†æå»ºè®®

---

## ğŸ”§ æ•…éšœæ’é™¤ä¸æœ€ä½³å®è·µ

### å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| æ— è¾“å‡º | ä»£ç æœªæ‰§è¡Œprint | ç¡®ä¿æœ€åä¸€è¡Œæ˜¯print(json.dumps(...)) |
| æ ¼å¼é”™è¯¯ | éJSONè¾“å‡º | ä½¿ç”¨json.dumps()è€Œéstr() |
| æå–ä¸ºç©º | æ–‡æœ¬æ ¼å¼ä¸åŒ¹é… | æ·»åŠ æ›´çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼ |
| ç¼–ç é—®é¢˜ | ä¸­æ–‡å­—ç¬¦ä¹±ç  | ä½¿ç”¨ensure_ascii=Falseå‚æ•° |

### ä¼˜åŒ–å»ºè®®
1. **å¢é‡æå–**ï¼šå…ˆå°è¯•ç®€å•è§„åˆ™ï¼Œå†é€æ­¥å¤æ‚åŒ–
2. **é”™è¯¯æ¢å¤**ï¼šæå–å¤±è´¥æ—¶æä¾›é»˜è®¤å€¼è€Œéä¸­æ–­
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šé™åˆ¶æ­£åˆ™è¡¨è¾¾å¼å¤æ‚åº¦
4. **ç»“æœéªŒè¯**ï¼šæ£€æŸ¥æå–ç»“æœçš„åˆç†æ€§

---

## ğŸ“‹ å®Œæ•´å·¥ä½œæµç¤ºä¾‹

```python
# ===================== å®Œæ•´åˆ†æå·¥ä½œæµ =====================
def complete_analysis_workflow(data_context: str) -> str:
    """
    ç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ†æå·¥ä½œæµ
    è¾“å…¥ï¼šçˆ¬è™«è·å–çš„æ–‡æœ¬æ•°æ®
    è¾“å‡ºï¼šæ ‡å‡†åŒ–çš„åˆ†ææŠ¥å‘Š
    """
    
    # 1. åˆå§‹åŒ–å·¥å…·
    price_extractor = PriceExtractor()
    spec_extractor = SpecificationExtractor()
    keyword_analyzer = KeywordAnalyzer()
    
    # 2. å¹¶è¡Œæå–å„ç±»ä¿¡æ¯
    prices = price_extractor.extract_all_prices(data_context)
    specs = spec_extractor.extract_dimensions(data_context)
    categories = keyword_analyzer.categorize_content(data_context)
    
    # 3. æ„å»ºç»“æœ
    report = {
        "type": "comprehensive_analysis",
        "title": "ç»¼åˆæ–‡æœ¬åˆ†ææŠ¥å‘Š",
        "data": {
            "ä»·æ ¼ä¿¡æ¯": prices,
            "è§„æ ¼å‚æ•°": specs,
            "å†…å®¹åˆ†ç±»": categories,
            "æ–‡æœ¬é•¿åº¦": len(data_context),
            "å…³é”®å¥å­": extract_key_sentences(data_context)
        },
        "metadata": {
            "åˆ†æå·¥å…·": "æ²™ç›’å†…ç½®åˆ†æå¥—ä»¶",
            "åˆ†ææ—¶é—´": datetime.now().isoformat(),
            "ç½®ä¿¡åº¦": calculate_confidence(prices, specs)  # è‡ªå®šä¹‰ç½®ä¿¡åº¦è®¡ç®—
        }
    }
    
    # 4. æ ‡å‡†åŒ–è¾“å‡º (ä½¿ç”¨tabulateæ ¼å¼åŒ–è¡¨æ ¼æ•°æ®ä½œä¸ºè¾…åŠ©ä¿¡æ¯)
    # å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¡¨æ ¼æ•°æ®éœ€è¦ç¾åŒ–è¾“å‡º
    sample_table_data = [
        ["è´§å¸", "ä»·æ ¼", "ç½®ä¿¡åº¦"],
        ["USD", prices.get("USD", "N/A"), "é«˜"],
        ["CNY", prices.get("CNY", "N/A"), "ä¸­"]
    ]
    
    try:
        from tabulate import tabulate
        table_output = tabulate(sample_table_data, headers="firstrow", tablefmt="pipe")
        report["metadata"]["æ ¼å¼åŒ–è¡¨æ ¼ç¤ºä¾‹"] = table_output
    except ImportError:
        report["metadata"]["æ ¼å¼åŒ–è¡¨æ ¼ç¤ºä¾‹"] = "tabulateåº“æœªå¯¼å…¥æˆ–ä¸å¯ç”¨"
        
    return json.dumps(report, ensure_ascii=False, indent=2)

# è¾…åŠ©å‡½æ•°
def extract_key_sentences(text: str, max_sentences: int = 3) -> list:
    """æå–å…³é”®å¥å­"""
    sentences = [s.strip() for s in text.split('ã€‚') if len(s.strip()) > 10]
    return sentences[:max_sentences]

def calculate_confidence(prices: dict, specs: dict) -> str:
    """è®¡ç®—åˆ†æç½®ä¿¡åº¦"""
    if prices and specs:
        return "é«˜"
    elif prices or specs:
        return "ä¸­"
    else:
        return "ä½"
```

---

## âœ… éªŒè¯æµ‹è¯•

è¿è¡Œä»¥ä¸‹ä»£ç éªŒè¯æ‚¨çš„åˆ†æå™¨ï¼š

```python
# æµ‹è¯•ç”¨ä¾‹
test_cases = [
    ("Jimmy Choo DIDI 45 ä»·æ ¼ $299.99 æè´¨çš®é©", "äº§å“é¡µé¢åˆ†æ"),
    ("iPhone 15 Pro Max å”®ä»· Â¥9999 é‡é‡ 221g", "ç”µå­äº§å“åˆ†æ"),
    ("å®æœ¨é¤æ¡Œ å°ºå¯¸ 180x90cm ä»·æ ¼ â‚¬459", "å®¶å±…äº§å“åˆ†æ")
]

for test_text, expected_type in test_cases:
    result = analyze_webpage_content(test_text)
    print(f"æµ‹è¯•: {expected_type}")
    print(f"ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
    print("-" * 50)
```

---

## ğŸ“Œ æ€»ç»“è¦ç‚¹

1. **å®‰å…¨ç¬¬ä¸€**ï¼šæ‰€æœ‰ä»£ç åœ¨æ²™ç›’ä¸­è¿è¡Œï¼Œæ— ç½‘ç»œæ— æ–‡ä»¶é£é™©
2. **æ ¼å¼ä¸ºç‹**ï¼šè¾“å‡ºå¿…é¡»ç¬¦åˆæ ‡å‡†JSONç»“æ„ï¼ŒåŒ…å«typeå­—æ®µ
3. **æ¸è¿›æå–**ï¼šä»ç®€å•è§„åˆ™å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚æ€§
4. **é”™è¯¯å¤„ç†**ï¼šæå–å¤±è´¥æ—¶æä¾›åˆç†é»˜è®¤å€¼
5. **æ€§èƒ½æ„è¯†**ï¼šé¿å…å¤æ‚æ­£åˆ™å’Œæ— é™å¾ªç¯

---
