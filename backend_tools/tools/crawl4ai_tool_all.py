import asyncio
import base64
import io
import gc
import psutil
import time
import json
from typing import Dict, Any, List, Optional, Literal
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler
from crawl4ai import CrawlerRunConfig, CacheMode
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, DFSDeepCrawlStrategy, BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter, DomainFilter, ContentTypeFilter
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter
import logging
from PIL import Image

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# 1. ä¿æŒåŸæœ‰çš„è¾“å…¥æ¨¡å‹ä¸å˜ï¼ˆç¡®ä¿æ¥å£å…¼å®¹ï¼‰
class ScrapeParams(BaseModel):
    url: str = Field(description="The URL of the page to scrape.")
    format: Literal['markdown', 'html', 'text'] = Field(default='markdown', description="Output format.")
    css_selector: Optional[str] = Field(default=None, description="CSS selector to extract specific content.")
    include_links: bool = Field(default=True, description="Whether to include links in the output.")
    include_images: bool = Field(default=True, description="Whether to include images in the output.")
    return_screenshot: bool = Field(default=False, description="Whether to return screenshot as base64.")
    return_pdf: bool = Field(default=False, description="Whether to return PDF as base64.")
    screenshot_quality: int = Field(default=70, ge=10, le=100, description="JPEG quality for screenshot (10-100).")
    screenshot_max_width: int = Field(default=1920, description="Maximum width for screenshot.")
    word_count_threshold: int = Field(default=10, description="Minimum words per content block.")
    exclude_external_links: bool = Field(default=True, description="Remove external links from content.")

class CrawlParams(BaseModel):
    url: str = Field(description="The starting URL for the crawl.")
    max_pages: int = Field(default=10, description="Maximum number of pages to crawl.")
    same_domain: bool = Field(default=True, description="Whether to only crawl same domain URLs.")
    depth: int = Field(default=2, description="Crawl depth.")
    strategy: Literal['bfs', 'dfs', 'best_first'] = Field(default='bfs', description="Crawl strategy.")
    include_external: bool = Field(default=False, description="Include external domains.")
    stream_results: bool = Field(default=False, description="Stream results as they complete.")

class DeepCrawlParams(BaseModel):
    url: str = Field(description="The starting URL for deep crawl.")
    max_depth: int = Field(default=2, description="Maximum crawl depth.")
    max_pages: int = Field(default=50, description="Maximum pages to crawl.")
    strategy: Literal['bfs', 'dfs', 'best_first'] = Field(default='bfs', description="Crawl strategy.")
    include_external: bool = Field(default=False, description="Follow external links.")
    keywords: Optional[List[str]] = Field(default=None, description="Keywords for relevance scoring.")
    url_patterns: Optional[List[str]] = Field(default=None, description="URL patterns to include.")
    stream: bool = Field(default=False, description="Stream results progressively.")

class ExtractParams(BaseModel):
    url: str = Field(description="The URL to extract structured data from.")
    schema_definition: Dict[str, Any] = Field(description="JSON schema for data extraction.")
    css_selector: Optional[str] = Field(default=None, description="Base CSS selector for extraction.")
    extraction_type: Literal['css', 'llm'] = Field(default='css', description="Extraction strategy type.")
    prompt: Optional[str] = Field(default=None, description="Prompt for LLM extraction.")

class BatchCrawlParams(BaseModel):
    urls: List[str] = Field(description="List of URLs to crawl.")
    stream: bool = Field(default=False, description="Stream results as they complete.")
    concurrent_limit: int = Field(default=3, description="Maximum concurrent crawls.")

class PdfExportParams(BaseModel):
    url: str = Field(description="The URL to export as PDF.")
    return_as_base64: bool = Field(default=True, description="Return PDF as base64 string.")

class ScreenshotParams(BaseModel):
    url: str = Field(description="The URL to capture screenshot.")
    full_page: bool = Field(default=True, description="Whether to capture full page.")
    return_as_base64: bool = Field(default=True, description="Return screenshot as base64 string.")
    quality: int = Field(default=70, ge=10, le=100, description="JPEG quality for screenshot (10-100).")
    max_width: int = Field(default=1920, description="Maximum width for screenshot.")
    max_height: int = Field(default=5000, description="Maximum height for screenshot.")

class Crawl4AIInput(BaseModel):
    mode: Literal['scrape', 'crawl', 'deep_crawl', 'extract', 'batch_crawl', 'pdf_export', 'screenshot'] = Field(
        description="The Crawl4AI function to execute."
    )
    parameters: Dict[str, Any] = Field(
        description="Parameters for the selected mode, matching the respective schema."
    )

class ScreenshotCompressor:
    """æˆªå›¾å‹ç¼©å™¨ - ä¿æŒåŸæœ‰ç»“æ„ä½†ä¸å®é™…ä½¿ç”¨"""
    
    @staticmethod
    def compress_screenshot(base64_data: str, quality: int = 70, max_width: int = 1920, max_height: int = 5000) -> str:
        """è½»é‡ç‰ˆä¸å¤„ç†æˆªå›¾ï¼Œç›´æ¥è¿”å›ç©ºå­—ç¬¦ä¸²"""
        return ""

    @staticmethod
    def get_screenshot_info(base64_data: str) -> Dict[str, Any]:
        """è½»é‡ç‰ˆä¸å¤„ç†æˆªå›¾ä¿¡æ¯"""
        return {"error": "è½»é‡ç‰ˆä¸æ”¯æŒæˆªå›¾åŠŸèƒ½"}

# 3. å®Œå…¨å…¼å®¹çš„è½»é‡çº§ Crawl4AI å·¥å…·ç±»
class EnhancedCrawl4AITool:
    name = "crawl4ai"
    description = (
        "è½»é‡çº§ç½‘é¡µæŠ“å–å·¥å…·ï¼Œä¸“ä¸ºä½å†…å­˜ç¯å¢ƒä¼˜åŒ–ã€‚æ”¯æŒæ–‡æœ¬å†…å®¹æå–ï¼Œè‡ªåŠ¨å¤„ç†å†…å­˜é™åˆ¶å’Œç½‘ç»œé”™è¯¯ã€‚"
    )
    input_schema = Crawl4AIInput

    def __init__(self):
        self.crawler = None
        self._initialized = False
        self._memory_threshold = 85  # æé«˜å†…å­˜é˜ˆå€¼
        self._max_memory_mb = 800   # é™ä½å†…å­˜é™åˆ¶
        self._browser_start_time = None
        self._max_browser_uptime = 600  # 10åˆ†é’Ÿé‡å¯
        self.compressor = ScreenshotCompressor()
        logger.info("è½»é‡çº§ Crawl4AI å·¥å…·å®ä¾‹åˆ›å»º")

    async def _check_memory_health(self) -> bool:
        """ç®€åŒ–å†…å­˜æ£€æŸ¥"""
        try:
            memory = psutil.virtual_memory()
            process = psutil.Process()
            process_memory_mb = process.memory_info().rss / 1024 / 1024
            
            logger.info(f"å†…å­˜çŠ¶æ€ - ç³»ç»Ÿ: {memory.percent}%, è¿›ç¨‹: {process_memory_mb:.1f}MB")
            
            if memory.percent > 95:
                logger.warning(f"âš ï¸ ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory.percent}%")
                return False
                
            if process_memory_mb > self._max_memory_mb:
                logger.warning(f"âš ï¸ è¿›ç¨‹å†…å­˜ä½¿ç”¨è¿‡é«˜: {process_memory_mb:.1f}MB")
                return False
                
            if (self._browser_start_time and 
                time.time() - self._browser_start_time > self._max_browser_uptime):
                logger.warning("ğŸ•’ æµè§ˆå™¨å®ä¾‹è¿è¡Œæ—¶é—´è¿‡é•¿ï¼Œéœ€è¦é‡å¯")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"å†…å­˜æ£€æŸ¥å¤±è´¥: {str(e)}")
            return True

    async def _get_memory_info(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä¿¡æ¯"""
        try:
            memory = psutil.virtual_memory()
            process = psutil.Process()
            return {
                "system_memory_percent": memory.percent,
                "system_memory_used_mb": memory.used / 1024 / 1024,
                "system_memory_total_mb": memory.total / 1024 / 1024,
                "process_memory_mb": process.memory_info().rss / 1024 / 1024,
                "browser_uptime_seconds": time.time() - self._browser_start_time if self._browser_start_time else 0,
                "lightweight_mode": True
            }
        except Exception as e:
            logger.error(f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e), "lightweight_mode": True}

    async def initialize(self):
        """è½»é‡çº§åˆå§‹åŒ–"""
        if not self._initialized:
            logger.info("ğŸš€ åˆå§‹åŒ–è½»é‡çº§çˆ¬è™«...")
            try:
                # æœ€ç®€é…ç½®ï¼Œæœ€å°åŒ–å†…å­˜ä½¿ç”¨
                self.crawler = AsyncWebCrawler(
                    browser_type="chromium",
                    headless=True,
                    verbose=False,
                    browser_args=[
                        '--disable-dev-shm-usage',
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-accelerated-2d-canvas',
                        '--no-first-run',
                        '--no-zygote',
                        '--disable-gpu',
                        '--memory-pressure-off',
                        '--window-size=1024,768',
                        '--disable-extensions',
                        '--disable-plugins',
                        '--disable-background-networking',
                        '--disable-default-apps',
                        '--disable-translate',
                        '--disable-sync'
                    ]
                )
                await self.crawler.__aenter__()
                self._browser_start_time = time.time()
                self._initialized = True
                logger.info("âœ… è½»é‡çº§çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ è½»é‡çº§çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
                self.crawler = None
                raise

    async def _restart_browser(self):
        """é‡å¯æµè§ˆå™¨å®ä¾‹"""
        logger.info("ğŸ”„ é‡å¯æµè§ˆå™¨å®ä¾‹...")
        try:
            if self.crawler:
                await self.crawler.__aexit__(None, None, None)
        except Exception as e:
            logger.error(f"å…³é—­æ—§æµè§ˆå™¨æ—¶å‡ºé”™: {e}")
        finally:
            self.crawler = None
            self._initialized = False
        
        gc.collect()
        await asyncio.sleep(2)
        
        try:
            await self.initialize()
            logger.info("âœ… æµè§ˆå™¨é‡å¯æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æµè§ˆå™¨é‡å¯å¤±è´¥: {e}")

    async def _execute_with_timeout(self, coro, timeout: int = 30):
        """å¸¦è¶…æ—¶çš„åç¨‹æ‰§è¡Œ"""
        try:
            return await asyncio.wait_for(coro, timeout=timeout)
        except asyncio.TimeoutError:
            logger.error(f"â° æ“ä½œè¶…æ—¶ ({timeout}ç§’)")
            raise
        except Exception as e:
            logger.error(f"âŒ æ“ä½œæ‰§è¡Œå¤±è´¥: {str(e)}")
            raise

    async def _scrape_single_url(self, params: ScrapeParams) -> Dict[str, Any]:
        """æŠ“å–å•ä¸ªURL - è½»é‡çº§ç‰ˆæœ¬"""
        # å†…å­˜æ£€æŸ¥
        if not await self._check_memory_health():
            return {
                "success": False, 
                "error": "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œæ— æ³•æ‰§è¡Œçˆ¬å–ä»»åŠ¡",
                "suggestion": "è¯·ç¨åé‡è¯•æˆ–ä½¿ç”¨ tavily_search è·å–æ‘˜è¦ä¿¡æ¯",
                "memory_info": await self._get_memory_info()
            }

        try:
            await self.initialize()
            if self.crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_memory_info()
                }

            # è½»é‡çº§é…ç½® - åªè·å–æ–‡æœ¬ï¼Œç¦ç”¨æ‰€æœ‰é¢å¤–åŠŸèƒ½
            config_kwargs = {
                "cache_mode": CacheMode.BYPASS,
                "css_selector": params.css_selector,
                "exclude_external_links": params.exclude_external_links,
                "exclude_external_images": not params.include_images,
                "pdf": False,  # è½»é‡ç‰ˆç¦ç”¨PDF
                "screenshot": False,  # è½»é‡ç‰ˆç¦ç”¨æˆªå›¾
                "word_count_threshold": params.word_count_threshold,
                "remove_overlay_elements": True,
                "process_iframes": False,  # ç¦ç”¨iframeå¤„ç†ä»¥èŠ‚çœå†…å­˜
                "remove_forms": True,
                "remove_scripts": True,
                "remove_styles": True
            }
            
            config = CrawlerRunConfig(**config_kwargs)
            
            logger.info(f"ğŸŒ è½»é‡æŠ“å– URL: {params.url}")
            
            result = await self._execute_with_timeout(
                crawler.arun(url=params.url, config=config),
                timeout=30
            )
            
            content = getattr(result, 'markdown', '') or getattr(result, 'cleaned_html', '')
            if not result.success or not content.strip():
                error_message = result.error_message or "æŠ“å–æˆåŠŸä½†æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬å†…å®¹ã€‚"
                logger.error(f"âŒ æŠ“å–å¤±è´¥ {params.url}: {error_message}")
                return {
                    "success": False, 
                    "error": f"æŠ“å–å¤±è´¥: {error_message}", 
                    "memory_info": await self._get_memory_info()
                }
            
            # ä¼˜åŒ–å†…å®¹é•¿åº¦
            optimized_content = self._optimize_content(content)
            
            # æ„å»ºå“åº”æ•°æ® - ä¿æŒåŸæœ‰ç»“æ„ä½†ç§»é™¤ä¸æ”¯æŒçš„åŠŸèƒ½
            output_data = {
                "success": True,
                "url": params.url,
                "content": optimized_content,
                "cleaned_html": getattr(result, 'cleaned_html', ''),
                "metadata": {
                    "title": getattr(result, 'title', ''),
                    "description": getattr(result, 'description', ''),
                    "word_count": len(optimized_content),
                    "status_code": getattr(result, 'status_code', 200),
                    "lightweight_mode": True
                },
                "memory_info": await self._get_memory_info()
            }
            
            # è½»é‡ç‰ˆä¸åŒ…å«é“¾æ¥ä¿¡æ¯
            output_data["links"] = {
                "internal": [],
                "external": []
            }
                
            # è½»é‡ç‰ˆä¸å¤„ç†æˆªå›¾å’ŒPDFï¼Œä½†ä¿æŒå­—æ®µå­˜åœ¨
            if params.return_screenshot:
                output_data["screenshot"] = {
                    "data": "",
                    "format": "base64",
                    "type": "image/jpeg",
                    "compression_info": {
                        "original": {"error": "è½»é‡ç‰ˆä¸æ”¯æŒæˆªå›¾"},
                        "compressed": {"error": "è½»é‡ç‰ˆä¸æ”¯æŒæˆªå›¾"}
                    },
                    "note": "è½»é‡ç‰ˆä¸æ”¯æŒæˆªå›¾åŠŸèƒ½ï¼Œè¯·ä½¿ç”¨å®Œæ•´ç‰ˆ"
                }
                
            if params.return_pdf:
                output_data["pdf"] = {
                    "data": "",
                    "format": "base64", 
                    "type": "application/pdf",
                    "size_bytes": 0,
                    "note": "è½»é‡ç‰ˆä¸æ”¯æŒPDFå¯¼å‡ºï¼Œè¯·ä½¿ç”¨å®Œæ•´ç‰ˆ"
                }
                
            logger.info(f"âœ… æˆåŠŸæŠ“å– {params.url}, å†…å®¹é•¿åº¦: {len(output_data['content'])}")
            return output_data
            
        except asyncio.TimeoutError:
            logger.error(f"â° æŠ“å–æ“ä½œè¶…æ—¶: {params.url}")
            return {
                "success": False, 
                "error": "æŠ“å–æ“ä½œè¶…æ—¶ï¼ˆ30ç§’ï¼‰",
                "suggestion": "ç›®æ ‡ç½‘ç«™å“åº”è¾ƒæ…¢ï¼Œè¯·å°è¯•ä½¿ç”¨ tavily_search è·å–æ‘˜è¦ä¿¡æ¯",
                "memory_info": await self._get_memory_info()
            }
        except Exception as e:
            logger.error(f"âŒ _scrape_single_url é”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._restart_browser()
            return {
                "success": False, 
                "error": f"æŠ“å–é”™è¯¯: {str(e)}",
                "suggestion": "è¯·å°è¯•ä½¿ç”¨ tavily_search è·å–æ‘˜è¦ä¿¡æ¯",
                "memory_info": await self._get_memory_info()
            }

    def _optimize_content(self, content: str) -> str:
        """ä¼˜åŒ–å†…å®¹ï¼Œç§»é™¤å†—ä½™ä¿¡æ¯"""
        if not content or len(content) < 100:
            return content
            
        # ç§»é™¤è¿‡é•¿çš„å†…å®¹
        if len(content) > 20000:
            content = content[:20000] + "\n\n[å†…å®¹è¿‡é•¿å·²ä¼˜åŒ–...]"
            
        # ç§»é™¤é‡å¤çš„ç©ºè¡Œ
        import re
        content = re.sub(r'\n\s*\n', '\n\n', content)
        
        return content

    async def _deep_crawl_website(self, params: DeepCrawlParams) -> Dict[str, Any]:
        """æ·±åº¦çˆ¬å–ç½‘ç«™ - è½»é‡ç‰ˆé™çº§ä¸ºå•é¡µé¢æŠ“å–"""
        logger.info(f"ğŸ•·ï¸ è½»é‡ç‰ˆæ·±åº¦çˆ¬å–é™çº§: {params.url}")
        
        # æ·±åº¦çˆ¬å–åœ¨è½»é‡ç‰ˆä¸­é™çº§ä¸ºå•é¡µé¢æŠ“å–
        scrape_params = ScrapeParams(url=params.url)
        result = await self._scrape_single_url(scrape_params)
        
        if result["success"]:
            return {
                "success": True,
                "crawled_pages": [
                    {
                        "url": params.url,
                        "title": result["metadata"]["title"],
                        "content": result["content"],
                        "depth": 1,
                        "score": 1.0,
                        "metadata": {
                            "word_count": len(result["content"]),
                        }
                    }
                ],
                "total_pages": 1,
                "summary": {
                    "start_url": params.url,
                    "max_depth": 1,
                    "strategy": "lightweight",
                    "pages_crawled": 1,
                    "note": "è½»é‡ç‰ˆå°†æ·±åº¦çˆ¬å–é™çº§ä¸ºå•é¡µé¢æŠ“å–"
                },
                "memory_info": await self._get_memory_info()
            }
        else:
            return {
                "success": False,
                "error": f"æ·±åº¦çˆ¬å–é™çº§å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                "suggestion": "è¯·ä½¿ç”¨ tavily_search è¿›è¡Œæ·±åº¦ä¿¡æ¯æ”¶é›†",
                "memory_info": await self._get_memory_info()
            }

    async def _batch_crawl_urls(self, params: BatchCrawlParams) -> Dict[str, Any]:
        """æ‰¹é‡çˆ¬å–å¤šä¸ªURL - è½»é‡ç‰ˆé™åˆ¶æ•°é‡"""
        logger.info(f"ğŸ”— è½»é‡ç‰ˆæ‰¹é‡çˆ¬å– {len(params.urls)} ä¸ªURL")
        
        # è½»é‡ç‰ˆé™åˆ¶æœ€å¤šå¤„ç†3ä¸ªURL
        urls_to_process = params.urls[:3]
        crawled_results = []
        successful_crawls = 0
        
        for url in urls_to_process:
            scrape_params = ScrapeParams(url=url)
            result = await self._scrape_single_url(scrape_params)
            
            if result["success"]:
                page_data = {
                    "url": result["url"],
                    "title": result["metadata"]["title"],
                    "content": result["content"],
                    "metadata": {
                        "word_count": len(result["content"]),
                        "status_code": result["metadata"]["status_code"]
                    }
                }
                crawled_results.append(page_data)
                successful_crawls += 1
            else:
                crawled_results.append({
                    "url": url,
                    "error": result.get("error", "æœªçŸ¥é”™è¯¯"),
                    "success": False
                })
                
            # æ¯ä¸ªURLä¹‹é—´çŸ­æš‚å»¶è¿Ÿ
            await asyncio.sleep(1)
        
        return {
            "success": True,
            "results": crawled_results,
            "summary": {
                "total_urls": len(params.urls),
                "successful_crawls": successful_crawls,
                "failed_crawls": len(params.urls) - successful_crawls,
                "success_rate": (successful_crawls / len(params.urls)) * 100 if params.urls else 0,
                "note": f"è½»é‡ç‰ˆåªå¤„ç†äº†å‰{len(urls_to_process)}ä¸ªURL"
            },
            "memory_info": await self._get_memory_info()
        }

    async def _extract_structured_data(self, params: ExtractParams) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–æ•°æ® - è½»é‡ç‰ˆé™çº§ä¸ºæ™®é€šæŠ“å–"""
        logger.info(f"ğŸ” è½»é‡ç‰ˆæ•°æ®æå–é™çº§: {params.url}")
        
        # ç»“æ„åŒ–æå–åœ¨è½»é‡ç‰ˆä¸­é™çº§ä¸ºæ™®é€šæŠ“å–
        scrape_params = ScrapeParams(
            url=params.url,
            css_selector=params.css_selector
        )
        result = await self._scrape_single_url(scrape_params)
        
        if result["success"]:
            return {
                "success": True, 
                "url": params.url, 
                "extracted_data": {
                    "content": result["content"],
                    "note": "è½»é‡ç‰ˆå°†ç»“æ„åŒ–æå–é™çº§ä¸ºæ™®é€šæ–‡æœ¬æŠ“å–"
                },
                "metadata": {
                    "extraction_type": "lightweight_fallback",
                    "success": True
                },
                "memory_info": await self._get_memory_info()
            }
        else:
            return {
                "success": False, 
                "error": f"æ•°æ®æå–é™çº§å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                "suggestion": "è¯·ä½¿ç”¨å®Œæ•´ç‰ˆ crawl4ai è¿›è¡Œç»“æ„åŒ–æ•°æ®æå–",
                "memory_info": await self._get_memory_info()
            }

    async def _export_pdf(self, params: PdfExportParams) -> Dict[str, Any]:
        """å¯¼å‡ºPDF - è½»é‡ç‰ˆä¸æ”¯æŒ"""
        logger.info(f"ğŸ“„ è½»é‡ç‰ˆPDFå¯¼å‡ºä¸æ”¯æŒ: {params.url}")
        
        return {
            "success": False,
            "error": "è½»é‡ç‰ˆä¸æ”¯æŒPDFå¯¼å‡ºåŠŸèƒ½",
            "suggestion": "è¯·ä½¿ç”¨å®Œæ•´ç‰ˆ crawl4ai æˆ–ç›´æ¥è®¿é—®ç½‘é¡µè·å–å†…å®¹",
            "memory_info": await self._get_memory_info()
        }

    async def _capture_screenshot(self, params: ScreenshotParams) -> Dict[str, Any]:
        """æ•è·æˆªå›¾ - è½»é‡ç‰ˆä¸æ”¯æŒ"""
        logger.info(f"ğŸ“¸ è½»é‡ç‰ˆæˆªå›¾æ•è·ä¸æ”¯æŒ: {params.url}")
        
        return {
            "success": False,
            "error": "è½»é‡ç‰ˆä¸æ”¯æŒæˆªå›¾æ•è·åŠŸèƒ½",
            "suggestion": "è¯·ä½¿ç”¨å®Œæ•´ç‰ˆ crawl4ai è¿›è¡Œæˆªå›¾",
            "memory_info": await self._get_memory_info()
        }

    async def execute(self, parameters: Crawl4AIInput) -> dict:
        """æ‰§è¡Œå·¥å…·çš„ä¸»è¦æ–¹æ³• - ä¿æŒæ¥å£å®Œå…¨å…¼å®¹"""
        try:
            mode = parameters.mode
            params = parameters.parameters

            logger.info(f"ğŸš€ æ‰§è¡Œ Crawl4AI è½»é‡ç‰ˆæ¨¡å¼: {mode}")

            # å†…å­˜æ£€æŸ¥
            if not await self._check_memory_health():
                return {
                    "success": False, 
                    "error": "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œæ— æ³•æ‰§è¡Œçˆ¬å–ä»»åŠ¡",
                    "suggestion": "è¯·ç¨åé‡è¯•æˆ–ä½¿ç”¨ tavily_search è·å–æ‘˜è¦ä¿¡æ¯",
                    "memory_info": await self._get_memory_info()
                }

            # ç¡®ä¿æµè§ˆå™¨å·²åˆå§‹åŒ–
            await self.initialize()

            if mode == 'scrape':
                validated_params = ScrapeParams(**params)
                result = await self._scrape_single_url(validated_params)
                
            elif mode == 'deep_crawl':
                validated_params = DeepCrawlParams(**params)
                result = await self._deep_crawl_website(validated_params)
                
            elif mode == 'batch_crawl':
                validated_params = BatchCrawlParams(**params)
                result = await self._batch_crawl_urls(validated_params)
                
            elif mode == 'extract':
                validated_params = ExtractParams(**params)
                result = await self._extract_structured_data(validated_params)
                
            elif mode == 'pdf_export':
                validated_params = PdfExportParams(**params)
                result = await self._export_pdf(validated_params)
                
            elif mode == 'screenshot':
                validated_params = ScreenshotParams(**params)
                result = await self._capture_screenshot(validated_params)
                
            else:
                logger.error(f"âŒ æ— æ•ˆçš„æ¨¡å¼è¯·æ±‚: {mode}")
                return {
                    "success": False, 
                    "error": f"æ— æ•ˆçš„æ¨¡å¼ '{mode}'.",
                    "memory_info": await self._get_memory_info()
                }

            return result

        except Exception as e:
            logger.error(f"âŒ Crawl4AI è½»é‡ç‰ˆå·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}")
            return {
                "success": False, 
                "error": f"å‘ç”Ÿé”™è¯¯: {str(e)}",
                "suggestion": "è¯·å°è¯•ä½¿ç”¨ tavily_search è·å–æ‘˜è¦ä¿¡æ¯",
                "memory_info": await self._get_memory_info()
            }

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.crawler:
            try:
                logger.info("ğŸ”š å…³é—­è½»é‡çº§çˆ¬è™«å®ä¾‹...")
                await self.crawler.__aexit__(None, None, None)
                self.crawler = None
                self._initialized = False
                self._browser_start_time = None
                
                gc.collect()
                logger.info("âœ… è½»é‡çº§çˆ¬è™«å®ä¾‹å…³é—­æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ å…³é—­è½»é‡çº§çˆ¬è™«æ—¶å‡ºé”™: {str(e)}")
                self.crawler = None
                self._initialized = False