import asyncio
import base64
import io
import gc
import psutil
import time
import json
from typing import Dict, Any, List, Optional, Literal
import random
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler
from crawl4ai import CrawlerRunConfig, CacheMode
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, DFSDeepCrawlStrategy, BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter, DomainFilter, ContentTypeFilter
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter
import logging
from PIL import Image

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# 1. æ‰©å±•è¾“å…¥æ¨¡å‹ä»¥æ”¯æŒæ–°åŠŸèƒ½
class ScrapeParams(BaseModel):
    url: str = Field(description="The URL of the page to scrape.")
    format: Literal['markdown', 'html', 'text'] = Field(default='markdown', description="Output format.")
    css_selector: Optional[str] = Field(default=None, description="CSS selector to extract specific content.")
    include_links: bool = Field(default=True, description="Whether to include links in the output.")
    include_images: bool = Field(default=True, description="Whether to include images in the output.")
    return_screenshot: bool = Field(default=False, description="Whether to return screenshot as base64.")
    return_pdf: bool = Field(default=False, description="Whether to return PDF as base64.")
    screenshot_quality: int = Field(default=70, ge=10, le=100, description="JPEG quality for screenshot (10-100).")
    screenshot_max_width: int = Field(default=1920, description="Maximum width for screenshot.")
    word_count_threshold: int = Field(default=10, description="Minimum words per content block.")
    exclude_external_links: bool = Field(default=True, description="Remove external links from content.")

class CrawlParams(BaseModel):
    url: str = Field(description="The starting URL for the crawl.")
    max_pages: int = Field(default=10, description="Maximum number of pages to crawl.")
    same_domain: bool = Field(default=True, description="Whether to only crawl same domain URLs.")
    depth: int = Field(default=2, description="Crawl depth.")
    strategy: Literal['bfs', 'dfs', 'best_first'] = Field(default='bfs', description="Crawl strategy.")
    include_external: bool = Field(default=False, description="Include external domains.")
    stream_results: bool = Field(default=False, description="Stream results as they complete.")

class DeepCrawlParams(BaseModel):
    url: str = Field(description="The starting URL for deep crawl.")
    max_depth: int = Field(default=3, description="Maximum crawl depth. (ä» 2 å¢åŠ åˆ° 3)")
    max_pages: int = Field(default=80, description="Maximum pages to crawl. (ä» 50 å¢åŠ åˆ° 80)")
    strategy: Literal['bfs', 'dfs', 'best_first'] = Field(default='bfs', description="Crawl strategy.")
    include_external: bool = Field(default=False, description="Follow external links.")
    keywords: Optional[List[str]] = Field(default=None, description="Keywords for relevance scoring.")
    url_patterns: Optional[List[str]] = Field(default=None, description="URL patterns to include.")
    stream: bool = Field(default=False, description="Stream results progressively.")

class ExtractParams(BaseModel):
    url: str = Field(description="The URL to extract structured data from.")
    schema_definition: Dict[str, Any] = Field(description="JSON schema for data extraction.")
    css_selector: Optional[str] = Field(default=None, description="Base CSS selector for extraction.")
    extraction_type: Literal['css', 'llm'] = Field(default='css', description="Extraction strategy type.")
    prompt: Optional[str] = Field(default=None, description="Prompt for LLM extraction.")

class BatchCrawlParams(BaseModel):
    urls: List[str] = Field(description="List of URLs to crawl.")
    stream: bool = Field(default=False, description="Stream results as they complete.")
    concurrent_limit: int = Field(default=4, description="Maximum concurrent crawls.")

class PdfExportParams(BaseModel):
    url: str = Field(description="The URL to export as PDF.")
    return_as_base64: bool = Field(default=True, description="Return PDF as base64 string.")

class ScreenshotParams(BaseModel):
    url: str = Field(description="The URL to capture screenshot.")
    full_page: bool = Field(default=True, description="Whether to capture full page.")
    return_as_base64: bool = Field(default=True, description="Return screenshot as base64 string.")
    quality: int = Field(default=70, ge=10, le=100, description="JPEG quality for screenshot (10-100).")
    max_width: int = Field(default=1920, description="Maximum width for screenshot.")
    max_height: int = Field(default=5000, description="Maximum height for screenshot.")

# 2. æ‰©å±•æ€»çš„å·¥å…·è¾“å…¥æ¨¡å‹
class Crawl4AIInput(BaseModel):
    mode: Literal['scrape', 'crawl', 'deep_crawl', 'extract', 'batch_crawl', 'pdf_export', 'screenshot'] = Field(
        description="The Crawl4AI function to execute."
    )
    parameters: Dict[str, Any] = Field(
        description="Parameters for the selected mode, matching the respective schema."
    )

class ScreenshotCompressor:
    """æˆªå›¾å‹ç¼©å™¨"""
    
    @staticmethod
    def compress_screenshot(base64_data: str, quality: int = 70, max_width: int = 1920, max_height: int = 5000) -> str:
        """å‹ç¼©base64æ ¼å¼çš„æˆªå›¾"""
        try:
            image_data = base64.b64decode(base64_data)
            
            with Image.open(io.BytesIO(image_data)) as img:
                original_format = img.format
                original_size = img.size
                
                if img.size[0] > max_width or img.size[1] > max_height:
                    img.thumbnail((max_width, max_height), Image.Resampling.LANCZOS)
                    logger.info(f"Resized screenshot from {original_size} to {img.size}")
                
                if img.mode in ('RGBA', 'LA'):
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    background.paste(img, mask=img.split()[-1])
                    img = background
                elif img.mode != 'RGB':
                    img = img.convert('RGB')
                
                output_buffer = io.BytesIO()
                img.save(output_buffer, format='JPEG', quality=quality, optimize=True)
                compressed_data = output_buffer.getvalue()
                
                compressed_base64 = base64.b64encode(compressed_data).decode('utf-8')
                
                original_size_kb = len(image_data) // 1024
                compressed_size_kb = len(compressed_data) // 1024
                compression_ratio = (1 - len(compressed_data) / len(image_data)) * 100
                
                logger.info(f"Screenshot compressed: {original_size_kb}KB -> {compressed_size_kb}KB "
                           f"({compression_ratio:.1f}% reduction)")
                
                return compressed_base64
                
        except Exception as e:
            logger.error(f"Screenshot compression failed: {str(e)}")
            return base64_data

    @staticmethod
    def get_screenshot_info(base64_data: str) -> Dict[str, Any]:
        """è·å–æˆªå›¾ä¿¡æ¯"""
        try:
            image_data = base64.b64decode(base64_data)
            with Image.open(io.BytesIO(image_data)) as img:
                return {
                    "format": img.format,
                    "size": img.size,
                    "mode": img.mode,
                    "data_size_kb": len(image_data) // 1024
                }
        except Exception as e:
            logger.error(f"Failed to get screenshot info: {str(e)}")
            return {"error": str(e)}

# 3. ä¼˜åŒ–å†…å­˜ç®¡ç†çš„ Crawl4AI å·¥å…·ç±»
class EnhancedCrawl4AITool:
    name = "crawl4ai"
    description = (
        "A powerful open-source tool to scrape, crawl, extract structured data, export PDFs, and capture screenshots from web pages. "
        "Supports deep crawling with multiple strategies (BFS, DFS, BestFirst), batch URL processing, AI-powered extraction, "
        "and advanced content filtering. All outputs are returned as memory streams (base64 for binary data)."
    )
    input_schema = Crawl4AIInput

    def __init__(self):
        self.crawler = None
        self._initialized = False
        self._task_count = 0
        self._cleanup_interval = 10  # å»¶é•¿æ¸…ç†é—´éš” (ä» 5 å¢åŠ åˆ° 10)
        self._memory_threshold = 80  # æé«˜å†…å­˜é˜ˆå€¼
        self._max_memory_mb = 6000   # å¢åŠ å†…å­˜é™åˆ¶ (ä» 1500 æå‡è‡³ 6000MB)
        self._browser_start_time = None
        self._max_browser_uptime = 1800 # å»¶é•¿æµè§ˆå™¨æœ€å¤§è¿è¡Œæ—¶é—´ (ä» 1200 å¢åŠ åˆ° 1800)
        self._last_memory_check = 0
        self._memory_check_interval = 90 # å»¶é•¿å†…å­˜æ£€æŸ¥é—´éš” (ä» 60 å¢åŠ åˆ° 90ç§’)
        self._browser_lock = asyncio.Lock()
        self.compressor = ScreenshotCompressor()
        logger.info("EnhancedCrawl4AITool instance created")

    async def _check_memory_health(self) -> bool:
        """æ£€æŸ¥ç³»ç»Ÿå†…å­˜å¥åº·çŠ¶æ€ - ä¼˜åŒ–ç‰ˆæœ¬"""
        current_time = time.time()
        
        # å‡å°‘å†…å­˜æ£€æŸ¥é¢‘ç‡
        if current_time - self._last_memory_check < self._memory_check_interval:
            return True
            
        self._last_memory_check = current_time
        
        try:
            memory = psutil.virtual_memory()
            process = psutil.Process()
            process_memory_mb = process.memory_info().rss / 1024 / 1024
            
            logger.info(f"å†…å­˜çŠ¶æ€ - ç³»ç»Ÿ: {memory.percent}%, è¿›ç¨‹: {process_memory_mb:.1f}MB")
            
            # åªæœ‰åœ¨å†…å­˜ä½¿ç”¨ç‡éå¸¸é«˜æ—¶æ‰è¿›è¡Œæ¸…ç†
            if memory.percent > 95:  # ç´§æ€¥æƒ…å†µé˜ˆå€¼
                logger.warning(f"âš ï¸ ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory.percent}%")
                return False
                
            if process_memory_mb > self._max_memory_mb:
                logger.warning(f"âš ï¸ è¿›ç¨‹å†…å­˜ä½¿ç”¨è¿‡é«˜: {process_memory_mb:.1f}MB")
                return False
                
            if (self._browser_start_time and 
                current_time - self._browser_start_time > self._max_browser_uptime):
                logger.warning("ğŸ•’ æµè§ˆå™¨å®ä¾‹è¿è¡Œæ—¶é—´è¿‡é•¿")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"å†…å­˜æ£€æŸ¥å¤±è´¥: {str(e)}")
            return True

    async def _get_system_memory_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯"""
        try:
            memory = psutil.virtual_memory()
            process = psutil.Process()
            return {
                "system_memory_percent": memory.percent,
                "system_memory_used_mb": memory.used / 1024 / 1024,
                "system_memory_total_mb": memory.total / 1024 / 1024,
                "process_memory_mb": process.memory_info().rss / 1024 / 1024,
                "browser_uptime_seconds": time.time() - self._browser_start_time if self._browser_start_time else 0
            }
        except Exception as e:
            logger.error(f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    async def initialize(self):
        """åˆå§‹åŒ–æµè§ˆå™¨å®ä¾‹"""
        async with self._browser_lock:
            if not self._initialized:
                logger.info("ğŸš€ åˆå§‹åŒ– crawl4ai æµè§ˆå™¨...")
                await self._create_crawler()
                self._initialized = True
                logger.info("âœ… crawl4ai æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")

    async def _create_crawler(self):
        """åˆ›å»ºæ–°çš„çˆ¬è™«å®ä¾‹ - å‡çº§ç‰ˆ"""
        logger.info("ğŸ†• åˆ›å»ºæ–°çš„ AsyncWebCrawler å®ä¾‹...")
        try:
            # å¢å¼ºæµè§ˆå™¨å‚æ•°ï¼Œæå‡åçˆ¬èƒ½åŠ›
            browser_args = [
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-accelerated-2d-canvas',
                '--no-first-run',
                '--no-zygote',
                '--single-process',
                '--disable-gpu',
                '--memory-pressure-off',
                '--window-size=1280,720',
                # æ–°å¢åçˆ¬å‚æ•°
                '--disable-blink-features=AutomationControlled',
                '--disable-features=IsolateOrigins,site-per-process',
                '--disable-web-security',
                '--disable-site-isolation-trials',
                '--disable-3d-apis',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-breakpad',
                '--disable-component-extensions-with-background-pages',
                '--disable-domain-reliability',
                '--disable-extensions',
                '--disable-features=AudioServiceOutOfProcess',
                '--disable-hang-monitor',
                '--disable-ipc-flooding-protection',
                '--disable-notifications',
                '--disable-renderer-backgrounding',
                '--disable-sync',
                '--force-color-profile=srgb',
                '--metrics-recording-only',
                '--mute-audio',
                '--no-default-browser-check',
                '--no-pings',
                '--password-store=basic',
                '--use-mock-keychain',
                '--disable-popup-blocking',
                '--disable-default-apps',
                '--disable-client-side-phishing-detection',
                '--disable-component-update',
                '--disable-speech-api',
                '--hide-scrollbars',
                '--disable-logging',
                '--disable-dev-tools',
            ]
            
            # éšæœºUser-Agentå’ŒViewport
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36",
            ]
            
            self.crawler = AsyncWebCrawler(
                browser_type="chromium",
                headless=True,
                verbose=False,
                browser_args=browser_args,
                # æ–°å¢éšæœºåŒ–é…ç½®
                user_agent=random.choice(user_agents),
                viewport={"width": random.randint(1280, 1920),
                         "height": random.randint(720, 1080)},
                # å¢å¼ºJSæ‰§è¡Œç¯å¢ƒ
                java_script_enabled=True,
                ignore_https_errors=True,
                # å¯ç”¨éšèº«æ¨¡å¼ï¼Œå‡å°‘æŒ‡çº¹è¿½è¸ª
                incognito=True,
            )
            await self.crawler.__aenter__()
            self._browser_start_time = time.time()
            logger.info("âœ… AsyncWebCrawler å®ä¾‹åˆ›å»ºå¹¶å¯åŠ¨ï¼ˆå¢å¼ºæ¨¡å¼ï¼‰")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºçˆ¬è™«å®ä¾‹å¤±è´¥: {e}")
            self.crawler = None
            raise

    async def _get_crawler(self):
        """è·å–çˆ¬è™«å®ä¾‹ï¼Œç¡®ä¿ä¸ä¸ºNone"""
        async with self._browser_lock:
            if self.crawler is None:
                logger.warning("ğŸ”„ çˆ¬è™«å®ä¾‹ä¸ºNoneï¼Œé‡æ–°åˆ›å»º...")
                await self._create_crawler()
            return self.crawler

    async def _handle_browser_crash(self, error: Exception):
        """å¤„ç†æµè§ˆå™¨å´©æºƒ"""
        logger.error(f"ğŸ”„ æµè§ˆå™¨å´©æºƒï¼Œå°è¯•æ¢å¤: {str(error)}")
        async with self._browser_lock:
            if self.crawler:
                try:
                    await self.crawler.__aexit__(None, None, None)
                except:
                    pass
            self.crawler = None
            self._initialized = False
            self._browser_start_time = None
        
        gc.collect()
        await asyncio.sleep(2)
        
        try:
            await self.initialize()
            logger.info("âœ… æµè§ˆå™¨å´©æºƒæ¢å¤æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æµè§ˆå™¨å´©æºƒæ¢å¤å¤±è´¥: {e}")
            raise

    async def _cleanup_after_task(self):
        """ä»»åŠ¡åæ¸…ç†é¡µé¢èµ„æº - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # âœ… 1. ä¸»åŠ¨æ¸…ç†ï¼šæ¯æ¬¡éƒ½å°è¯•å…³é—­å¤šä½™é¡µé¢
            crawler = await self._get_crawler()
            if crawler and hasattr(crawler, 'browser') and crawler.browser and crawler.browser.is_connected():
                try:
                    pages = await crawler.browser.pages()
                    # ä¿ç•™ç¬¬ä¸€ä¸ªé¡µé¢ï¼ˆé€šå¸¸æ˜¯ about:blankï¼‰ï¼Œå…³é—­å…¶ä»–æ‰€æœ‰é¡µé¢
                    if len(pages) > 1:
                        for page in pages[1:]:
                            await page.close()
                except Exception as e:
                    logger.warning(f"æ¸…ç†é¡µé¢æ—¶å‡ºé”™: {e}")

            # âœ… 2. å®šæœŸæ·±åº¦æ£€æŸ¥ï¼šè¾¾åˆ°æ¸…ç†é—´éš”æ—¶æ‰æ£€æŸ¥å†…å­˜
            if self._task_count % self._cleanup_interval == 0:
                if not await self._check_memory_health():
                    logger.warning("å†…å­˜å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œæ‰§è¡Œå¼ºåˆ¶æ¸…ç†ã€‚")
                    await self._force_memory_cleanup()
            
            gc.collect()
            
        except Exception as e:
            logger.warning(f"ä»»åŠ¡åæ¸…ç†å‡ºç°è­¦å‘Š: {e}")

    async def _force_memory_cleanup(self):
        """å¼ºåˆ¶å†…å­˜æ¸…ç† - é‡å¯æµè§ˆå™¨å®ä¾‹"""
        async with self._browser_lock:
            if self.crawler:
                logger.info("ğŸ”„ æ‰§è¡Œå¼ºåˆ¶å†…å­˜æ¸…ç† - é‡å¯æµè§ˆå™¨å®ä¾‹")
                try:
                    await self.crawler.__aexit__(None, None, None)
                except Exception as e:
                    logger.error(f"å…³é—­æ—§æµè§ˆå™¨å®ä¾‹æ—¶å‡ºé”™: {e}")
                finally:
                    self.crawler = None
                    self._initialized = False
                    self._browser_start_time = None
            
            gc.collect()
            await asyncio.sleep(1) # çŸ­æš‚ç­‰å¾…èµ„æºé‡Šæ”¾
            
            # âœ… 3. é‡å»ºå®ä¾‹ï¼šæ¸…ç†åç«‹å³é‡å»ºï¼Œç¡®ä¿ä¸‹ä¸€æ¬¡è°ƒç”¨å¯ç”¨
            try:
                await self.initialize()
                logger.info("âœ… æµè§ˆå™¨å®ä¾‹é‡å¯å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ é‡å¯æµè§ˆå™¨å®ä¾‹å¤±è´¥: {e}")

    async def _execute_with_timeout(self, coro, timeout: int = 60):
        """å¸¦è¶…æ—¶çš„åç¨‹æ‰§è¡Œ"""
        try:
            return await asyncio.wait_for(coro, timeout=timeout)
        except asyncio.TimeoutError:
            logger.error(f"â° æ“ä½œè¶…æ—¶ ({timeout}ç§’)")
            raise
        except Exception as e:
            logger.error(f"âŒ æ“ä½œæ‰§è¡Œå¤±è´¥: {str(e)}")
            raise

    async def _scrape_single_url(self, params: ScrapeParams) -> Dict[str, Any]:
        """æŠ“å–å•ä¸ªURL - ä½¿ç”¨æ–‡æ¡£æ¨èçš„æœ€ä½³å®è·µ"""
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # ä½¿ç”¨æ–‡æ¡£æ¨èçš„ CrawlerRunConfig é…ç½®
            config_kwargs = {
                "cache_mode": CacheMode.BYPASS,
                "css_selector": params.css_selector,
                "exclude_external_links": params.exclude_external_links,
                "exclude_external_images": not params.include_images,
                "pdf": params.return_pdf,
                "screenshot": params.return_screenshot,
                "word_count_threshold": params.word_count_threshold,
                "remove_overlay_elements": True,
                "process_iframes": True
            }
            
            config = CrawlerRunConfig(**config_kwargs)
            
            logger.info(f"ğŸŒ æŠ“å– URL: {params.url}")
            
            result = await self._execute_with_timeout(
                crawler.arun(url=params.url, config=config),
                timeout=90 # ç¼©çŸ­è¶…æ—¶æ—¶é—´è‡³ 90 ç§’ï¼Œä»¥é¿å… Cloudflare 524 é”™è¯¯
            )
            
            # ğŸ¯ æ ¸å¿ƒä¿®å¤ï¼šå¢åŠ å¯¹ç»“æœå’Œå†…å®¹çš„åŒé‡æ£€æŸ¥
            content = getattr(result, 'markdown', '') or getattr(result, 'cleaned_html', '')
            if not result.success or not content.strip():
                error_message = result.error_message or "æŠ“å–æˆåŠŸä½†æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬å†…å®¹ã€‚"
                logger.error(f"âŒ æŠ“å–å¤±è´¥ {params.url}: {error_message}")
                return {"success": False, "error": f"æŠ“å–å¤±è´¥: {error_message}", "memory_info": await self._get_system_memory_info()}
            
            # æ„å»ºå“åº”æ•°æ®
            output_data = {
                "success": True,
                "url": params.url,
                "content": content, # ä½¿ç”¨å·²æ ¡éªŒçš„å†…å®¹
                "cleaned_html": getattr(result, 'cleaned_html', ''),
                "metadata": {
                    "title": getattr(result, 'title', ''),
                    "description": getattr(result, 'description', ''),
                    "word_count": len(content),
                    "status_code": getattr(result, 'status_code', 200)
                },
                "memory_info": await self._get_system_memory_info()
            }
            
            # æ·»åŠ é“¾æ¥ä¿¡æ¯
            if hasattr(result, 'links'):
                output_data["links"] = {
                    "internal": getattr(result, 'internal_links', []),
                    "external": getattr(result, 'external_links', [])
                }
                
            # æ·»åŠ æˆªå›¾ï¼ˆå¸¦å‹ç¼©ï¼‰
            if params.return_screenshot and hasattr(result, 'screenshot') and result.screenshot:
                compressed_screenshot = self.compressor.compress_screenshot(
                    result.screenshot,
                    quality=params.screenshot_quality,
                    max_width=params.screenshot_max_width
                )
                
                original_info = self.compressor.get_screenshot_info(result.screenshot)
                compressed_info = self.compressor.get_screenshot_info(compressed_screenshot)
                
                output_data["screenshot"] = {
                    "data": compressed_screenshot,
                    "format": "base64",
                    "type": "image/jpeg",
                    "compression_info": {
                        "original": original_info,
                        "compressed": compressed_info
                    }
                }
                
            # æ·»åŠ PDF
            if params.return_pdf and hasattr(result, 'pdf') and result.pdf:
                pdf_base64 = base64.b64encode(result.pdf).decode('utf-8')
                output_data["pdf"] = {
                    "data": pdf_base64,
                    "format": "base64",
                    "type": "application/pdf",
                    "size_bytes": len(result.pdf)
                }
                
            logger.info(f"âœ… æˆåŠŸæŠ“å– {params.url}, å†…å®¹é•¿åº¦: {len(output_data['content'])}")
            return output_data
            
        except asyncio.TimeoutError:
            logger.error(f"â° æŠ“å–æ“ä½œè¶…æ—¶: {params.url}")
            return {
                "success": False,
                "error": "æŠ“å–æ“ä½œè¶…æ—¶ï¼ˆ90ç§’ï¼‰",
                "memory_info": await self._get_system_memory_info()
            }
        except Exception as e:
            logger.error(f"âŒ _scrape_single_url é”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False, 
                "error": f"æŠ“å–é”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def _deep_crawl_website(self, params: DeepCrawlParams) -> Dict[str, Any]:
        """æ·±åº¦çˆ¬å–ç½‘ç«™ - ä¿®å¤Contexté”™è¯¯ç‰ˆæœ¬"""
        logger.info(f"ğŸ•·ï¸ å¼€å§‹æ·±åº¦ç½‘ç«™çˆ¬å–: {params.url}, æ·±åº¦: {params.max_depth}, æœ€å¤§é¡µé¢: {params.max_pages}")
        
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # æ ¹æ®ç­–ç•¥é€‰æ‹©æ·±åº¦çˆ¬å–æ–¹æ³•
            if params.strategy == 'bfs':
                deep_crawl_strategy = BFSDeepCrawlStrategy(
                    max_depth=params.max_depth,
                    include_external=params.include_external,
                    max_pages=params.max_pages
                )
            elif params.strategy == 'dfs':
                deep_crawl_strategy = DFSDeepCrawlStrategy(
                    max_depth=params.max_depth,
                    include_external=params.include_external,
                    max_pages=params.max_pages
                )
            elif params.strategy == 'best_first':
                # ä¸ºBestFirstç­–ç•¥æ·»åŠ å…³é”®è¯è¯„åˆ†å™¨
                scorer = None
                if params.keywords:
                    scorer = KeywordRelevanceScorer(
                        keywords=params.keywords,
                        weight=0.7
                    )
                
                deep_crawl_strategy = BestFirstCrawlingStrategy(
                    max_depth=params.max_depth,
                    include_external=params.include_external,
                    max_pages=params.max_pages,
                    url_scorer=scorer
                )
            else:
                deep_crawl_strategy = BFSDeepCrawlStrategy(
                    max_depth=params.max_depth,
                    include_external=params.include_external,
                    max_pages=params.max_pages
                )
            
            # æ„å»ºè¿‡æ»¤å™¨é“¾ï¼ˆå¦‚æœæä¾›äº†URLæ¨¡å¼ï¼‰
            filter_chain = None
            if params.url_patterns:
                url_filter = URLPatternFilter(patterns=params.url_patterns)
                filter_chain = FilterChain([url_filter])
                deep_crawl_strategy.filter_chain = filter_chain
            
            # ğŸ¯ å¼ºåˆ¶å¯ç”¨æµå¼æ¨¡å¼ï¼Œä»¥ä¾¿åœ¨çˆ¬å–è¿‡ç¨‹ä¸­å‘é€è¿›åº¦æ›´æ–°ï¼Œé¿å… Cloudflare 524 é”™è¯¯
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                deep_crawl_strategy=deep_crawl_strategy,
                scraping_strategy=LXMLWebScrapingStrategy(),
                stream=True,  # å¼ºåˆ¶æµå¼
                verbose=False # å‡å°‘è¯¦ç»†æ—¥å¿—
            )
            
            crawled_pages = []
            total_pages = 0
            progress_report_count = 0
            start_time = time.time()
            
            # ğŸ¯ å‘é€å¼€å§‹è¿›åº¦
            logger.info(f"DeepCrawl Started: {params.url}")
            
            # ğŸ¯ ä½¿ç”¨æµå¼å¤„ç†æ”¶é›†ç»“æœ
            # ä½¿ç”¨ _execute_with_timeout åŒ…è£… arunï¼Œä»¥ç¡®ä¿æ•´ä¸ªæµå¼æ“ä½œåœ¨ 400 ç§’å†…å®Œæˆ
            try:
                # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šæ•è·ContextVaré”™è¯¯
                async for result in await self._execute_with_timeout(
                    crawler.arun(params.url, config=config),
                    timeout=400
                ):
                    if result.success:
                        page_data = {
                            "url": result.url,
                            "title": getattr(result, 'title', ''),
                            "content": getattr(result, 'markdown', ''),
                            "depth": result.metadata.get('depth', 0),
                            "score": result.metadata.get('score', 0),
                            "metadata": {
                                "word_count": len(getattr(result, 'markdown', '')),
                            }
                        }
                        crawled_pages.append(page_data)
                        total_pages += 1
                        progress_report_count += 1
                        
                        # ğŸ¯ å®šæœŸå‘é€è¿›åº¦å¿ƒè·³ï¼ˆæ¯2ä¸ªé¡µé¢æˆ–æ¯30ç§’ï¼‰
                        current_time = time.time()
                        if progress_report_count % 2 == 0 or (current_time - start_time) > 30:
                            elapsed = current_time - start_time
                            logger.info(f"DeepCrawl Progress: Crawled {total_pages} pages in {elapsed:.1f}s")
                            start_time = current_time  # é‡ç½®è®¡æ—¶å™¨
                        
                        # å®‰å…¨é™åˆ¶
                        if total_pages >= params.max_pages:
                            logger.info(f"DeepCrawl Max Pages limit reached: {params.max_pages}")
                            break
                            
            except ValueError as e:
                if "was created in a different Context" in str(e):
                    logger.warning(f"âš ï¸ æ•è·åˆ°Crawl4AIå†…éƒ¨Contexté”™è¯¯ï¼Œå·²æˆåŠŸè·å–{total_pages}ä¸ªé¡µé¢")
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè¿”å›å·²æ”¶é›†çš„ç»“æœ
                else:
                    raise e
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸æ­£å¸¸å¤„ç†
                logger.error(f"æ·±åº¦çˆ¬å–è¿­ä»£é”™è¯¯: {str(e)}")
                raise
            
            logger.info(f"âœ… DeepCrawl Completed: {total_pages} pages")
            
            return {
                "success": True,
                "crawled_pages": crawled_pages,
                "total_pages": total_pages,
                "summary": {
                    "start_url": params.url,
                    "max_depth": params.max_depth,
                    "strategy": params.strategy,
                    "pages_crawled": total_pages
                },
                "memory_info": await self._get_system_memory_info()
            }
            
        except asyncio.TimeoutError:
            logger.warning(f"â° DeepCrawl Timeout, returning {total_pages} pages as partial result.")
            return {
                "success": True, # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿè¿”å›éƒ¨åˆ†ç»“æœ
                "error": f"æ·±åº¦çˆ¬å–æ“ä½œè¶…æ—¶ï¼ˆ400ç§’ï¼‰ï¼Œè¿”å›éƒ¨åˆ†ç»“æœ ({total_pages} é¡µ)",
                "crawled_pages": crawled_pages,
                "total_pages": total_pages,
                "partial_result": True,
                "memory_info": await self._get_system_memory_info()
            }
        except Exception as e:
            logger.error(f"âŒ æ·±åº¦çˆ¬å–é”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False, 
                "error": f"æ·±åº¦çˆ¬å–é”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def _batch_crawl_urls(self, params: BatchCrawlParams) -> Dict[str, Any]:
        """æ‰¹é‡çˆ¬å–å¤šä¸ªURL - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info(f"ğŸ”— å¼€å§‹æ‰¹é‡çˆ¬å– {len(params.urls)} ä¸ªURL")
        
        # ğŸ¯ æ•´ä½“è¶…æ—¶æ§åˆ¶
        start_time = time.time()
        overall_timeout = 300  # 5åˆ†é’Ÿæ•´ä½“è¶…æ—¶
        
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False,
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # ğŸ¯ å¼ºåˆ¶å¯ç”¨æµå¼æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨ arun_many è¿›è¡ŒçœŸæ­£çš„æ‰¹é‡å¤„ç†
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                word_count_threshold=10,
                stream=True # å¼ºåˆ¶æµå¼
            )
            
            crawled_results = []
            successful_crawls = 0
            progress_counter = 0
            
            # ğŸ¯ å‘é€å¼€å§‹è¿›åº¦
            logger.info(f"BatchCrawl Started: {len(params.urls)} URLs")
            
            # ğŸ¯ ä½¿ç”¨ arun_many è¿›è¡Œæ‰¹é‡æµå¼å¤„ç†
            # ğŸ¯ å®‰å…¨é™åˆ¶ï¼šå¹¶å‘æ•°ä¸è¶…è¿‡ 4ï¼Œä»¥ä¿è¯æœåŠ¡å™¨ç¨³å®šæ€§
            async for result in await crawler.arun_many(
                urls=params.urls,
                config=config,
                concurrent_limit=min(params.concurrent_limit, 4)
            ):
                progress_counter += 1
                
                # ğŸ¯ æ£€æŸ¥æ•´ä½“è¶…æ—¶
                current_time = time.time()
                elapsed = current_time - start_time
                if elapsed > overall_timeout:
                    logger.warning(f"â° BatchCrawl overall timeout after {overall_timeout}s. Returning partial results.")
                    break
                
                if result.success:
                    page_data = {
                        "url": result.url,
                        "title": getattr(result, 'title', ''),
                        "content": getattr(result, 'markdown', ''),
                        "metadata": {
                            "word_count": len(getattr(result, 'markdown', '')),
                            "status_code": getattr(result, 'status_code', 200)
                        }
                    }
                    crawled_results.append(page_data)
                    successful_crawls += 1
                else:
                    crawled_results.append({
                        "url": result.url,
                        "error": result.error_message or "Unknown error during crawl",
                        "success": False
                    })
                
                # ğŸ¯ æ¯ä¸ªURLå¤„ç†åå‘é€è¿›åº¦å¿ƒè·³
                logger.info(f"BatchCrawl Progress: {progress_counter}/{len(params.urls)} URLs processed in {elapsed:.1f}s")
                
                # å®‰å…¨é™åˆ¶
                if progress_counter >= 20:
                    logger.info(f"BatchCrawl Max Pages limit reached: 20")
                    break
            
            logger.info(f"âœ… BatchCrawl Completed: {successful_crawls}/{len(params.urls)} successful")
            
            # ç¡®å®šæœ€ç»ˆæˆåŠŸçŠ¶æ€
            final_success = True
            final_error = None
            if elapsed > overall_timeout:
                final_success = False
                final_error = f"æ‰¹é‡çˆ¬å–æ“ä½œè¶…æ—¶ï¼ˆ{overall_timeout}ç§’ï¼‰ï¼Œè¿”å›éƒ¨åˆ†ç»“æœ ({successful_crawls} æˆåŠŸ)"
            
            return {
                "success": final_success,
                "error": final_error,
                "results": crawled_results,
                "summary": {
                    "total_urls": len(params.urls),
                    "successful_crawls": successful_crawls,
                    "failed_crawls": len(params.urls) - successful_crawls,
                    "success_rate": (successful_crawls / len(params.urls)) * 100 if params.urls else 0
                },
                "memory_info": await self._get_system_memory_info()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡çˆ¬å–é”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False,
                "error": f"æ‰¹é‡çˆ¬å–é”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def _extract_structured_data(self, params: ExtractParams) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–æ•°æ® - æœ€ç»ˆä¿®å¤ç‰ˆï¼ˆä½¿ç”¨æ­£ç¡®çš„éæµå¼å¤„ç†ï¼‰"""
        logger.info(f"ğŸ” ä»é¡µé¢æå–ç»“æ„åŒ–æ•°æ®: {params.url}, ç±»å‹: {params.extraction_type}")
        
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {"success": False, "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–", "memory_info": await self._get_system_memory_info()}
            
            # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šä¸º extract æ¨¡å¼ç‰¹åˆ«å¤„ç† schema
            schema = params.schema_definition.copy()
            
            # è‡ªåŠ¨ä¿®å¤å¸¸è§çš„ schema æ ¼å¼é—®é¢˜
            if params.extraction_type == 'css':
                # å¦‚æœ schema æ˜¯æ•°ç»„æ ¼å¼ï¼Œè½¬æ¢ä¸ºå¯¹è±¡æ ¼å¼
                if isinstance(schema, list):
                    schema = {
                        "name": "ExtractedData",
                        "baseSelector": params.css_selector or "body",
                        "fields": schema
                    }
                    logger.info(f"ğŸ”§ è‡ªåŠ¨è½¬æ¢æ•°ç»„æ ¼å¼ schema ä¸ºå¯¹è±¡æ ¼å¼")
                
                # ç¡®ä¿æœ‰å¿…è¦çš„å­—æ®µ
                if not isinstance(schema, dict):
                    schema = {"fields": []} if schema is None else schema
                
                if 'name' not in schema:
                    schema['name'] = "ExtractedData"
                
                if 'baseSelector' not in schema:
                    schema['baseSelector'] = params.css_selector or "body"
                    logger.info(f"ğŸ”§ è‡ªåŠ¨æ·»åŠ  baseSelector: {schema['baseSelector']}")
                
                if 'fields' not in schema or not schema['fields']:
                    schema['fields'] = [
                        {
                            "name": "content",
                            "selector": schema['baseSelector'],
                            "type": "text",
                            "multiple": True
                        }
                    ]
                    logger.info(f"ğŸ”§ è‡ªåŠ¨æ·»åŠ é»˜è®¤ fields")
            
            # ğŸ¯ æ ¸å¿ƒé…ç½®ï¼šextract æ¨¡å¼ä½¿ç”¨éæµå¼è°ƒç”¨
            config_kwargs = {
                "cache_mode": CacheMode.BYPASS,
                "word_count_threshold": 0,
                "excluded_tags": [],
                "remove_forms": False,
                "remove_overlay_elements": False,
                "css_selector": params.css_selector or 'body',
                "stream": False,  # ğŸ”¥ å…³é”®ï¼šextract ä½¿ç”¨éæµå¼
            }
            
            # é…ç½®æå–ç­–ç•¥
            if params.extraction_type == 'css':
                try:
                    extraction_strategy = JsonCssExtractionStrategy(schema=schema)
                    config_kwargs["extraction_strategy"] = extraction_strategy
                except Exception as e:
                    logger.error(f"âŒ åˆ›å»º CSS æå–ç­–ç•¥å¤±è´¥: {e}")
                    return {
                        "success": False,
                        "error": f"åˆ›å»ºæå–ç­–ç•¥å¤±è´¥: {str(e)}",
                        "memory_info": await self._get_system_memory_info()
                    }
                    
            elif params.extraction_type == 'llm':
                logger.warning("âš ï¸ LLM æå–æ¨¡å¼éœ€è¦æœ‰æ•ˆçš„ LLM å®ä¾‹")
                try:
                    extraction_strategy = LLMExtractionStrategy(
                        schema=schema,
                        instruction=params.prompt or "Extract structured data from the content",
                        llm=None  # éœ€è¦å®é™…çš„ LLM å®ä¾‹
                    )
                    config_kwargs["extraction_strategy"] = extraction_strategy
                except Exception as e:
                    logger.error(f"âŒ åˆ›å»º LLM æå–ç­–ç•¥å¤±è´¥: {e}")
                    return {
                        "success": False,
                        "error": f"åˆ›å»º LLM æå–ç­–ç•¥å¤±è´¥: {str(e)}",
                        "memory_info": await self._get_system_memory_info()
                    }
            
            config = CrawlerRunConfig(**config_kwargs)
            
            logger.info(f"ğŸš€ å¼€å§‹æ•°æ®æå–: {params.url}")
            logger.info(f"ğŸ“‹ Schema é…ç½®: {json.dumps(schema, ensure_ascii=False)[:500]}...")
            
            # ğŸ¯ æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨éæµå¼è°ƒç”¨ï¼Œè·å–å•ä¸ªç»“æœ
            try:
                result = await self._execute_with_timeout(
                    crawler.arun(url=params.url, config=config),
                    timeout=90  # extract æ¨¡å¼ä¸éœ€è¦å¤ªé•¿æ—¶é—´
                )
            except asyncio.TimeoutError:
                logger.warning(f"â° Extract Timeout for {params.url}")
                return {
                    "success": False,
                    "error": "æ•°æ®æå–è¶…æ—¶ï¼ˆ90ç§’ï¼‰",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # å¤„ç†ç»“æœ
            if not result.success:
                error_msg = result.error_message or "æå–å¤±è´¥ï¼ŒæœªçŸ¥åŸå› "
                logger.error(f"âŒ æ•°æ®æå–å¤±è´¥: {params.url} - {error_msg}")
                return {
                    "success": False,
                    "error": f"æ•°æ®æå–å¤±è´¥: {error_msg}",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # æ£€æŸ¥æå–çš„å†…å®¹
            extracted_data = None
            if hasattr(result, 'extracted_content') and result.extracted_content:
                try:
                    # å°è¯•è§£æä¸º JSON
                    extracted_data = json.loads(result.extracted_content)
                    logger.info(f"âœ… æˆåŠŸæå–ç»“æ„åŒ–æ•°æ®ï¼Œæ ¼å¼: JSON")
                except (json.JSONDecodeError, TypeError):
                    # å¦‚æœä¸æ˜¯ JSONï¼Œä¿ç•™åŸå§‹æ ¼å¼
                    extracted_data = result.extracted_content
                    logger.info(f"âœ… æˆåŠŸæå–æ•°æ®ï¼Œæ ¼å¼: {type(extracted_data).__name__}")
            else:
                # å¦‚æœæ²¡æœ‰ extracted_contentï¼Œå°è¯•ä» markdown æˆ– cleaned_html ä¸­æå–
                content = getattr(result, 'markdown', '') or getattr(result, 'cleaned_html', '')
                if content:
                    extracted_data = {
                        "raw_content": content[:1000] + "..." if len(content) > 1000 else content,
                        "note": "æœªä½¿ç”¨æå–ç­–ç•¥ï¼Œè¿”å›åŸå§‹å†…å®¹"
                    }
                    logger.info(f"âš ï¸ æœªæ‰¾åˆ°æå–å†…å®¹ï¼Œè¿”å›åŸå§‹å†…å®¹ç‰‡æ®µ")
            
            if extracted_data is None:
                error_msg = "æœªèƒ½æå–åˆ°ä»»ä½•å†…å®¹ã€‚å¯èƒ½çš„åŸå› ï¼š1) é¡µé¢ç»“æ„ä¸åŒ¹é… schemaï¼Œ2) é¡µé¢éœ€è¦ JavaScript æ¸²æŸ“ï¼Œ3) æå–ç­–ç•¥é…ç½®é”™è¯¯"
                logger.error(f"âŒ æ•°æ®æå–æ— ç»“æœ: {params.url} - {error_msg}")
                return {
                    "success": False,
                    "error": error_msg,
                    "memory_info": await self._get_system_memory_info()
                }
            
            logger.info(f"âœ… Extract æˆåŠŸ: {params.url}")
            
            return {
                "success": True,
                "url": params.url,
                "extracted_data": extracted_data,
                "metadata": {
                    "extraction_type": params.extraction_type,
                    "schema_used": schema,
                    "success": True
                },
                "memory_info": await self._get_system_memory_info()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æå–æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}", exc_info=True)
            return {
                "success": False,
                "error": f"æ•°æ®æå–æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def _export_pdf(self, params: PdfExportParams) -> Dict[str, Any]:
        """å¯¼å‡ºPDFä¸ºbase64"""
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            logger.info(f"ğŸ“„ å¯¼å‡ºPDF: {params.url}")
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                pdf=True
            )
            
            result = await self._execute_with_timeout(
                crawler.arun(url=params.url, config=config),
                timeout=90 # ç¼©çŸ­è¶…æ—¶æ—¶é—´è‡³ 90 ç§’ï¼Œä»¥é¿å… Cloudflare 524 é”™è¯¯
            )
            
            if not result.success or not result.pdf:
                logger.error(f"âŒ PDFå¯¼å‡ºå¤±è´¥: {params.url}")
                return {
                    "success": False, 
                    "error": "PDFå¯¼å‡ºå¤±è´¥",
                    "memory_info": await self._get_system_memory_info()
                }
            
            if params.return_as_base64:
                pdf_base64 = base64.b64encode(result.pdf).decode('utf-8')
                return {
                    "success": True,
                    "url": params.url,
                    "pdf_data": pdf_base64,
                    "format": "base64",
                    "type": "application/pdf",
                    "size_bytes": len(result.pdf),
                    "message": "PDFæˆåŠŸå¯¼å‡ºä¸ºbase64å­—ç¬¦ä¸²",
                    "memory_info": await self._get_system_memory_info()
                }
            else:
                return {
                    "success": True,
                    "url": params.url,
                    "size_bytes": len(result.pdf),
                    "message": "PDFæ•°æ®ä»¥äºŒè¿›åˆ¶æ ¼å¼æä¾›",
                    "memory_info": await self._get_system_memory_info()
                }
        except asyncio.TimeoutError:
            return {
                "success": False, 
                "error": "PDFå¯¼å‡ºè¶…æ—¶ï¼ˆ120ç§’ï¼‰",
                "memory_info": await self._get_system_memory_info()
            }
        except Exception as e:
            logger.error(f"âŒ PDFå¯¼å‡ºé”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False, 
                "error": f"PDFå¯¼å‡ºé”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def _capture_screenshot(self, params: ScreenshotParams) -> Dict[str, Any]:
        """æ•è·æˆªå›¾ä¸ºbase64ï¼ˆå¸¦å‹ç¼©ï¼‰"""
        try:
            crawler = await self._get_crawler()
            if crawler is None:
                return {
                    "success": False, 
                    "error": "æµè§ˆå™¨å®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–",
                    "memory_info": await self._get_system_memory_info()
                }
            
            logger.info(f"ğŸ“¸ æ•è·æˆªå›¾: {params.url}")
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                screenshot=True
            )
            
            result = await self._execute_with_timeout(
                crawler.arun(url=params.url, config=config),
                timeout=120
            )
            
            if not result.success or not result.screenshot:
                logger.error(f"âŒ æˆªå›¾æ•è·å¤±è´¥: {params.url}")
                return {
                    "success": False, 
                    "error": "æˆªå›¾æ•è·å¤±è´¥",
                    "memory_info": await self._get_system_memory_info()
                }
            
            # å‹ç¼©æˆªå›¾
            compressed_screenshot = self.compressor.compress_screenshot(
                result.screenshot,
                quality=params.quality,
                max_width=params.max_width,
                max_height=params.max_height
            )
            
            # è·å–å‹ç¼©ä¿¡æ¯
            original_info = self.compressor.get_screenshot_info(result.screenshot)
            compressed_info = self.compressor.get_screenshot_info(compressed_screenshot)
            
            if params.return_as_base64:
                return {
                    "success": True,
                    "url": params.url,
                    "screenshot_data": compressed_screenshot,
                    "format": "base64", 
                    "type": "image/jpeg",
                    "size_bytes": len(base64.b64decode(compressed_screenshot)),
                    "compression_info": {
                        "original": original_info,
                        "compressed": compressed_info
                    },
                    "message": "æˆªå›¾æˆåŠŸæ•è·å¹¶å‹ç¼©ä¸ºbase64å­—ç¬¦ä¸²",
                    "memory_info": await self._get_system_memory_info()
                }
            else:
                return {
                    "success": True,
                    "url": params.url,
                    "size_bytes": len(base64.b64decode(compressed_screenshot)),
                    "compression_info": {
                        "original": original_info,
                        "compressed": compressed_info
                    },
                    "message": "æˆªå›¾æ•°æ®ä»¥base64æ ¼å¼æä¾›",
                    "memory_info": await self._get_system_memory_info()
                }
        except asyncio.TimeoutError:
            return {
                "success": False, 
                "error": "æˆªå›¾æ•è·è¶…æ—¶ï¼ˆ120ç§’ï¼‰",
                "memory_info": await self._get_system_memory_info()
            }
        except Exception as e:
            logger.error(f"âŒ æˆªå›¾æ•è·é”™è¯¯: {str(e)}")
            if "browser" in str(e).lower() or "context" in str(e).lower() or "NoneType" in str(e):
                await self._handle_browser_crash(e)
            return {
                "success": False, 
                "error": f"æˆªå›¾æ•è·é”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }
        finally:
            await self._cleanup_after_task()

    async def execute(self, parameters: Crawl4AIInput) -> dict:
        """æ‰§è¡Œå·¥å…·çš„ä¸»è¦æ–¹æ³•"""
        try:
            mode = parameters.mode
            params = parameters.parameters

            logger.info(f"ğŸš€ æ‰§è¡Œ Crawl4AI æ¨¡å¼: {mode}")

            # ğŸ”¥ è‡ªåŠ¨ä¿®å¤å¸¸è§å‚æ•°é—®é¢˜
            if mode == 'extract':
                # ä¿®å¤ schema_definition å‚æ•°å
                if 'schema' in params and 'schema_definition' not in params:
                    params['schema_definition'] = params.pop('schema')
                    logger.info("ğŸ”§ è‡ªåŠ¨ä¿®å¤: å°† 'schema' é‡å‘½åä¸º 'schema_definition'")
                
                # ç¡®ä¿æœ‰ url å‚æ•°
                if 'url' not in params:
                    # å°è¯•ä»å…¶ä»–ä½ç½®è·å–
                    for key in ['target_url', 'page_url', 'source']:
                        if key in params:
                            params['url'] = params.pop(key)
                            logger.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤: å°† '{key}' é‡å‘½åä¸º 'url'")
                            break
            
            elif mode == 'deep_crawl':
                # ä¿®å¤ strategy å¤§å°å†™é—®é¢˜
                if 'strategy' in params and isinstance(params['strategy'], str):
                    params['strategy'] = params['strategy'].lower()
                    logger.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤: strategy è½¬æ¢ä¸ºå°å†™: {params['strategy']}")

            # ä»»åŠ¡è®¡æ•°å’Œå®šæœŸå¼ºåˆ¶æ¸…ç†
            self._task_count += 1

            # åªæœ‰åœ¨è¾¾åˆ°æ¸…ç†é—´éš”æ—¶æ‰æ‰§è¡Œå†…å­˜æ£€æŸ¥
            if self._task_count % self._cleanup_interval == 0:
                memory_ok = await self._check_memory_health()
                if not memory_ok:
                    logger.warning("âš ï¸ æ‰§è¡Œå‰å†…å­˜æ£€æŸ¥å¤±è´¥ï¼Œå…ˆæ‰§è¡Œæ¸…ç†")
                    await self._force_memory_cleanup()

            # ç¡®ä¿æµè§ˆå™¨å·²åˆå§‹åŒ–
            await self.initialize()

            if mode == 'scrape':
                validated_params = ScrapeParams(**params)
                result = await self._scrape_single_url(validated_params)
                
            elif mode == 'deep_crawl':
                validated_params = DeepCrawlParams(**params)
                result = await self._deep_crawl_website(validated_params)
                
            elif mode == 'batch_crawl':
                validated_params = BatchCrawlParams(**params)
                result = await self._batch_crawl_urls(validated_params)
                
            elif mode == 'extract':
                validated_params = ExtractParams(**params)
                result = await self._extract_structured_data(validated_params)
                
            elif mode == 'pdf_export':
                validated_params = PdfExportParams(**params)
                result = await self._export_pdf(validated_params)
                
            elif mode == 'screenshot':
                validated_params = ScreenshotParams(**params)
                result = await self._capture_screenshot(validated_params)
                
            else:
                logger.error(f"âŒ æ— æ•ˆçš„æ¨¡å¼è¯·æ±‚: {mode}")
                return {
                    "success": False, 
                    "error": f"æ— æ•ˆçš„æ¨¡å¼ '{mode}'.",
                    "memory_info": await self._get_system_memory_info()
                }

            return result

        except Exception as e:
            logger.error(f"âŒ Crawl4AI å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}")
            return {
                "success": False, 
                "error": f"å‘ç”Ÿé”™è¯¯: {str(e)}",
                "memory_info": await self._get_system_memory_info()
            }

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        async with self._browser_lock:
            if self.crawler:
                try:
                    logger.info("ğŸ”š å…³é—­ crawl4ai æµè§ˆå™¨å®ä¾‹...")
                    await self.crawler.__aexit__(None, None, None)
                    self.crawler = None
                    self._initialized = False
                    self._task_count = 0
                    self._browser_start_time = None
                    
                    collected = gc.collect()
                    logger.info(f"æœ€ç»ˆåƒåœ¾å›æ”¶é‡Šæ”¾äº† {collected} ä¸ªå¯¹è±¡")
                    
                    logger.info("âœ… crawl4ai æµè§ˆå™¨å®ä¾‹å…³é—­æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ å…³é—­ crawl4ai æµè§ˆå™¨æ—¶å‡ºé”™: {str(e)}")
                    self.crawler = None
                    self._initialized = False
                    self._task_count = 0
                    self._browser_start_time = None