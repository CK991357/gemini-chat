"""AlphaVantageé‡‘èæ•°æ®è·å–å·¥å…· - æœ€ç»ˆç‰ˆï¼ˆä»…ä».envè¯»å–API Keyï¼‰"""
import os
import json
import logging
import shutil
import pandas as pd
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from enum import Enum
from datetime import datetime, timedelta
from pathlib import Path

# å¯¼å…¥AlphaVantageæ•°æ®è·å–å™¨
from src.core.data_fetcher_alphavantage import AlphaVantageFetcher

logger = logging.getLogger(__name__)

# ==================== é…ç½®åŒº ====================
# ğŸ¯ æ ¸å¿ƒï¼šä¸ä»£ç è§£é‡Šå™¨å…±äº«çš„å·¥ä½œåŒºæ ¹ç›®å½•
SESSION_WORKSPACE_ROOT = Path("/srv/sandbox_workspaces")
SESSION_TIMEOUT_HOURS = 24  # ä¸ä»£ç è§£é‡Šå™¨ä¿æŒä¸€è‡´çš„ä¼šè¯è¶…æ—¶æ—¶é—´

# ==================== Pydanticæ¨¡å‹å®šä¹‰ ====================

class AlphaVantageFunction(str, Enum):
    """æ”¯æŒçš„AlphaVantageåŠŸèƒ½åˆ—è¡¨"""
    FETCH_WEEKLY_ADJUSTED = "fetch_weekly_adjusted"
    FETCH_GLOBAL_QUOTE = "fetch_global_quote"
    FETCH_HISTORICAL_OPTIONS = "fetch_historical_options"
    FETCH_EARNINGS_TRANSCRIPT = "fetch_earnings_transcript"
    FETCH_INSIDER_TRANSACTIONS = "fetch_insider_transactions"
    FETCH_ETF_PROFILE = "fetch_etf_profile"
    FETCH_FOREX_DAILY = "fetch_forex_daily"
    FETCH_DIGITAL_CURRENCY_DAILY = "fetch_digital_currency_daily"
    FETCH_WTI = "fetch_wti"
    FETCH_BRENT = "fetch_brent"
    FETCH_COPPER = "fetch_copper"
    FETCH_TREASURY_YIELD = "fetch_treasury_yield"
    FETCH_NEWS_SENTIMENT = "fetch_news_sentiment"

class AlphaVantageInput(BaseModel):
    """AlphaVantageå·¥å…·è¾“å…¥æ¨¡å‹"""
    function: AlphaVantageFunction = Field(description="è¦è°ƒç”¨çš„AlphaVantageåŠŸèƒ½")
    parameters: Dict[str, Any] = Field(description="åŠŸèƒ½å‚æ•°")

# ==================== å·¥å…·ç±» ====================

class AlphaVantageTool:
    name = "alphavantage"
    description = (
        "ä»AlphaVantageè·å–é‡‘èæ•°æ®çš„å·¥å…·ã€‚æ”¯æŒè‚¡ç¥¨ã€å¤–æ±‡ã€åŠ å¯†è´§å¸ã€å¤§å®—å•†å“ã€"
        "å›½å€ºæ”¶ç›Šç‡ã€æ–°é—»æƒ…ç»ªç­‰å¤šç§æ•°æ®ç±»å‹ã€‚æ•°æ®ä¼šä¿å­˜åˆ°ä¼šè¯å·¥ä½œåŒºï¼Œä»¥ä¾¿åç»­ä½¿ç”¨ä»£ç è§£é‡Šå™¨è¿›è¡Œåˆ†æå’Œå¯è§†åŒ–ã€‚"
    )
    input_schema = AlphaVantageInput
    
    def __init__(self):
        # ç¡®ä¿å·¥ä½œåŒºæ ¹ç›®å½•å­˜åœ¨
        SESSION_WORKSPACE_ROOT.mkdir(exist_ok=True, parents=True)
        logger.info(f"AlphaVantageå·¥å…·åˆå§‹åŒ–ï¼Œå·¥ä½œåŒºç›®å½•: {SESSION_WORKSPACE_ROOT}")
        
        # éªŒè¯API Keyæ˜¯å¦å·²é…ç½®
        self._validate_api_key()
    
    def _validate_api_key(self):
        """éªŒè¯API Keyæ˜¯å¦é…ç½®"""
        # æ£€æŸ¥data_fetcher_alphavantage.pyä¸­çš„API Keyè·å–é€»è¾‘
        try:
            from src.core.data_fetcher_alphavantage import AlphaVantageFetcher
            api_key = AlphaVantageFetcher.get_api_key()
            if api_key and api_key != "U5KM36DHDXR95Q7Q":  # ä¸æ˜¯é»˜è®¤key
                logger.info("âœ… AlphaVantage API Key å·²æ­£ç¡®é…ç½®")
            else:
                logger.warning("âš ï¸ AlphaVantage API Key æœªé…ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        except Exception as e:
            logger.error(f"éªŒè¯API Keyæ—¶å‡ºé”™: {e}")
    
    def _ensure_session_workspace(self, session_id: str) -> Path:
        """ç¡®ä¿ä¼šè¯å·¥ä½œåŒºå­˜åœ¨å¹¶è¿”å›è·¯å¾„"""
        if not session_id:
            # å¦‚æœæ²¡æœ‰session_idï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•
            temp_dir = SESSION_WORKSPACE_ROOT / "temp" / str(int(datetime.now().timestamp()))
            temp_dir.mkdir(parents=True, exist_ok=True)
            return temp_dir
        
        session_dir = SESSION_WORKSPACE_ROOT / session_id
        session_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ•°æ®å­ç›®å½•
        data_subdirs = ["alphavantage", "stock", "forex", "crypto", "commodities", "news"]
        for subdir in data_subdirs:
            (session_dir / subdir).mkdir(exist_ok=True)
        
        return session_dir
    
    def _cleanup_old_sessions(self):
        """æ¸…ç†è¿‡æœŸçš„ä¼šè¯å·¥ä½œåŒºï¼ˆä¸ä»£ç è§£é‡Šå™¨ä¿æŒä¸€è‡´ï¼‰"""
        try:
            current_time = datetime.now()
            cleaned_count = 0
            
            for session_dir in SESSION_WORKSPACE_ROOT.iterdir():
                if session_dir.is_dir():
                    # è·³è¿‡tempç›®å½•
                    if session_dir.name == "temp":
                        # æ¸…ç†tempç›®å½•ï¼ˆåˆ›å»ºè¶…è¿‡1å°æ—¶çš„ï¼‰
                        temp_creation_time = datetime.fromtimestamp(session_dir.stat().st_ctime)
                        if current_time - temp_creation_time > timedelta(hours=1):
                            try:
                                shutil.rmtree(session_dir)
                                logger.info(f"æ¸…ç†ä¸´æ—¶ç›®å½•: {session_dir.name}")
                            except Exception as e:
                                logger.error(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥ {session_dir.name}: {e}")
                        continue
                    
                    # æ£€æŸ¥ç›®å½•ä¿®æ”¹æ—¶é—´
                    stat = session_dir.stat()
                    modify_time = datetime.fromtimestamp(stat.st_mtime)
                    if current_time - modify_time > timedelta(hours=SESSION_TIMEOUT_HOURS):
                        try:
                            shutil.rmtree(session_dir)
                            logger.info(f"æ¸…ç†è¿‡æœŸä¼šè¯: {session_dir.name}")
                            cleaned_count += 1
                        except Exception as e:
                            logger.error(f"æ¸…ç†ä¼šè¯å¤±è´¥ {session_dir.name}: {e}")
            
            if cleaned_count > 0:
                logger.info(f"ä¼šè¯æ¸…ç†å®Œæˆ: ç§»é™¤äº† {cleaned_count} ä¸ªè¿‡æœŸä¼šè¯")
                
        except Exception as e:
            logger.error(f"ä¼šè¯æ¸…ç†è¿‡ç¨‹å¤±è´¥: {e}")
    
    async def execute(self, parameters: AlphaVantageInput, session_id: str = None) -> dict:
        """
        æ‰§è¡ŒAlphaVantageæ•°æ®è·å–
        """
        try:
            function_name = parameters.function.value
            function_params = parameters.parameters
            
            # ç¡®ä¿ä¼šè¯å·¥ä½œåŒºå­˜åœ¨
            session_dir = self._ensure_session_workspace(session_id)
            logger.info(f"ä½¿ç”¨ä¼šè¯ç›®å½•: {session_dir}")
            
            # æ ¹æ®function_nameè°ƒç”¨ä¸åŒçš„æ–¹æ³•
            result = None
            metadata = {
                "function": function_name,
                "parameters": function_params,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "saved_files": [],
                "data_type": self._get_data_type(function_name)
            }
            
            # ğŸ¯ æ ¸å¿ƒï¼šè°ƒç”¨AlphaVantageæ•°æ®è·å–å™¨
            try:
                # åŠ¨æ€è°ƒç”¨å¯¹åº”çš„æ–¹æ³•
                method = getattr(AlphaVantageFetcher, function_name)
                
                # ç‰¹æ®Šå¤„ç†ä¸åŒå‡½æ•°çš„æ•°æ®ä¿å­˜
                if function_name == "fetch_weekly_adjusted":
                    symbol = function_params.get("symbol", "")
                    result = method(symbol)
                    # æ–‡ä»¶å·²ç”±data_fetcher_alphavantage.pyä¿å­˜åˆ°data/raw/us_stock/
                    # æˆ‘ä»¬éœ€è¦å¤åˆ¶åˆ°ä¼šè¯ç›®å½•
                    source_file = Path("data/raw/us_stock") / f"{symbol}.parquet"
                    if source_file.exists():
                        dest_file = session_dir / "alphavantage" / f"{symbol}_weekly.parquet"
                        dest_file.parent.mkdir(exist_ok=True)
                        shutil.copy2(source_file, dest_file)
                        metadata["saved_files"].append(str(dest_file))
                
                elif function_name == "fetch_global_quote":
                    symbol = function_params.get("symbol", "")
                    result = method(symbol)
                    # ä¿å­˜åˆ°JSONæ–‡ä»¶
                    if result:
                        json_file = session_dir / "alphavantage" / f"{symbol}_quote.json"
                        json_file.parent.mkdir(exist_ok=True)
                        with open(json_file, 'w', encoding='utf-8') as f:
                            json.dump(result, f, ensure_ascii=False, indent=2)
                        metadata["saved_files"].append(str(json_file))
                
                elif function_name == "fetch_forex_daily":
                    from_symbol = function_params.get("from_symbol", "USD")
                    to_symbol = function_params.get("to_symbol", "JPY")
                    outputsize = function_params.get("outputsize", "full")
                    result = method(from_symbol, to_symbol, outputsize)
                    # å¤åˆ¶Parquetæ–‡ä»¶
                    source_file = Path("data/raw/forex") / f"{from_symbol}_{to_symbol}_daily.parquet"
                    if source_file.exists():
                        dest_file = session_dir / "forex" / f"{from_symbol}_{to_symbol}.parquet"
                        dest_file.parent.mkdir(exist_ok=True)
                        shutil.copy2(source_file, dest_file)
                        metadata["saved_files"].append(str(dest_file))
                
                elif function_name == "fetch_digital_currency_daily":
                    symbol = function_params.get("symbol", "BTC")
                    market = function_params.get("market", "USD")
                    result = method(symbol, market)
                    # å¤„ç†æ•°å­—è´§å¸æ•°æ®
                    if result and isinstance(result, dict):
                        for key, df in result.items():
                            if hasattr(df, 'to_parquet'):
                                parquet_file = session_dir / "crypto" / f"{symbol}_{market}_{key}.parquet"
                                parquet_file.parent.mkdir(exist_ok=True)
                                df.to_parquet(parquet_file)
                                metadata["saved_files"].append(str(parquet_file))
                
                elif function_name == "fetch_news_sentiment":
                    tickers = function_params.get("tickers")
                    topics = function_params.get("topics")
                    limit = function_params.get("limit", 50)
                    result = method(
                        tickers=tickers,
                        topics=topics,
                        limit=limit,
                        sort=function_params.get("sort", "LATEST"),
                        time_from=function_params.get("time_from"),
                        time_to=function_params.get("time_to")
                    )
                    # ä¿å­˜åˆ°JSONæ–‡ä»¶
                    if result:
                        safe_name = (tickers or "news").replace(',', '_')[:50]
                        json_file = session_dir / "news" / f"{safe_name}_{int(datetime.now().timestamp())}.json"
                        json_file.parent.mkdir(exist_ok=True)
                        with open(json_file, 'w', encoding='utf-8') as f:
                            json.dump(result, f, ensure_ascii=False, indent=2)
                        metadata["saved_files"].append(str(json_file))
                
                # å¤„ç†å…¶ä»–é€šç”¨å‡½æ•°
                elif function_name in ["fetch_wti", "fetch_brent", "fetch_copper"]:
                    interval = function_params.get("interval", "monthly")
                    result = method(interval)
                    # ä¿å­˜åˆ°Parquetæ–‡ä»¶
                    if hasattr(result, 'to_parquet'):
                        commodity_name = function_name.replace("fetch_", "").upper()
                        parquet_file = session_dir / "commodities" / f"{commodity_name}_{interval}.parquet"
                        parquet_file.parent.mkdir(exist_ok=True)
                        result.to_parquet(parquet_file)
                        metadata["saved_files"].append(str(parquet_file))
                
                elif function_name == "fetch_treasury_yield":
                    interval = function_params.get("interval", "monthly")
                    maturity = function_params.get("maturity", "10year")
                    result = method(interval, maturity)
                    # ä¿å­˜åˆ°Parquetæ–‡ä»¶
                    if hasattr(result, 'to_parquet'):
                        parquet_file = session_dir / "alphavantage" / f"treasury_{maturity}_{interval}.parquet"
                        parquet_file.parent.mkdir(exist_ok=True)
                        result.to_parquet(parquet_file)
                        metadata["saved_files"].append(str(parquet_file))
                
                # å…¶ä»–å‡½æ•°ä½¿ç”¨é€šç”¨å¤„ç†
                else:
                    result = method(**function_params)
                    
                    # å°è¯•æŸ¥æ‰¾å¹¶å¤åˆ¶ç›¸å…³æ–‡ä»¶
                    data_raw_dir = Path("data/raw")
                    if data_raw_dir.exists():
                        for root, dirs, files in os.walk(data_raw_dir):
                            for file in files:
                                if "parquet" in file or "json" in file:
                                    # ç®€å•åˆ¤æ–­æ˜¯å¦æ˜¯æœ¬æ¬¡ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆé€šè¿‡æ—¶é—´æˆ³ï¼‰
                                    file_path = Path(root) / file
                                    try:
                                        stat = file_path.stat()
                                        file_time = datetime.fromtimestamp(stat.st_mtime)
                                        if (datetime.now() - file_time).total_seconds() < 60:  # 1åˆ†é’Ÿå†…åˆ›å»ºçš„
                                            rel_path = file_path.relative_to(data_raw_dir)
                                            dest_file = session_dir / "alphavantage" / rel_path
                                            dest_file.parent.mkdir(parents=True, exist_ok=True)
                                            shutil.copy2(file_path, dest_file)
                                            metadata["saved_files"].append(str(dest_file))
                                    except Exception:
                                        continue
                
            except AttributeError:
                return {
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„å‡½æ•°: {function_name}",
                    "available_functions": [name for name in dir(AlphaVantageFetcher) 
                                          if name.startswith("fetch_")]
                }
            except Exception as e:
                logger.error(f"AlphaVantage APIè°ƒç”¨å¤±è´¥: {e}", exc_info=True)
                return {
                    "success": False,
                    "error": f"æ•°æ®è·å–å¤±è´¥: {str(e)}",
                    "function": function_name
                }
            
            # å¤„ç†è¿”å›ç»“æœ
            processed_result = self._process_result(result, function_name)
            
            # æ„å»ºæˆåŠŸå“åº”
            response = {
                "success": True,
                "data": processed_result,
                "metadata": metadata
            }
            
            # æ·»åŠ æ•°æ®ç›®å½•ä¿¡æ¯ï¼Œä¾¿äºå‰ç«¯å’Œä»£ç è§£é‡Šå™¨ä½¿ç”¨
            if session_id:
                response["metadata"]["session_dir"] = str(session_dir)
                response["metadata"]["data_dir"] = str(session_dir / "alphavantage")
                
                # æä¾›Pythonä»£ç ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨ä»£ç è§£é‡Šå™¨ä¸­è¯»å–è¿™äº›æ•°æ®
                example_code = self._generate_example_code(function_name, function_params, session_dir)
                response["metadata"]["example_code"] = example_code
            
            # å®šæœŸæ¸…ç†æ—§ä¼šè¯
            self._cleanup_old_sessions()
            
            logger.info(f"AlphaVantageå·¥å…·æ‰§è¡ŒæˆåŠŸ: {function_name}, ä¿å­˜æ–‡ä»¶æ•°: {len(metadata['saved_files'])}")
            return response
            
        except Exception as e:
            logger.error(f"AlphaVantageå·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
            return {
                "success": False,
                "error": f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}",
                "function": parameters.function.value if hasattr(parameters, 'function') else "unknown"
            }
    
    def _process_result(self, result, function_name: str):
        """å¤„ç†ä¸åŒç±»å‹çš„è¿”å›ç»“æœ"""
        if result is None:
            return {"message": "æœªè·å–åˆ°æ•°æ®"}
        
        # å¯¹äºDataFrameï¼Œè½¬æ¢ä¸ºåˆ—è¡¨å­—å…¸æ ¼å¼
        if hasattr(result, 'to_dict'):
            try:
                if hasattr(result, 'index'):
                    # å¦‚æœæ˜¯æ—¶é—´åºåˆ—æ•°æ®
                    df = result.reset_index()
                    if 'index' in df.columns:
                        df = df.rename(columns={'index': 'date'})
                    
                    # é™åˆ¶è¿”å›æ•°æ®é‡ï¼Œé¿å…å“åº”è¿‡å¤§
                    if len(df) > 100:
                        summary = {
                            "total_records": len(df),
                            "date_range": {
                                "start": str(df['date'].min()) if 'date' in df.columns else None,
                                "end": str(df['date'].max()) if 'date' in df.columns else None
                            },
                            "sample_data": df.head(10).to_dict(orient='records'),
                            "message": f"æ•°æ®è¿‡å¤šï¼Œåªæ˜¾ç¤ºå‰10æ¡è®°å½•ï¼Œå…±{len(df)}æ¡"
                        }
                        return summary
                    else:
                        return df.to_dict(orient='records')
                else:
                    return result.to_dict(orient='records')
            except Exception as e:
                logger.warning(f"DataFrameè½¬æ¢å¤±è´¥: {e}")
                return {"raw_result": str(result), "error": "æ•°æ®è½¬æ¢å¤±è´¥"}
        
        # å¯¹äºå­—å…¸æˆ–åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
        if isinstance(result, (dict, list)):
            # å¦‚æœæ•°æ®é‡è¿‡å¤§ï¼Œä¹Ÿè¿›è¡Œé™åˆ¶
            if isinstance(result, list) and len(result) > 100:
                return {
                    "total_records": len(result),
                    "sample_data": result[:10],
                    "message": f"æ•°æ®è¿‡å¤šï¼Œåªæ˜¾ç¤ºå‰10æ¡è®°å½•ï¼Œå…±{len(result)}æ¡"
                }
            return result
        
        # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return {"result": str(result)}
    
    def _get_data_type(self, function_name: str) -> str:
        """è·å–æ•°æ®ç±»å‹æ ‡ç­¾"""
        type_map = {
            "fetch_weekly_adjusted": "stock_weekly_data",
            "fetch_global_quote": "stock_realtime_quote",
            "fetch_historical_options": "options_data",
            "fetch_earnings_transcript": "earnings_transcript",
            "fetch_insider_transactions": "insider_transactions",
            "fetch_etf_profile": "etf_profile",
            "fetch_forex_daily": "forex_daily",
            "fetch_digital_currency_daily": "crypto_daily",
            "fetch_wti": "commodity_wti",
            "fetch_brent": "commodity_brent",
            "fetch_copper": "commodity_copper",
            "fetch_treasury_yield": "treasury_yield",
            "fetch_news_sentiment": "news_sentiment"
        }
        return type_map.get(function_name, "unknown")
    
    def _generate_example_code(self, function_name: str, params: dict, session_dir: Path) -> str:
        """ç”ŸæˆPythonä»£ç ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨ä»£ç è§£é‡Šå™¨ä¸­è¯»å–æ•°æ®"""
        symbol = params.get("symbol", "")
        
        if function_name == "fetch_weekly_adjusted":
            return f'''# è¯»å–è‚¡ç¥¨æ•°æ®å¹¶è¿›è¡Œç®€å•åˆ†æ
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®
df = pd.read_parquet('{session_dir}/alphavantage/{symbol}_weekly.parquet')
print(f"æ•°æ®å½¢çŠ¶: {{df.shape}}")
print("å‰5è¡Œæ•°æ®:")
print(df.head())

# ç®€å•çš„å¯è§†åŒ–
plt.figure(figsize=(12, 6))
plt.plot(df['date'], df['close'], label='æ”¶ç›˜ä»·', color='blue')
plt.title('{symbol} å‘¨æ”¶ç›˜ä»·èµ°åŠ¿')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('ä»·æ ¼ (USD)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# åŸºæœ¬ç»Ÿè®¡
print("\\nåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
print(df[['open', 'high', 'low', 'close', 'volume']].describe())
'''
        
        elif function_name == "fetch_global_quote":
            return f'''# è¯»å–å®æ—¶è¡Œæƒ…æ•°æ®
import json

with open('{session_dir}/alphavantage/{symbol}_quote.json', 'r') as f:
    quote_data = json.load(f)

print("å®æ—¶è¡Œæƒ…æ•°æ®:")
print("-" * 40)
for key, value in quote_data.items():
    print(f"{{key:20s}}: {{value}}")
print("-" * 40)

# è®¡ç®—æ¶¨è·Œå¹…
if quote_data.get('previous_close') and quote_data.get('price'):
    prev_close = float(quote_data['previous_close'])
    current_price = float(quote_data['price'])
    change_pct = ((current_price - prev_close) / prev_close) * 100
    print(f"\\næ¶¨è·Œå¹…: {{change_pct:.2f}}%")
'''
        
        elif function_name == "fetch_forex_daily":
            from_sym = params.get("from_symbol", "USD")
            to_sym = params.get("to_symbol", "JPY")
            return f'''# è¯»å–å¤–æ±‡æ•°æ®å¹¶åˆ†æ
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®
df = pd.read_parquet('{session_dir}/forex/{from_sym}_{to_sym}.parquet')
df.index = pd.to_datetime(df.index)
df = df.sort_index()

print(f"å¤–æ±‡æ•°æ® {{from_sym}}/{{to_sym}} å½¢çŠ¶: {{df.shape}}")
print("æœ€è¿‘5å¤©æ•°æ®:")
print(df.tail())

# ç»˜åˆ¶æ±‡ç‡èµ°åŠ¿
plt.figure(figsize=(14, 7))
plt.plot(df.index, df['close'], label=f'{{from_sym}}/{{to_sym}}', color='green')
plt.title('æ±‡ç‡èµ°åŠ¿å›¾')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('æ±‡ç‡')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# è®¡ç®—ç§»åŠ¨å¹³å‡
df['MA_20'] = df['close'].rolling(window=20).mean()
df['MA_50'] = df['close'].rolling(window=50).mean()

plt.figure(figsize=(14, 7))
plt.plot(df.index, df['close'], label='æ”¶ç›˜ä»·', alpha=0.5)
plt.plot(df.index, df['MA_20'], label='20æ—¥ç§»åŠ¨å¹³å‡', linewidth=2)
plt.plot(df.index, df['MA_50'], label='50æ—¥ç§»åŠ¨å¹³å‡', linewidth=2)
plt.title('æ±‡ç‡ç§»åŠ¨å¹³å‡åˆ†æ')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
'''
        
        elif function_name == "fetch_news_sentiment":
            tickers = params.get("tickers", "general")
            return f'''# è¯»å–æ–°é—»æƒ…ç»ªæ•°æ®
import json
import pandas as pd

with open('{session_dir}/news/{tickers}_*.json', 'r') as f:  # * æ›¿æ¢ä¸ºå®é™…æ—¶é—´æˆ³
    news_data = json.load(f)

print(f"æ–°é—»æ•°é‡: {{news_data.get('items', 0)}}")
print("\\næœ€æ–°æ–°é—»æ ‡é¢˜:")
for i, item in enumerate(news_data.get('feed', [])[:5]):
    print(f"{{i+1}}. {{item.get('title', 'No title')}}")
    print(f"   æƒ…ç»ª: {{item.get('overall_sentiment_label', 'N/A')}}")
    print()

# æƒ…ç»ªåˆ†æ
if news_data.get('feed'):
    sentiments = [item.get('overall_sentiment_label', 'Neutral') for item in news_data['feed']]
    sentiment_counts = pd.Series(sentiments).value_counts()
    print("æƒ…ç»ªåˆ†å¸ƒ:")
    print(sentiment_counts)
'''
        
        return f'''# æ•°æ®å·²ä¿å­˜åˆ°ä¼šè¯ç›®å½•
# æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è¯»å–æ•°æ®ï¼š
import pandas as pd
import json
from pathlib import Path

# åˆ—å‡ºä¼šè¯ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
session_path = Path('{session_dir}')
print("å¯ç”¨æ–‡ä»¶:")
for file_path in session_path.rglob('*'):
    if file_path.is_file():
        print(f"  - {{file_path.relative_to(session_path)}}")

# æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–æ•°æ®
# å¯¹äºParquetæ–‡ä»¶: pd.read_parquet('æ–‡ä»¶è·¯å¾„')
# å¯¹äºJSONæ–‡ä»¶: json.load(open('æ–‡ä»¶è·¯å¾„', 'r'))
'''

# ==================== è¾…åŠ©å‡½æ•° ====================

def get_available_functions() -> List[str]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„AlphaVantageå‡½æ•°"""
    return [func.value for func in AlphaVantageFunction]

def get_function_description(function_name: str) -> str:
    """è·å–å‡½æ•°æè¿°"""
    descriptions = {
        "fetch_weekly_adjusted": "è·å–è‚¡ç¥¨å‘¨è°ƒæ•´æ•°æ®ï¼ˆå¼€ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·ã€æ”¶ç›˜ä»·ã€è°ƒæ•´åæ”¶ç›˜ä»·ã€æˆäº¤é‡ã€è‚¡æ¯ï¼‰",
        "fetch_global_quote": "è·å–å®æ—¶è¡Œæƒ…æ•°æ®ï¼ˆå½“å‰ä»·æ ¼ã€æ¶¨è·Œå¹…ã€æˆäº¤é‡ç­‰ï¼‰",
        "fetch_historical_options": "è·å–å†å²æœŸæƒæ•°æ®",
        "fetch_earnings_transcript": "è·å–è´¢æŠ¥ç”µè¯ä¼šè®®è®°å½•",
        "fetch_insider_transactions": "è·å–å…¬å¸å†…éƒ¨äººäº¤æ˜“æ•°æ®",
        "fetch_etf_profile": "è·å–ETFè¯¦ç»†ä¿¡æ¯å’ŒæŒä»“æ•°æ®",
        "fetch_forex_daily": "è·å–å¤–æ±‡æ¯æ—¥æ•°æ®",
        "fetch_digital_currency_daily": "è·å–æ•°å­—è´§å¸æ¯æ—¥æ•°æ®",
        "fetch_wti": "è·å–WTIåŸæ²¹ä»·æ ¼æ•°æ®",
        "fetch_brent": "è·å–BrentåŸæ²¹ä»·æ ¼æ•°æ®",
        "fetch_copper": "è·å–å…¨çƒé“œä»·æ•°æ®",
        "fetch_treasury_yield": "è·å–ç¾å›½å›½å€ºæ”¶ç›Šç‡æ•°æ®",
        "fetch_news_sentiment": "è·å–å¸‚åœºæ–°é—»å’Œæƒ…ç»ªæ•°æ®"
    }
    return descriptions.get(function_name, "æœªçŸ¥åŠŸèƒ½")