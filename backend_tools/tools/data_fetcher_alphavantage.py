"""AlphaVantageæ•°æ®è·å–ä¸“ç”¨æ¨¡å— - æ”¯æŒä¼šè¯ç›®å½•å®Œæ•´ç‰ˆï¼ˆ13ä¸ªåŠŸèƒ½ï¼‰"""
import os
import logging
import json
import pandas as pd
import requests
from pathlib import Path
from typing import Optional, Dict, List, Union
from tenacity import retry, stop_after_attempt, wait_exponential

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AlphaVantageFetcher:
    """AlphaVantageæ•°æ®è·å–å™¨ - å®Œæ•´ç‰ˆ"""
    
    BASE_URL = "https://www.alphavantage.co/query"
    
    @staticmethod
    def get_api_key():
        """ä»ç¯å¢ƒå˜é‡è·å–API Key"""
        key = os.getenv("ALPHAVANTAGE_API_KEY")
        if not key:
            # å›é€€åˆ°ä½ çš„é»˜è®¤key
            logger.warning("âš ï¸ ALPHAVANTAGE_API_KEYæœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤key")
            return "U5KM36DHDXR95Q7Q"  # ä½ çš„é»˜è®¤key
        return key
    
    # ============ è‚¡ç¥¨æ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_weekly_adjusted(symbol: str, session_dir: Path = None) -> pd.DataFrame:
        """è·å–å‘¨è°ƒæ•´åæ•°æ®"""
        try:
            params = {
                "function": "TIME_SERIES_WEEKLY_ADJUSTED",
                "symbol": symbol, 
                "apikey": AlphaVantageFetcher.get_api_key()
            }

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            time_series = data.get("Weekly Adjusted Time Series", {})
            if not time_series:
                raise ValueError("No weekly data found in response")

            df = pd.DataFrame.from_dict(time_series, orient="index")
            df.index = pd.to_datetime(df.index)
            df = df.sort_index()

            df = df.rename(columns={
                "1. open": "open",
                "2. high": "high",
                "3. low": "low",
                "4. close": "close",
                "5. adjusted close": "adjusted_close",
                "6. volume": "volume",
                "7. dividend amount": "dividend"
            })

            df = df.astype({
                "open": float,
                "high": float,
                "low": float,
                "close": float,
                "adjusted_close": float,
                "volume": int,
                "dividend": float
            })

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "stock" / f"{symbol}.parquet"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                df.to_parquet(file_path)
                logger.info(f"è‚¡ç¥¨æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•ï¼š{file_path}")
            else:
                # åå¤‡ï¼šä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
                temp_dir = Path("/tmp/alphavantage_data") / "us_stock"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"{symbol}.parquet"
                df.to_parquet(file_path)
                logger.info(f"è‚¡ç¥¨æ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•ï¼š{file_path}")

            return df[["open", "high", "low", "close", "adjusted_close", "volume", "dividend"]]

        except Exception as e:
            logger.error(f"è·å–AlphaVantageæ•°æ®å¤±è´¥: {e}")
            raise
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_global_quote(symbol: str, session_dir: Path = None) -> Dict[str, Union[str, float, int]]:
        """è·å–å®æ—¶è¡Œæƒ…æ•°æ®"""
        try:
            params = {
                "function": "GLOBAL_QUOTE",
                "symbol": symbol,
                "apikey": AlphaVantageFetcher.get_api_key()
            }

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            quote = data.get("Global Quote", {})
            if not quote:
                raise ValueError("No quote data found in response")

            result = {
                'symbol': quote.get('01. symbol'),
                'open': float(quote.get('02. open', 0)) if quote.get('02. open', '') != '' else 0.0,
                'high': float(quote.get('03. high', 0)) if quote.get('03. high', '') != '' else 0.0,
                'low': float(quote.get('04. low', 0)) if quote.get('04. low', '') != '' else 0.0,
                'price': float(quote.get('05. price', 0)) if quote.get('05. price', '') != '' else 0.0,
                'volume': int(quote.get('06. volume', 0)) if quote.get('06. volume', '') != '' else 0,
                'latest_trading_day': quote.get('07. latest trading day'),
                'previous_close': float(quote.get('08. previous close', 0)) if quote.get('08. previous close', '') != '' else 0.0,
                'change': float(quote.get('09. change', 0)) if quote.get('09. change', '') != '' else 0.0,
                'change_percent': quote.get('10. change percent', '0%')
            }

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "stock" / f"{symbol}_quote.json"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                logger.info(f"å®æ—¶è¡Œæƒ…å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•ï¼š{file_path}")

            return result

        except Exception as e:
            logger.error(f"è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
            raise
    
    # ============ æœŸæƒæ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=4, max=10))  # å‡å°‘é‡è¯•æ¬¡æ•°
    def fetch_historical_options(symbol: str, date: str = None, session_dir: Path = None) -> List[Dict]:
        """è·å–å†å²æœŸæƒæ•°æ®"""
        try:
            params = {
                "function": "HISTORICAL_OPTIONS",
                "symbol": symbol,
                "apikey": AlphaVantageFetcher.get_api_key()
            }
            if date:
                params["date"] = date

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            # æ£€æŸ¥APIè¿”å›çš„é”™è¯¯ä¿¡æ¯
            if "Information" in data:
                # è¿™æ˜¯AlphaVantageçš„å…¸å‹é”™è¯¯å“åº”ï¼Œè¯´æ˜éœ€è¦ä»˜è´¹API
                error_msg = data["Information"]
                logger.warning(f"AlphaVantage APIé™åˆ¶: {error_msg}")
                raise ValueError(f"éœ€è¦AlphaVantageä»˜è´¹APIå¥—é¤æ‰èƒ½è®¿é—®æœŸæƒæ•°æ®: {error_msg}")
            
            if "Note" in data:
                # APIè°ƒç”¨é¢‘ç‡é™åˆ¶æç¤º
                logger.warning(f"APIé¢‘ç‡é™åˆ¶æç¤º: {data['Note']}")
            
            if not data.get("data"):
                if "Error Message" in data:
                    raise ValueError(f"AlphaVantage APIé”™è¯¯: {data['Error Message']}")
                else:
                    # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                    logger.warning(f"æœªæ‰¾åˆ°{symbol}åœ¨{date}çš„æœŸæƒæ•°æ®")
                    return []

            # è½¬æ¢æ•°æ®ç±»å‹
            for contract in data["data"]:
                for field in ["strike", "last", "mark", "bid", "ask", 
                            "implied_volatility", "delta", "gamma", 
                            "theta", "vega", "rho"]:
                    if contract.get(field):
                        contract[field] = float(contract[field])
                for field in ["bid_size", "ask_size", "volume", "open_interest"]:
                    if contract.get(field):
                        contract[field] = int(contract[field])

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "options" / f"{symbol}_{date if date else 'latest'}.parquet"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                pd.DataFrame(data["data"]).to_parquet(file_path)
                logger.info(f"æœŸæƒæ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•ï¼š{file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "options"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"{symbol}_{date if date else 'latest'}.parquet"
                pd.DataFrame(data["data"]).to_parquet(file_path)
                logger.info(f"æœŸæƒæ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•ï¼š{file_path}")

            return data["data"]

        except Exception as e:
            logger.error(f"è·å–æœŸæƒæ•°æ®å¤±è´¥: {e}")
            # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…ä¸­æ–­æ•´ä¸ªæµç¨‹
            return []
    
    # ============ è´¢æŠ¥æ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_earnings_transcript(symbol: str, quarter: str, session_dir: Path = None) -> Dict:
        """è·å–è´¢æŠ¥ç”µè¯ä¼šè®®è®°å½•"""
        try:
            params = {
                "function": "EARNINGS_CALL_TRANSCRIPT",
                "symbol": symbol,
                "quarter": quarter,
                "apikey": AlphaVantageFetcher.get_api_key()
            }
            
            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "transcripts" / f"{symbol}_{quarter}.json"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False)
                logger.info(f"è´¢æŠ¥ä¼šè®®è®°å½•å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•ï¼š{file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "transcripts"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"{symbol}_{quarter}.json"
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False)
                logger.info(f"è´¢æŠ¥ä¼šè®®è®°å½•å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•ï¼š{file_path}")

            return data
            
        except Exception as e:
            logger.error(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•å¤±è´¥: {e}")
            raise
    
    # ============ å†…éƒ¨äº¤æ˜“æ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_insider_transactions(symbol: str, session_dir: Path = None) -> List[Dict]:
        """è·å–å…¬å¸å†…éƒ¨äººäº¤æ˜“æ•°æ®"""
        try:
            params = {
                "function": "INSIDER_TRANSACTIONS",
                "symbol": symbol,
                "apikey": AlphaVantageFetcher.get_api_key()
            }
            
            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            # è½¬æ¢æ•°æ®ç±»å‹
            transactions = []
            for item in data.get("data", []):
                transactions.append({
                    "transaction_date": item.get("transaction_date"),
                    "ticker": item.get("ticker"),
                    "executive": item.get("executive"),
                    "executive_title": item.get("executive_title"),
                    "security_type": item.get("security_type"),
                    "acquisition_or_disposal": item.get("acquisition_or_disposal"),
                    "trade_type": "ä¹°å…¥" if item.get("acquisition_or_disposal") == "A" else "å–å‡º",
                    "shares": float(item.get("shares", 0)) if item.get("shares") else 0,
                    "share_price": float(item.get("share_price", 0)) if item.get("share_price") else 0,
                    "total_value": float(item.get("shares", 0)) * float(item.get("share_price", 0)) if item.get("shares") and item.get("share_price") else 0
                })

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "insider" / f"{symbol}_insider.json"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(transactions, f, ensure_ascii=False)
                logger.info(f"å†…éƒ¨äººäº¤æ˜“æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•ï¼š{file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "insider"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"{symbol}_insider.json"
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(transactions, f, ensure_ascii=False)
                logger.info(f"å†…éƒ¨äººäº¤æ˜“æ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•ï¼š{file_path}")

            return transactions
            
        except Exception as e:
            logger.error(f"è·å–å†…éƒ¨äººäº¤æ˜“æ•°æ®å¤±è´¥: {e}")
            raise
    
    # ============ ETFæ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_etf_profile(symbol: str, session_dir: Path = None) -> Dict:
        """è·å–ETFè¯¦ç»†ä¿¡æ¯å’ŒæŒä»“æ•°æ®"""
        try:
            params = {
                "function": "ETF_PROFILE",
                "symbol": symbol,
                "apikey": AlphaVantageFetcher.get_api_key()
            }

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            # æ ‡å‡†åŒ–æ•°æ®ç»“æ„
            profile = {
                "symbol": data.get("symbol", symbol),
                "name": data.get("name"),
                "description": data.get("description"),
                "exchange": data.get("exchange"),
                "net_assets": float(data.get("net_assets", 0)),
                "expense_ratio": float(data.get("expense_ratio", 0)),
                "portfolio_turnover": float(data.get("portfolio_turnover", 0)),
                "dividend_yield": float(data.get("dividend_yield", 0)),
                "inception_date": data.get("inception_date"),
                "leveraged": data.get("leveraged", "").upper() == "YES",
                "sectors": [],
                "holdings": []
            }

            # å¤„ç†è¡Œä¸šé…ç½®æ•°æ®
            if isinstance(data.get("sectors"), list):
                for sector in data["sectors"]:
                    if isinstance(sector, dict):
                        profile["sectors"].append({
                            "sector": sector.get("sector"),
                            "weight": float(sector.get("weight", 0))
                        })

            # å¤„ç†æŒä»“æ•°æ®
            if isinstance(data.get("holdings"), list):
                for holding in data["holdings"]:
                    if isinstance(holding, dict):
                        profile["holdings"].append({
                            "symbol": holding.get("symbol"),
                            "name": holding.get("name"),
                            "weight": float(holding.get("weight", 0)),
                            "shares": int(holding.get("shares", 0)) 
                        })

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "etf" / f"{symbol}_profile.json"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(profile, f, ensure_ascii=False, indent=2)
                logger.info(f"ETFæ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•ï¼š{file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "etf"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"{symbol}_profile.json"
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(profile, f, ensure_ascii=False, indent=2)
                logger.info(f"ETFæ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•ï¼š{file_path}")
            
            return profile
            
        except Exception as e:
            logger.error(f"è·å–ETFæ•°æ®å¤±è´¥: {e}")
            raise
    
    # ============ å¤–æ±‡æ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_forex_daily(
        from_symbol: str = "USD",
        to_symbol: str = "JPY",
        outputsize: str = "full",
        session_dir: Path = None
    ) -> pd.DataFrame:
        """è·å–å¤–æ±‡æ¯æ—¥æ•°æ®"""
        try:
            params = {
                "function": "FX_DAILY",
                "from_symbol": from_symbol,
                "to_symbol": to_symbol,
                "outputsize": outputsize,
                "apikey": AlphaVantageFetcher.get_api_key()
            }

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            time_series = data.get("Time Series FX (Daily)", {})
            if not time_series:
                raise ValueError(f"æœªè·å–åˆ°å¤–æ±‡æ•°æ®ï¼Œå“åº”: {data}")

            df = pd.DataFrame.from_dict(time_series, orient="index")
            df.index = pd.to_datetime(df.index)
            df = df.sort_index()

            df = df.rename(columns={
                "1. open": "open",
                "2. high": "high", 
                "3. low": "low",
                "4. close": "close"
            })

            df = df.astype({
                "open": float,
                "high": float,
                "low": float,
                "close": float
            })

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "forex" / f"{from_symbol}_{to_symbol}.parquet"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                df.to_parquet(file_path)
                logger.info(f"å¤–æ±‡æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•: {file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "forex"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"{from_symbol}_{to_symbol}_daily.parquet"
                df.to_parquet(file_path)
                logger.info(f"å¤–æ±‡æ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•: {file_path}")

            return df

        except Exception as e:
            logger.error(f"è·å–{from_symbol}/{to_symbol}å¤–æ±‡æ•°æ®å¤±è´¥: {e}")
            raise
    
    # ============ æ•°å­—è´§å¸æ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_digital_currency_daily(
        symbol: str,
        market: str,
        session_dir: Path = None
    ) -> Dict[str, pd.DataFrame]:
        """è·å–æ•°å­—è´§å¸æ¯æ—¥æ•°æ®"""
        try:
            params = {
                "function": "DIGITAL_CURRENCY_DAILY",
                "symbol": symbol,
                "market": market,
                "apikey": AlphaVantageFetcher.get_api_key()
            }

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            time_series = data.get("Time Series (Digital Currency Daily)", {})
            if not time_series:
                raise ValueError(f"æœªè·å–åˆ°æ•°å­—è´§å¸æ•°æ®ï¼Œå“åº”: {data}")

            # è½¬æ¢ä¸ºDataFrameå¹¶å¤„ç†æ•°æ®
            df = pd.DataFrame.from_dict(time_series, orient="index")
            df.index = pd.to_datetime(df.index)
            df = df.sort_index()

            # å¤„ç†å¸‚åœºè´§å¸æ•°æ®
            if market == "USD":
                # USDå¸‚åœºä½¿ç”¨åŸºæœ¬åˆ—å
                market_df = df[[
                    "1. open",
                    "2. high",
                    "3. low",
                    "4. close",
                    "5. volume"
                ]].rename(columns={
                    "1. open": "open",
                    "2. high": "high",
                    "3. low": "low",
                    "4. close": "close",
                    "5. volume": "volume"
                })
                usd_df = market_df[["open", "high", "low", "close"]].copy()
            else:
                def get_column(df, prefix, currency):
                    """è¾…åŠ©å‡½æ•°è·å–æŒ‡å®šå‰ç¼€å’Œå¤§å°çš„åˆ—"""
                    for col in df.columns:
                        if f"{prefix}. " in col and f"({currency})" in col and not "(convert)" in col:
                            return col
                    raise ValueError(f"æ‰¾ä¸åˆ°{prefix} {currency}åˆ—")

                # è·å–å¸‚åœºè´§å¸è®¡ä»·æ•°æ®åˆ—
                market_open = get_column(df, "1a", market)
                market_high = get_column(df, "2a", market)
                market_low = get_column(df, "3a", market)
                market_close = get_column(df, "4a", market)
                volume_col = "5. volume"

                market_df = df[[
                    market_open,
                    market_high,
                    market_low,
                    market_close,
                    volume_col
                ]].rename(columns={
                    market_open: "open",
                    market_high: "high", 
                    market_low: "low",
                    market_close: "close",
                    volume_col: "volume"
                })

                # è·å–ç¾å…ƒè®¡ä»·æ•°æ®åˆ—
                usd_open = get_column(df, "1b", "USD")
                usd_high = get_column(df, "2b", "USD")
                usd_low = get_column(df, "3b", "USD")
                usd_close = get_column(df, "4b", "USD")

                usd_df = df[[
                    usd_open,
                    usd_high,
                    usd_low,
                    usd_close
                ]].rename(columns={
                    usd_open: "open",
                    usd_high: "high",
                    usd_low: "low",
                    usd_close: "close"
                })

            # è½¬æ¢æ•°æ®ç±»å‹
            market_df = market_df.astype({
                "open": float, "high": float, "low": float, 
                "close": float, "volume": float
            })
            usd_df = usd_df.astype({
                "open": float, "high": float, "low": float, "close": float
            })

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                dir_path = session_dir / "crypto"
                dir_path.mkdir(parents=True, exist_ok=True)
                
                # ç‰¹æ®Šå¤„ç†USDå¸‚åœºæ•°æ®
                if market == "USD":
                    file_path = dir_path / f"{symbol}_USD.parquet"
                    market_df.to_parquet(file_path)
                    logger.info(f"USDå¸‚åœºæ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•: {file_path}")
                else:
                    market_file = dir_path / f"{symbol}_{market}.parquet"
                    usd_file = dir_path / f"{symbol}_USD.parquet"
                    market_df.to_parquet(market_file)
                    usd_df.to_parquet(usd_file)
                    logger.info(f"æ•°å­—è´§å¸{symbol}æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•: {dir_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "digital_currency"
                temp_dir.mkdir(parents=True, exist_ok=True)
                
                if market == "USD":
                    file_path = temp_dir / "USD_market_values.parquet"
                    market_df.to_parquet(file_path)
                    logger.info(f"USDå¸‚åœºæ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•: {file_path}")
                else:
                    market_df.to_parquet(temp_dir / f"{market}_market_values.parquet")
                    usd_df.to_parquet(temp_dir / "usd_market_values.parquet")
                    logger.info(f"æ•°å­—è´§å¸{symbol}æ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•: {temp_dir}")

            return {
                "market": market_df,
                "usd": usd_df
            }

        except Exception as e:
            logger.error(f"è·å–{symbol}/{market}æ•°å­—è´§å¸æ•°æ®å¤±è´¥: {e}")
            raise
    
    # ============ å¤§å®—å•†å“æ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_wti(
        interval: str = "monthly",
        session_dir: Path = None
    ) -> pd.DataFrame:
        """è·å–WTIåŸæ²¹ä»·æ ¼æ•°æ®"""
        try:
            params = {
                "function": "WTI",
                "interval": interval,
                "apikey": AlphaVantageFetcher.get_api_key()
            }

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            if not data.get("data"):
                raise ValueError("No WTI data found in response")

            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(data["data"])
            df["date"] = pd.to_datetime(df["date"])
            # å¤„ç†å¯èƒ½çš„æ— æ•ˆæ•°å€¼ï¼ˆå¦‚'.'ç­‰ï¼‰
            df["price"] = pd.to_numeric(df["value"], errors='coerce')
            df = df.dropna(subset=['price'])
            df["price"] = df["price"].astype(float)
            df = df.drop(columns=["value"])
            df = df.set_index("date").sort_index()

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "commodities" / f"WTI_{interval}.parquet"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                df.to_parquet(file_path)
                logger.info(f"WTIåŸæ²¹æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•: {file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "commodities"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"WTI_{interval}.parquet"
                df.to_parquet(file_path)
                logger.info(f"WTIåŸæ²¹æ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•: {file_path}")

            return df

        except Exception as e:
            logger.error(f"è·å–WTIåŸæ²¹æ•°æ®å¤±è´¥: {e}")
            raise
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_brent(
        interval: str = "monthly",
        session_dir: Path = None
    ) -> pd.DataFrame:
        """è·å–BrentåŸæ²¹ä»·æ ¼æ•°æ®"""
        try:
            params = {
                "function": "BRENT",
                "interval": interval,
                "apikey": AlphaVantageFetcher.get_api_key()
            }

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            if not data.get("data"):
                raise ValueError("No Brent data found in response")

            # è½¬æ¢ä¸ºDataFrameå¹¶ä¸¥æ ¼å¤„ç†æ•°æ®
            df = pd.DataFrame(data["data"])
            df["date"] = pd.to_datetime(df["date"])
            
            # å®‰å…¨è½¬æ¢æ•°å€¼ï¼Œå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
            df["price"] = pd.to_numeric(df["value"], errors='coerce')
            
            # è®°å½•å¹¶è¿‡æ»¤æ— æ•ˆæ•°æ®
            invalid_count = df["price"].isna().sum()
            if invalid_count > 0:
                logger.warning(f"è¿‡æ»¤æ‰{invalid_count}æ¡æ— æ•ˆåŸæ²¹æ•°æ®")
                df = df.dropna(subset=['price'])
            
            df["price"] = df["price"].astype(float)
            df = df.drop(columns=["value"])
            df = df.set_index("date").sort_index()
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            if len(df) == 0:
                raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„åŸæ²¹æ•°æ®å¯ç”¨")

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "commodities" / f"BRENT_{interval}.parquet"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                df.to_parquet(file_path)
                logger.info(f"BrentåŸæ²¹æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•: {file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "commodities"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"BRENT_{interval}.parquet"
                df.to_parquet(file_path)
                logger.info(f"BrentåŸæ²¹æ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•: {file_path}")

            return df

        except Exception as e:
            logger.error(f"è·å–BrentåŸæ²¹æ•°æ®å¤±è´¥: {e}")
            raise
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_copper(
        interval: str = "monthly",
        session_dir: Path = None
    ) -> pd.DataFrame:
        """è·å–å…¨çƒé“œä»·æ•°æ®"""
        try:
            params = {
                "function": "COPPER",
                "interval": interval,
                "apikey": AlphaVantageFetcher.get_api_key()
            }

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            if not data.get("data"):
                raise ValueError("No copper price data found in response")

            # è½¬æ¢ä¸ºDataFrameå¹¶ä¸¥æ ¼å¤„ç†æ•°æ®
            df = pd.DataFrame(data["data"])
            df["date"] = pd.to_datetime(df["date"])
            
            # å®‰å…¨è½¬æ¢æ•°å€¼ï¼Œå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
            df["price"] = pd.to_numeric(df["value"], errors='coerce')
            
            # è®°å½•å¹¶è¿‡æ»¤æ— æ•ˆæ•°æ®
            invalid_count = df["price"].isna().sum()
            if invalid_count > 0:
                logger.warning(f"è¿‡æ»¤æ‰{invalid_count}æ¡æ— æ•ˆé“œä»·æ•°æ®")
                df = df.dropna(subset=['price'])
            
            df["price"] = df["price"].astype(float)
            df = df.drop(columns=["value"])
            df = df.set_index("date").sort_index()
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            if len(df) == 0:
                raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„é“œä»·æ•°æ®å¯ç”¨")

            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "commodities" / f"COPPER_{interval}.parquet"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                df.to_parquet(file_path)
                logger.info(f"é“œä»·æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•: {file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "commodities"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"COPPER_{interval}.parquet"
                df.to_parquet(file_path)
                logger.info(f"é“œä»·æ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•: {file_path}")

            return df

        except Exception as e:
            logger.error(f"è·å–é“œä»·æ•°æ®å¤±è´¥: {e}")
            raise
    
    # ============ å›½å€ºæ”¶ç›Šç‡æ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10)) 
    def fetch_treasury_yield(
        interval: str = "monthly",
        maturity: str = "10year",
        session_dir: Path = None
    ) -> pd.DataFrame:
        """è·å–ç¾å›½å›½å€ºæ”¶ç›Šç‡æ•°æ®"""
        try:
            params = {
                "function": "TREASURY_YIELD",
                "interval": interval,
                "maturity": maturity,
                "apikey": AlphaVantageFetcher.get_api_key()
            }

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            if not data.get("data"):
                raise ValueError("æœªè·å–åˆ°å›½å€ºæ”¶ç›Šç‡æ•°æ®")
                
            # è½¬æ¢ä¸ºDataFrameå¹¶å¤„ç†æ•°æ®
            df = pd.DataFrame(data["data"])
            df["date"] = pd.to_datetime(df["date"])
            df["yield"] = pd.to_numeric(df["value"], errors="coerce")
            
            # è¿‡æ»¤æ— æ•ˆæ•°æ®
            df = df.dropna(subset=["yield"])
            
            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "treasury" / f"TREASURY_{maturity}_{interval}.parquet"
                file_path.parent.mkdir(parents=True, exist_ok=True)
                df.to_parquet(file_path)
                logger.info(f"å›½å€ºæ”¶ç›Šç‡æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•: {file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "treasury"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / f"TREASURY_{maturity}_{interval}.parquet"
                df.to_parquet(file_path)
                logger.info(f"å›½å€ºæ”¶ç›Šç‡æ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•: {file_path}")
            
            return df[["date", "yield"]]
            
        except Exception as e:
            logger.error(f"è·å–å›½å€ºæ”¶ç›Šç‡æ•°æ®å¤±è´¥: {e}")
            raise
    
    # ============ æ–°é—»æƒ…ç»ªæ•°æ®æ–¹æ³• ============
    
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_news_sentiment(
        tickers: str = None,
        topics: str = None,
        time_from: str = None,
        time_to: str = None,
        sort: str = "LATEST",
        limit: int = 50,
        session_dir: Path = None
    ) -> Dict:
        """è·å–å¸‚åœºæ–°é—»å’Œæƒ…ç»ªæ•°æ®"""
        try:
            params = {
                "function": "NEWS_SENTIMENT",
                "apikey": AlphaVantageFetcher.get_api_key(),
                "sort": sort,
                "limit": limit
            }
            if tickers:
                params["tickers"] = tickers
            if topics:
                params["topics"] = topics
            if time_from:
                params["time_from"] = time_from
            if time_to:
                params["time_to"] = time_to

            response = requests.get(AlphaVantageFetcher.BASE_URL, params=params)
            response.raise_for_status()
            data = response.json()

            filename_parts = []
            if tickers:
                filename_parts.append(tickers.replace(',','_'))
            if topics:
                filename_parts.append(topics.replace(',','_'))
            if time_from:
                filename_parts.append(f"from_{time_from}")
            if time_to:
                filename_parts.append(f"to_{time_to}")
            if not filename_parts:
                filename_parts.append("latest")
            
            safe_filename = '_'.join(filename_parts).replace(':', '_').replace('/', '_').replace(' ', '_')
            filename = f"news_{safe_filename}.json"
            
            # ğŸ¯ ä¿å­˜åˆ°ä¼šè¯ç›®å½•
            if session_dir:
                file_path = session_dir / "news" / filename
                file_path.parent.mkdir(parents=True, exist_ok=True)
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False)
                logger.info(f"æ–°é—»æ•°æ®å·²ä¿å­˜è‡³ä¼šè¯ç›®å½•ï¼š{file_path}")
            else:
                # åå¤‡
                temp_dir = Path("/tmp/alphavantage_data") / "news"
                temp_dir.mkdir(parents=True, exist_ok=True)
                file_path = temp_dir / filename
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False)
                logger.info(f"æ–°é—»æ•°æ®å·²ä¿å­˜è‡³ä¸´æ—¶ç›®å½•ï¼š{file_path}")

            return data

        except Exception as e:
            logger.error(f"è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}")
            raise